Title,abstract,Processed_Title,Processed_Abstract
Brittle System Analysis,"The goal of this paper is to define and analyze systems which exhibit brittle behavior. This behavior is characterized by a sudden and steep decline in performance as the system approaches the limits of tolerance. This can be due to input parameters which exceed a specified input, or environmental conditions which exceed specified operating boundaries. An analogy is made between brittle commmunication systems in particular and materials science.",brittle system analysis,goal paper define analyze systems exhibit brittle behavior behavior characterized sudden steep decline performance system approaches limits tolerance due input parameters exceed specified input environmental conditions exceed specified operating boundaries analogy made brittle commmunication systems particular materials science
The Unix KISS: A Case Study,"In this paper we show that the initial philosophy used in designing and developing UNIX in early times has been forgotten due to ""fast practices"". We question the leitmotif that microkernels, though being by design adherent to the KISS principle, have a number of context switches higher than their monolithic counterparts, running a test suite and verify the results with standard statistical validation tests. We advocate a wiser distribution of shared libraries by statistically analyzing the weight of each shared object in a typical UNIX system, showing that the majority of shared libraries exist in a common space for no real evidence of need. Finally we examine the UNIX heritage with an historical point of view, noticing how habits swiftly replaced the intents of the original authors, moving the focus from the earliest purpose of is avoiding complications, keeping a system simple to use and maintain.",unix kiss case study,paper show initial philosophy used designing developing unix early times forgotten due fast practices question leitmotif microkernels though design adherent kiss principle number context switches higher monolithic counterparts running test suite verify results standard statistical validation tests advocate wiser distribution shared libraries statistically analyzing weight shared object typical unix system showing majority shared libraries exist common space real evidence need finally examine unix heritage historical point view noticing habits swiftly replaced intents original authors moving focus earliest purpose avoiding complications keeping system simple use maintain
ASC-Hook: fast and transparent system call hook for Arm,"Intercepting system calls is crucial for tools that aim to modify or monitor application behavior. However, existing system call interception tools on the ARM platform still suffer from limitations in terms of performance and completeness. This paper presents an efficient and comprehensive binary rewriting framework, ASC-Hook, specifically designed for intercepting system calls on the ARM platform. ASC-Hook addresses two key challenges on the ARM architecture: the misalignment of the target address caused by directly replacing the SVC instruction with br x8, and the return to the original control flow after system call interception. This is achieved through a hybrid replacement strategy and our specially designed trampoline mechanism. By implementing multiple completeness strategies specifically for system calls, we ensured comprehensive and thorough interception. Experimental results show that ASC-Hook reduces overhead to at least 1/29 of that of existing system call interception tools. We conducted extensive performance evaluations of ASC-Hook, and the average performance loss for system call-intensive applications is 3.7\% .",aschook fast transparent system call hook arm,intercepting system calls crucial tools aim modify monitor application behavior however existing system call interception tools arm platform still suffer limitations terms performance completeness paper presents efficient comprehensive binary rewriting framework aschook specifically designed intercepting system calls arm platform aschook addresses two key challenges arm architecture misalignment target address caused directly replacing svc instruction br x return original control flow system call interception achieved hybrid replacement strategy specially designed trampoline mechanism implementing multiple completeness strategies specifically system calls ensured comprehensive thorough interception experimental results show aschook reduces overhead least existing system call interception tools conducted extensive performance evaluations aschook average performance loss system callintensive applications
Learnings from an Under the Hood Analysis of an Object Storage Node IO   Stack,"Conventional object-stores are built on top of traditional OS storage stack, where I/O requests typically transfers through multiple hefty and redundant layers. The complexity of object management has grown dramatically with the ever increasing requirements of performance, consistency and fault-tolerance from storage subsystems. Simply stated, more number of intermediate layers are encountered in the I/O data path, with each passing layer adding its own syntax and semantics. Thereby increasing the overheads of request processing. In this paper, through comprehensive under-the-hood analysis of an object-storage node, we characterize the impact of object-store (and user-application) workloads on the OS I/O stack and its subsequent rippling effect on the underlying object-storage devices (OSD). We observe that the legacy architecture of the OS based I/O storage stack coupled with complex data management policies leads to a performance mismatch between what an end-storage device is capable of delivering and what it actually delivers in a production environment. Therefore, the gains derived from developing faster storage devices is often nullified. These issues get more pronounced in highly concurrent and multiplexed cloud environments. Owing to the associated issues of object-management and the vulnerabilities of the OS I/O software stacks, we discuss the potential of a new class of storage devices, known as Object-Drives. Samsung Key-Value SSD (KV-SSD) [1] and Seagate Kinetic Drive [2] are classic industrial implementations of object-drives, where host data management functionalities can be offloaded to the storage device. This leads towards the simplification of the over-all storage stack. Based on our analysis, we believe object-drives can alleviate object-stores from highly taxing overheads of data management with 20-38% time-savings over traditional Operating Systems (OS) stack.",learnings hood analysis object storage node io stack,conventional objectstores built top traditional os storage stack io requests typically transfers multiple hefty redundant layers complexity object management grown dramatically ever increasing requirements performance consistency faulttolerance storage subsystems simply stated number intermediate layers encountered io data path passing layer adding syntax semantics thereby increasing overheads request processing paper comprehensive underthehood analysis objectstorage node characterize impact objectstore userapplication workloads os io stack subsequent rippling effect underlying objectstorage devices osd observe legacy architecture os based io storage stack coupled complex data management policies leads performance mismatch endstorage device capable delivering actually delivers production environment therefore gains derived developing faster storage devices often nullified issues get pronounced highly concurrent multiplexed cloud environments owing associated issues objectmanagement vulnerabilities os io software stacks discuss potential new class storage devices known objectdrives samsung keyvalue ssd kvssd seagate kinetic drive classic industrial implementations objectdrives host data management functionalities offloaded storage device leads towards simplification overall storage stack based analysis believe objectdrives alleviate objectstores highly taxing overheads data management timesavings traditional operating systems os stack
An Example of Clifford Algebras Calculations with GiNaC,"This example of Clifford algebras calculations uses GiNaC (http://www.ginac.de/) library, which includes a support for generic Clifford algebra starting from version~1.3.0. Both symbolic and numeric calculation are possible and can be blended with other functions of GiNaC. This calculations was made for the paper math.CV/0410399.   Described features of GiNaC are already available at PyGiNaC (http://sourceforge.net/projects/pyginac/) and due to course should propagate into other software like GNU Octave (http://www.octave.org/), gTybalt (http://www.fis.unipr.it/~stefanw/gtybalt.html), which use GiNaC library as their back-end.",example clifford algebras calculations ginac,example clifford algebras calculations uses ginac httpwwwginacde library includes support generic clifford algebra starting version symbolic numeric calculation possible blended functions ginac calculations made paper mathcv described features ginac already available pyginac httpsourceforgenetprojectspyginac due course propagate software like gnu octave httpwwwoctaveorg gtybalt httpwwwfisunipritstefanwgtybalthtml use ginac library backend
Accelerator-as-a-Service in Public Clouds: An Intra-Host Traffic   Management View for Performance Isolation in the Wild,"I/O devices in public clouds have integrated increasing numbers of hardware accelerators, e.g., AWS Nitro, Azure FPGA and Nvidia BlueField. However, such specialized compute (1) is not explicitly accessible to cloud users with performance guarantee, (2) cannot be leveraged simultaneously by both providers and users, unlike general-purpose compute (e.g., CPUs). Through ten observations, we present that the fundamental difficulty of democratizing accelerators is insufficient performance isolation support. The key obstacles to enforcing accelerator isolation are (1) too many unknown traffic patterns in public clouds and (2) too many possible contention sources in the datapath. In this work, instead of scheduling such complex traffic on-the-fly and augmenting isolation support on each system component, we propose to model traffic as network flows and proactively re-shape the traffic to avoid unpredictable contention. We discuss the implications of our findings on the design of future I/O management stacks and device interfaces.",acceleratorasaservice public clouds intrahost traffic management view performance isolation wild,io devices public clouds integrated increasing numbers hardware accelerators eg aws nitro azure fpga nvidia bluefield however specialized compute explicitly accessible cloud users performance guarantee cannot leveraged simultaneously providers users unlike generalpurpose compute eg cpus ten observations present fundamental difficulty democratizing accelerators insufficient performance isolation support key obstacles enforcing accelerator isolation many unknown traffic patterns public clouds many possible contention sources datapath work instead scheduling complex traffic onthefly augmenting isolation support system component propose model traffic network flows proactively reshape traffic avoid unpredictable contention discuss implications findings design future io management stacks device interfaces
Theory and practice,"The author argues to Silicon Valley that the most important and powerful part of computer science is work that is simultaneously theoretical and practical. He particularly considers the intersection of the theory of algorithms and practical software development. He combines examples from the development of the TeX typesetting system with clever jokes, criticisms, and encouragements.",theory practice,author argues silicon valley important powerful part computer science work simultaneously theoretical practical particularly considers intersection theory algorithms practical software development combines examples development tex typesetting system clever jokes criticisms encouragements
The Revolution Yet to Happen,"All information about physical objects including humans, buildings, processes, and organizations will be online. This trend is both desirable and inevitable. Cyberspace will provide the basis for wonderful new ways to inform, entertain, and educate people. The information and the corresponding systems will streamline commerce, but will also provide new levels of personal service, health care, and automation. The most significant benefit will be a breakthrough in our ability to remotely communicate with one another using all our senses.   The ACM and the transistor were born in 1947. At that time the stored program computer was a revolutionary idea and the transistor was just a curiosity. Both ideas evolved rapidly. By the mid 1960s integrated circuits appeared -- allowing mass fabrication of transistors on silicon substrates. This allowed low-cost mass-produced computers. These technologies enabled extraordinary increases in processing speed and memory coupled with extraordinary price declines.   The only form of processing and memory more easily, cheaply, and rapidly fabricated is the human brain. Peter Cohrane (1996) estimates the brain to have a processing power of around 1000 million-million operations per second, (one Petaops) and a memory of 10 Terabytes. If current trends continue, computers could have these capabilities by 2047. Such computers could be 'on body' personal assistants able to recall everything one reads, hears, and sees.",revolution yet happen,information physical objects including humans buildings processes organizations online trend desirable inevitable cyberspace provide basis wonderful new ways inform entertain educate people information corresponding systems streamline commerce also provide new levels personal service health care automation significant benefit breakthrough ability remotely communicate one another using senses acm transistor born time stored program computer revolutionary idea transistor curiosity ideas evolved rapidly mid integrated circuits appeared allowing mass fabrication transistors silicon substrates allowed lowcost massproduced computers technologies enabled extraordinary increases processing speed memory coupled extraordinary price declines form processing memory easily cheaply rapidly fabricated human brain peter cohrane estimates brain processing power around millionmillion operations per second one petaops memory terabytes current trends continue computers could capabilities computers could body personal assistants able recall everything one reads hears sees
What Next? A Dozen Information-Technology Research Goals,"Charles Babbage's vision of computing has largely been realized. We are on the verge of realizing Vannevar Bush's Memex. But, we are some distance from passing the Turing Test. These three visions and their associated problems have provided long-range research goals for many of us. For example, the scalability problem has motivated me for several decades. This talk defines a set of fundamental research problems that broaden the Babbage, Bush, and Turing visions. They extend Babbage's computational goal to include highly-secure, highly-available, self-programming, self-managing, and self-replicating systems. They extend Bush's Memex vision to include a system that automatically organizes, indexes, digests, evaluates, and summarizes information (as well as a human might). Another group of problems extends Turing's vision of intelligent machines to include prosthetic vision, speech, hearing, and other senses. Each problem is simply stated and each is orthogonal from the others, though they share some common core technologies",next dozen informationtechnology research goals,charles babbages vision computing largely realized verge realizing vannevar bushs memex distance passing turing test three visions associated problems provided longrange research goals many us example scalability problem motivated several decades talk defines set fundamental research problems broaden babbage bush turing visions extend babbages computational goal include highlysecure highlyavailable selfprogramming selfmanaging selfreplicating systems extend bushs memex vision include system automatically organizes indexes digests evaluates summarizes information well human might another group problems extends turings vision intelligent machines include prosthetic vision speech hearing senses problem simply stated orthogonal others though share common core technologies
Questions for a Materialist Philosophy Implying the Equivalence of   Computers and Human Cognition,"Issues related to a materialist philosophy are explored as concerns the implied equivalence of computers running software and human observers. One issue explored concerns the measurement process in quantum mechanics. Another issue explored concerns the nature of experience as revealed by the existence of dreams. Some difficulties stemming from a materialist philosophy as regards these issues are pointed out. For example, a gedankenexperiment involving what has been called ""negative"" observation is discussed that illustrates the difficulty with a materialist assumption in quantum mechanics. Based on an exploration of these difficulties, specifications are outlined briefly that would provide a means to demonstrate the equivalence of of computers running software and human experience given a materialist assumption.",questions materialist philosophy implying equivalence computers human cognition,issues related materialist philosophy explored concerns implied equivalence computers running software human observers one issue explored concerns measurement process quantum mechanics another issue explored concerns nature experience revealed existence dreams difficulties stemming materialist philosophy regards issues pointed example gedankenexperiment involving called negative observation discussed illustrates difficulty materialist assumption quantum mechanics based exploration difficulties specifications outlined briefly would provide means demonstrate equivalence computers running software human experience given materialist assumption
One More Revolution to Make: Free Scientific Publishing,"Computer scientists are in the position to create new, free high-quality journals. So what would it take?",one revolution make free scientific publishing,computer scientists position create new free highquality journals would take
ENUM: The Collision of Telephony and DNS Policy,"ENUM marks either the convergence or collision of the public telephone network with the Internet. ENUM is an innovation in the domain name system (DNS). It starts with numerical domain names that are used to query DNS name servers. The servers respond with address information found in DNS records. This can be telephone numbers, email addresses, fax numbers, SIP addresses, or other information. The concept is to use a single number in order to obtain a plethora of contact information.   By convention, the Internet Engineering Task Force (IETF) ENUM Working Group determined that an ENUM number would be the same numerical string as a telephone number. In addition, the assignee of an ENUM number would be the assignee of that telephone number. But ENUM could work with any numerical string or, in fact, any domain name. The IETF is already working on using E.212 numbers with ENUM. [Abridged]",enum collision telephony dns policy,enum marks either convergence collision public telephone network internet enum innovation domain name system dns starts numerical domain names used query dns name servers servers respond address information found dns records telephone numbers email addresses fax numbers sip addresses information concept use single number order obtain plethora contact information convention internet engineering task force ietf enum working group determined enum number would numerical string telephone number addition assignee enum number would assignee telephone number enum could work numerical string fact domain name ietf already working using e numbers enum abridged
Edsger Wybe Dijkstra (1930 -- 2002): A Portrait of a Genius,"We discuss the scientific contributions of Edsger Wybe Dijkstra, his opinions and his legacy.",edsger wybe dijkstra portrait genius,discuss scientific contributions edsger wybe dijkstra opinions legacy
Classical and Nonextensive Information Theory,"In this work we firstly review some results in Classical Information Theory. Next, we try to generalize these results by using the Tsallis entropy. We present a preliminary result and discuss our aims in this field.",classical nonextensive information theory,work firstly review results classical information theory next try generalize results using tsallis entropy present preliminary result discuss aims field
The pre-history of quantum computation,The main ideas behind developments in the theory and technology of quantum computation were formulated in the late 1970s and early 1980s by two physicists in the West and a mathematician in the former Soviet Union. It is not generally known in the West that the subject has roots in the Russian technical literature. The author hopes to present as impartial a synthesis as possible of the early history of thought on this subject. The role of reversible and irreversible computational processes is examined briefly as it relates to the origins of quantum computing and the so-called Information Paradox in physics.,prehistory quantum computation,main ideas behind developments theory technology quantum computation formulated late early two physicists west mathematician former soviet union generally known west subject roots russian technical literature author hopes present impartial synthesis possible early history thought subject role reversible irreversible computational processes examined briefly relates origins quantum computing socalled information paradox physics
Some first thoughts on the stability of the asynchronous systems,"The (non-initialized, non-deterministic) asynchronous systems (in the input-output sense) are multi-valued functions from m-dimensional signals to sets of n-dimensional signals, the concept being inspired by the modeling of the asynchronous circuits. Our purpose is to state the problem of the their stability.",first thoughts stability asynchronous systems,noninitialized nondeterministic asynchronous systems inputoutput sense multivalued functions mdimensional signals sets ndimensional signals concept inspired modeling asynchronous circuits purpose state problem stability
The equations of the ideal latches,"The latches are simple circuits with feedback from the digital electrical engineering. We have included in our work the C element of Muller, the RS latch, the clocked RS latch, the D latch and also circuits containing two interconnected latches: the edge triggered RS flip-flop, the D flip-flop, the JK flip-flop, the T flip-flop. The purpose of this study is to model with equations the previous circuits, considered to be ideal, i.e. non-inertial. The technique of analysis is the pseudoboolean differential calculus.",equations ideal latches,latches simple circuits feedback digital electrical engineering included work c element muller rs latch clocked rs latch latch also circuits containing two interconnected latches edge triggered rs flipflop flipflop jk flipflop flipflop purpose study model equations previous circuits considered ideal ie noninertial technique analysis pseudoboolean differential calculus
Real Time Models of the Asynchronous Circuits: The Delay Theory,"The chapter from the book introduces the delay theory, whose purpose is the modeling of the asynchronous circuits from digital electrical engineering with ordinary and differential pseudo-boolean equations.",real time models asynchronous circuits delay theory,chapter book introduces delay theory whose purpose modeling asynchronous circuits digital electrical engineering ordinary differential pseudoboolean equations
Methods for scaling a large member base,"The technical challenges of scaling websites with large and growing member bases, like social networking sites, are numerous. One of these challenges is how to evenly distribute the growing member base across all available resources. This paper will explore various methods that address this issue. The techniques used in this paper can be generalized and applied to various other problems that need to distribute data evenly amongst a finite amount of resources.",methods scaling large member base,technical challenges scaling websites large growing member bases like social networking sites numerous one challenges evenly distribute growing member base across available resources paper explore various methods address issue techniques used paper generalized applied various problems need distribute data evenly amongst finite amount resources
Ten Incredibly Dangerous Software Ideas,This is a rough draft synopsis of a book presently in preparation. This book provides a systematic critique of the software industry. This critique is accomplished using classical methods in practical design science.,ten incredibly dangerous software ideas,rough draft synopsis book presently preparation book provides systematic critique software industry critique accomplished using classical methods practical design science
The intersection and the union of the asynchronous systems,"The asynchronous systems $f$ are the models of the asynchronous circuits from digital electrical engineering. They are multi-valued functions that associate to each input $u:\mathbf{R}\to \{0,1\}^{m}$ a set of states $x\in f(u),$ where $x:\mathbf{R}\to \{0,1\}^{n}.$ The intersection of the systems allows adding supplementary conditions in modeling and the union of the systems allows considering the validity of one of two systems in modeling, for example when testing the asynchronous circuits and the circuit is supposed to be 'good' or 'bad'. The purpose of the paper is that of analyzing the intersection and the union against the initial/final states, initial/final time, initial/final state functions, subsystems, dual systems, inverse systems, Cartesian product of systems, parallel connection and serial connection of systems.",intersection union asynchronous systems,asynchronous systems f models asynchronous circuits digital electrical engineering multivalued functions associate input umathbfrto set states xin fu xmathbfrto n intersection systems allows adding supplementary conditions modeling union systems allows considering validity one two systems modeling example testing asynchronous circuits circuit supposed good bad purpose paper analyzing intersection union initialfinal states initialfinal time initialfinal state functions subsystems dual systems inverse systems cartesian product systems parallel connection serial connection systems
"Recruitment, Preparation, Retention: A case study of computing culture   at the University of Illinois at Urbana-Champaign","Computer science is seeing a decline in enrollment at all levels of education, including undergraduate and graduate study. This paper reports on the results of a study conducted at the University of Illinois at Urbana-Champaign which evaluated students attitudes regarding three areas which can contribute to improved enrollment in the Department of Computer Science: Recruitment, preparation and retention. The results of our study saw two themes. First, the department's tight research focus appears to draw significant attention from other activities -- such as teaching, service, and other community-building activities -- that are necessary for a department's excellence. Yet, as demonstrated by our second theme, one partial solution is to better promote such activities already employed by the department to its students and faculty. Based on our results, we make recommendations for improvements and enhancements based on the current state of practice at peer institutions.",recruitment preparation retention case study computing culture university illinois urbanachampaign,computer science seeing decline enrollment levels education including undergraduate graduate study paper reports results study conducted university illinois urbanachampaign evaluated students attitudes regarding three areas contribute improved enrollment department computer science recruitment preparation retention results study saw two themes first departments tight research focus appears draw significant attention activities teaching service communitybuilding activities necessary departments excellence yet demonstrated second theme one partial solution better promote activities already employed department students faculty based results make recommendations improvements enhancements based current state practice peer institutions
"Motivation, Design, and Ubiquity: A Discussion of Research Ethics and   Computer Science","Modern society is permeated with computers, and the software that controls them can have latent, long-term, and immediate effects that reach far beyond the actual users of these systems. This places researchers in Computer Science and Software Engineering in a critical position of influence and responsibility, more than any other field because computer systems are vital research tools for other disciplines. This essay presents several key ethical concerns and responsibilities relating to research in computing. The goal is to promote awareness and discussion of ethical issues among computer science researchers. A hypothetical case study is provided, along with questions for reflection and discussion.",motivation design ubiquity discussion research ethics computer science,modern society permeated computers software controls latent longterm immediate effects reach far beyond actual users systems places researchers computer science software engineering critical position influence responsibility field computer systems vital research tools disciplines essay presents several key ethical concerns responsibilities relating research computing goal promote awareness discussion ethical issues among computer science researchers hypothetical case study provided along questions reflection discussion
Stop That Subversive Spreadsheet!,This paper documents the formation of the European Spreadsheet Risks Interest Group (EuSpRIG www.eusprig.org) and outlines some of the research undertaken and reported upon by interested parties in EuSpRIG publications,stop subversive spreadsheet,paper documents formation european spreadsheet risks interest group eusprig wwweusprigorg outlines research undertaken reported upon interested parties eusprig publications
The equations of the ideal latches,"The latches are simple circuits with feedback from the digital electrical engineering. We have included in our work the C element of Muller, the RS latch, the clocked RS latch, the D latch and also circuits containing two interconnected latches: the edge triggered RS flip-flop, the D flip-flop, the JK flip-flop, the T flip-flop. The purpose of this study is to model with equations the previous circuits, considered to be ideal, i.e. non-inertial. The technique of analysis is the pseudoboolean differential calculus.",equations ideal latches,latches simple circuits feedback digital electrical engineering included work c element muller rs latch clocked rs latch latch also circuits containing two interconnected latches edge triggered rs flipflop flipflop jk flipflop flipflop purpose study model equations previous circuits considered ideal ie noninertial technique analysis pseudoboolean differential calculus
The non-anticipation of the asynchronous systems,The asynchronous systems are the models of the asynchronous circuits from the digital electrical engineering and non-anticipation is one of the most important properties in systems theory. Our present purpose is to introduce several concepts of non-anticipation of the asynchronous systems.,nonanticipation asynchronous systems,asynchronous systems models asynchronous circuits digital electrical engineering nonanticipation one important properties systems theory present purpose introduce several concepts nonanticipation asynchronous systems
Design and Implementation of a Master of Science in Information and   Computer Sciences - An Inventory and retrospect for the last four years,"This Master of Science in Computer and Information Sciences (MICS) is an international accredited master program that has been initiated in 2004 and started in September 2005. MICS is a research-oriented academic study of 4 semesters and a continuation of the Bachelor towards the PhD. It is completely taught in English, supported by lecturers coming from more than ten different countries. This report compass a description of its underlying architecture, describes some implementation details and gives a presentation of diverse experiences and results. As the program has been designed and implemented right after the creation of the University, the significance of the program is moreover a self-discovery of the computer science department, which has finally led to the creation of the today's research institutes and research axes.",design implementation master science information computer sciences inventory retrospect last four years,master science computer information sciences mics international accredited master program initiated started september mics researchoriented academic study semesters continuation bachelor towards phd completely taught english supported lecturers coming ten different countries report compass description underlying architecture describes implementation details gives presentation diverse experiences results program designed implemented right creation university significance program moreover selfdiscovery computer science department finally led creation todays research institutes research axes
Modeling Time in Computing: A Taxonomy and a Comparative Survey,"The increasing relevance of areas such as real-time and embedded systems, pervasive computing, hybrid systems control, and biological and social systems modeling is bringing a growing attention to the temporal aspects of computing, not only in the computer science domain, but also in more traditional fields of engineering.   This article surveys various approaches to the formal modeling and analysis of the temporal features of computer-based systems, with a level of detail that is suitable also for non-specialists. In doing so, it provides a unifying framework, rather than just a comprehensive list of formalisms.   The paper first lays out some key dimensions along which the various formalisms can be evaluated and compared. Then, a significant sample of formalisms for time modeling in computing are presented and discussed according to these dimensions. The adopted perspective is, to some extent, historical, going from ""traditional"" models and formalisms to more modern ones.",modeling time computing taxonomy comparative survey,increasing relevance areas realtime embedded systems pervasive computing hybrid systems control biological social systems modeling bringing growing attention temporal aspects computing computer science domain also traditional fields engineering article surveys various approaches formal modeling analysis temporal features computerbased systems level detail suitable also nonspecialists provides unifying framework rather comprehensive list formalisms paper first lays key dimensions along various formalisms evaluated compared significant sample formalisms time modeling computing presented discussed according dimensions adopted perspective extent historical going traditional models formalisms modern ones
Free and Open Source Software for Development,"Development organizations and International Non-Governmental Organizations have been emphasizing the high potential of Free and Open Source Software for the Less Developed Countries. Cost reduction, less vendor dependency and increased potential for local capacity development have been their main arguments. In spite of its advantages, Free and Open Source Software is not widely adopted at the African continent. In this book the authors will explore the grounds on with these expectations are based. Where do they come from and is there evidence to support these expectations? Over the past years several projects have been initiated and some good results have been achieved, but at the same time many challenges were encountered. What lessons can be drawn from these experiences and do these experiences contain enough evidence to support the high expectations? Several projects and their achievements will be considered. In the final part of the book the future of Free and Open Source Software for Development will be explored. Special attention is given to the African continent since here challenges are highest. What is the role of Free and open Source Software for Development and how do we need to position and explore the potential? What are the threats? The book aims at professionals that are engaged in the design and implementation of ICT for Development (ICT4D) projects and want to improve their understanding of the role Free and Open Source Software can play.",free open source software development,development organizations international nongovernmental organizations emphasizing high potential free open source software less developed countries cost reduction less vendor dependency increased potential local capacity development main arguments spite advantages free open source software widely adopted african continent book authors explore grounds expectations based come evidence support expectations past years several projects initiated good results achieved time many challenges encountered lessons drawn experiences experiences contain enough evidence support high expectations several projects achievements considered final part book future free open source software development explored special attention given african continent since challenges highest role free open source software development need position explore potential threats book aims professionals engaged design implementation ict development ictd projects want improve understanding role free open source software play
A Dialogue Concerning Two World Systems: Info-Computational vs.   Mechanistic,"The dialogue develops arguments for and against adopting a new world system, info-computationalist naturalism, that is poised to replace the traditional mechanistic world system. We try to figure out what the info-computational paradigm would mean, in particular its pancomputationalism. We make some steps towards developing the notion of computing that is necessary here, especially in relation to traditional notions. We investigate whether pancomputationalism can possibly provide the basic causal structure to the world, whether the overall research programme appears productive and whether it can revigorate computationalism in the philosophy of mind.",dialogue concerning two world systems infocomputational vs mechanistic,dialogue develops arguments adopting new world system infocomputationalist naturalism poised replace traditional mechanistic world system try figure infocomputational paradigm would mean particular pancomputationalism make steps towards developing notion computing necessary especially relation traditional notions investigate whether pancomputationalism possibly provide basic causal structure world whether overall research programme appears productive whether revigorate computationalism philosophy mind
Making Sense of the Evolution of a Scientific Domain: A Visual Analytic   Study of the Sloan Digital Sky Survey Research,"We introduce a new visual analytic approach to the study of scientific discoveries and knowledge diffusion. Our approach enhances contemporary co-citation network analysis by enabling analysts to identify co-citation clusters of cited references intuitively, synthesize thematic contexts in which these clusters are cited, and trace how research focus evolves over time. The new approach integrates and streamlines a few previously isolated techniques such as spectral clustering and feature selection algorithms. The integrative procedure is expected to empower and strengthen analytical and sense making capabilities of scientists, learners, and researchers to understand the dynamics of the evolution of scientific domains in a wide range of scientific fields, science studies, and science policy evaluation and planning. We demonstrate the potential of our approach through a visual analysis of the evolution of astronomical research associated with the Sloan Digital Sky Survey (SDSS) using bibliographic data between 1994 and 2008. In addition, we also demonstrate that the approach can be consistently applied to a set of heterogeneous data sources such as e-prints on arXiv, publications on ADS, and NSF awards related to the same topic of SDSS.",making sense evolution scientific domain visual analytic study sloan digital sky survey research,introduce new visual analytic approach study scientific discoveries knowledge diffusion approach enhances contemporary cocitation network analysis enabling analysts identify cocitation clusters cited references intuitively synthesize thematic contexts clusters cited trace research focus evolves time new approach integrates streamlines previously isolated techniques spectral clustering feature selection algorithms integrative procedure expected empower strengthen analytical sense making capabilities scientists learners researchers understand dynamics evolution scientific domains wide range scientific fields science studies science policy evaluation planning demonstrate potential approach visual analysis evolution astronomical research associated sloan digital sky survey sdss using bibliographic data addition also demonstrate approach consistently applied set heterogeneous data sources eprints arxiv publications ads nsf awards related topic sdss
Removing Barriers to Interdisciplinary Research,"A significant amount of high-impact contemporary scientific research occurs where biology, computer science, engineering and chemistry converge. Although programmes have been put in place to support such work, the complex dynamics of interdisciplinarity are still poorly understood. In this paper we interrogate the nature of interdisciplinary research and how we might measure its ""success"", identify potential barriers to its implementation, and suggest possible mechanisms for removing these impediments.",removing barriers interdisciplinary research,significant amount highimpact contemporary scientific research occurs biology computer science engineering chemistry converge although programmes put place support work complex dynamics interdisciplinarity still poorly understood paper interrogate nature interdisciplinary research might measure success identify potential barriers implementation suggest possible mechanisms removing impediments
On the serial connection of the regular asynchronous systems,"The asynchronous systems f are multi-valued functions, representing the non-deterministic models of the asynchronous circuits from the digital electrical engineering. In real time, they map an 'admissible input' function u:R\rightarrow{0,1}^{m} to a set f(u) of 'possible states' x\inf(u), where x:R\rightarrow{0,1}^{m}. When f is defined by making use of a 'generator function' {\Phi}:{0,1}^{n}\times{0,1}^{m}\rightarrow{0,1}^{n}, the system is called regular. The usual definition of the serial connection of systems as composition of multi-valued functions does not bring the regular systems into regular systems, thus the first issue in this study is to modify in an acceptable manner the definition of the serial connection in a way that matches regularity. This intention was expressed for the first time, without proving the regularity of the serial connection of systems, in a previous work. Our present purpose is to restate with certain corrections and prove that result.",serial connection regular asynchronous systems,asynchronous systems f multivalued functions representing nondeterministic models asynchronous circuits digital electrical engineering real time map admissible input function urrightarrowm set fu possible states xinfu xrrightarrowm f defined making use generator function phintimesmrightarrown system called regular usual definition serial connection systems composition multivalued functions bring regular systems regular systems thus first issue study modify acceptable manner definition serial connection way matches regularity intention expressed first time without proving regularity serial connection systems previous work present purpose restate certain corrections prove result
Info-Computationalism and Philosophical Aspects of Research in   Information Sciences,"The historical development has lead to the decay of Natural Philosophy which until 19th century included all of our knowledge about the physical world into the growing multitude of specialized sciences. The focus on the in-depth enquiry disentangled from its broad context lead to the problem of loss of common world-view and impossibility of communication between specialist research fields because of different languages they developed in isolation. The need for a new unifying framework is becoming increasingly apparent with the information technology enabling and intensifying the communication between different research fields and knowledge communities. This time, not only natural sciences, but also all of human knowledge is being integrated in a global network such as Internet with its diverse knowledge and language communities. Info-computationalism (ICON) as a synthesis of pancomputationalism and paninformationalism presents a unifying framework for understanding of natural phenomena including living beings and their cognition, their ways of processing information and producing knowledge. Within ICON physical universe is understood as a network of computational processes on an informational structure.",infocomputationalism philosophical aspects research information sciences,historical development lead decay natural philosophy th century included knowledge physical world growing multitude specialized sciences focus indepth enquiry disentangled broad context lead problem loss common worldview impossibility communication specialist research fields different languages developed isolation need new unifying framework becoming increasingly apparent information technology enabling intensifying communication different research fields knowledge communities time natural sciences also human knowledge integrated global network internet diverse knowledge language communities infocomputationalism icon synthesis pancomputationalism paninformationalism presents unifying framework understanding natural phenomena including living beings cognition ways processing information producing knowledge within icon physical universe understood network computational processes informational structure
Alan Turing's Legacy: Info-Computational Philosophy of Nature,"Alan Turing's pioneering work on computability, and his ideas on morphological computing support Andrew Hodges' view of Turing as a natural philosopher. Turing's natural philosophy differs importantly from Galileo's view that the book of nature is written in the language of mathematics (The Assayer, 1623). Computing is more than a language of nature as computation produces real time physical behaviors. This article presents the framework of Natural Info-computationalism as a contemporary natural philosophy that builds on the legacy of Turing's computationalism. Info-computationalism is a synthesis of Informational Structural Realism (the view that nature is a web of informational structures) and Natural Computationalism (the view that nature physically computes its own time development). It presents a framework for the development of a unified approach to nature, with common interpretation of inanimate nature as well as living organisms and their social networks. Computing is understood as information processing that drives all the changes on different levels of organization of information and can be modeled as morphological computing on data sets pertinent to informational structures. The use of infocomputational conceptualizations, models and tools makes possible for the first time in history the study of complex selforganizing adaptive systems, including basic characteristics and functions of living systems, intelligence, and cognition.",alan turings legacy infocomputational philosophy nature,alan turings pioneering work computability ideas morphological computing support andrew hodges view turing natural philosopher turings natural philosophy differs importantly galileos view book nature written language mathematics assayer computing language nature computation produces real time physical behaviors article presents framework natural infocomputationalism contemporary natural philosophy builds legacy turings computationalism infocomputationalism synthesis informational structural realism view nature web informational structures natural computationalism view nature physically computes time development presents framework development unified approach nature common interpretation inanimate nature well living organisms social networks computing understood information processing drives changes different levels organization information modeled morphological computing data sets pertinent informational structures use infocomputational conceptualizations models tools makes possible first time history study complex selforganizing adaptive systems including basic characteristics functions living systems intelligence cognition
Le droit du numrique : une histoire  prserver,"Although the history of informatics is recent, this field poses unusual problems with respect to its preservation. These problems are amplified by legal issues, digital law being in itself a subject matter whose history is also worth presenting in a computer science museum. The purpose of this paper is to present a quick overview of the evolution of law regarding digital matters, from an historical perspective as well as with respect to the preservation and presentation of the works.",le droit du numrique une histoire prserver,although history informatics recent field poses unusual problems respect preservation problems amplified legal issues digital law subject matter whose history also worth presenting computer science museum purpose paper present quick overview evolution law regarding digital matters historical perspective well respect preservation presentation works
Computing Nature: A Network of Networks of Concurrent Information   Processes,"This text presents the research field of natural/unconventional computing as it appears in the book COMPUTING NATURE. The articles discussed consist a selection of works from the Symposium on Natural Computing at AISB-IACAP (British Society for the Study of Artificial Intelligence and the Simulation of Behaviour and The International Association for Computing and Philosophy) World Congress 2012, held at the University of Birmingham, celebrating Turing centenary. The COMPUTING NATURE is about nature considered as the totality of physical existence, the universe. By physical we mean all phenomena, objects and processes, that are possible to detect either directly by our senses or via instruments. Historically, there have been many ways of describing the universe (cosmic egg, cosmic tree, theistic universe, mechanistic universe) while a particularly prominent contemporary approach is computational universe, as discussed in this article.",computing nature network networks concurrent information processes,text presents research field naturalunconventional computing appears book computing nature articles discussed consist selection works symposium natural computing aisbiacap british society study artificial intelligence simulation behaviour international association computing philosophy world congress held university birmingham celebrating turing centenary computing nature nature considered totality physical existence universe physical mean phenomena objects processes possible detect either directly senses via instruments historically many ways describing universe cosmic egg cosmic tree theistic universe mechanistic universe particularly prominent contemporary approach computational universe discussed article
NanoInfoBio: A case-study in interdisciplinary research,"A significant amount of high-impact contemporary scientific research occurs where biology, computer science, engineering and chemistry converge. Although programmes have been put in place to support such work, the complex dynamics of interdisciplinarity are still poorly understood. In this paper we highlight potential barriers to effective research across disciplines, and suggest, using a case study, possible mechanisms for removing these impediments.",nanoinfobio casestudy interdisciplinary research,significant amount highimpact contemporary scientific research occurs biology computer science engineering chemistry converge although programmes put place support work complex dynamics interdisciplinarity still poorly understood paper highlight potential barriers effective research across disciplines suggest using case study possible mechanisms removing impediments
Typologies of Computation and Computational Models,"We need much better understanding of information processing and computation as its primary form. Future progress of new computational devices capable of dealing with problems of big data, internet of things, semantic web, cognitive robotics and neuroinformatics depends on the adequate models of computation. In this article we first present the current state of the art through systematization of existing models and mechanisms, and outline basic structural framework of computation. We argue that defining computation as information processing, and given that there is no information without (physical) representation, the dynamics of information on the fundamental level is physical/ intrinsic/ natural computation. As a special case, intrinsic computation is used for designed computation in computing machinery. Intrinsic natural computation occurs on variety of levels of physical processes, containing the levels of computation of living organisms (including highly intelligent animals) as well as designed computational devices. The present article offers a typology of current models of computation and indicates future paths for the advancement of the field; both by the development of new computational models and by learning from nature how to better compute using different mechanisms of intrinsic computation.",typologies computation computational models,need much better understanding information processing computation primary form future progress new computational devices capable dealing problems big data internet things semantic web cognitive robotics neuroinformatics depends adequate models computation article first present current state art systematization existing models mechanisms outline basic structural framework computation argue defining computation information processing given information without physical representation dynamics information fundamental level physical intrinsic natural computation special case intrinsic computation used designed computation computing machinery intrinsic natural computation occurs variety levels physical processes containing levels computation living organisms including highly intelligent animals well designed computational devices present article offers typology current models computation indicates future paths advancement field development new computational models learning nature better compute using different mechanisms intrinsic computation
Les connaissances de la toile,How to manage knowledge on the Web.,les connaissances de la toile,manage knowledge web
Levels of Abstraction and the Apparent Contradictory Philosophical   Legacy of Turing and Shannon,"In a recent article, Luciano Floridi explains his view of Turing's legacy in connection to the philosophy of information. I will very briefly survey one of Turing's other contributions to the philosophy of information and computation, including similarities to Shannon's own methodological approach to information through communication, showing how crucial they are and have been as methodological strategies to understanding key aspects of these concepts. While Floridi's concept of Levels of Abstraction is related to the novel methodology of Turing's imitation game for tackling the question of machine intelligence, Turing's other main contribution to the philosophy of information runs contrary to it. Indeed, the seminal concept of computation universality strongly suggests the deletion of fundamental differences among seemingly different levels of description. How might we reconcile these apparently contradictory contributions? I will argue that Turing's contribution should prompt us to plot some directions for a philosophy of information and computation, one that closely parallels the most important developments in computer science, one that understands the profound implications of the works of Turing, Shannon and others.",levels abstraction apparent contradictory philosophical legacy turing shannon,recent article luciano floridi explains view turings legacy connection philosophy information briefly survey one turings contributions philosophy information computation including similarities shannons methodological approach information communication showing crucial methodological strategies understanding key aspects concepts floridis concept levels abstraction related novel methodology turings imitation game tackling question machine intelligence turings main contribution philosophy information runs contrary indeed seminal concept computation universality strongly suggests deletion fundamental differences among seemingly different levels description might reconcile apparently contradictory contributions argue turings contribution prompt us plot directions philosophy information computation one closely parallels important developments computer science one understands profound implications works turing shannon others
Writing and Publishing Scientific Articles in Computer Science,"Over 15 years of teaching, advising students and coordinating scientific research activities and projects in computer science, we have observed the difficulties of students to write scientific papers to present the results of their research practices. In addition, they repeatedly have doubts about the publishing process. In this article we propose a conceptual framework to support the writing and publishing of scientific papers in computer science, providing a kind of guide for computer science students to effectively present the results of their research practices, particularly for experimental research.",writing publishing scientific articles computer science,years teaching advising students coordinating scientific research activities projects computer science observed difficulties students write scientific papers present results research practices addition repeatedly doubts publishing process article propose conceptual framework support writing publishing scientific papers computer science providing kind guide computer science students effectively present results research practices particularly experimental research
"Bouncing Towers move faster than Hanoi Towers, but still require   exponential time","The problem of the Hanoi Tower is a classic exercise in recursive programming: the solution has a simple recursive definition, and its complexity and the matching lower bound are the solution of a simple recursive function (the solution is so easy that most students memorize it and regurgitate it at exams without truly understanding it). We describe how some very minor changes in the rules of the Hanoi Tower yield various increases of complexity in the solution, so that they require a deeper analysis than the classical Hanoi Tower problem while still yielding exponential solutions. In particular, we analyze the problem fo the Bouncing Tower, where just changing the insertion and extraction position from the top to the middle of the tower results in a surprising increase of complexity in the solution: such a tower of $n$ disks can be optimally moved in $\sqrt{3}^n$ moves for $n$ even (i.e. less than a Hanoi Tower of same height), via $5$ recursive functions (or, equivalently, one recursion function with $5$ states).",bouncing towers move faster hanoi towers still require exponential time,problem hanoi tower classic exercise recursive programming solution simple recursive definition complexity matching lower bound solution simple recursive function solution easy students memorize regurgitate exams without truly understanding describe minor changes rules hanoi tower yield various increases complexity solution require deeper analysis classical hanoi tower problem still yielding exponential solutions particular analyze problem fo bouncing tower changing insertion extraction position top middle tower results surprising increase complexity solution tower n disks optimally moved sqrtn moves n even ie less hanoi tower height via recursive functions equivalently one recursion function states
"Life, The Mind, and Everything","Incompleteness theorems of Godel, Turing, Chaitin, and Algorithmic Information Theory have profound epistemological implications. Incompleteness limits our ability to ever understand every observable phenomenon in the universe. Incompleteness limits the ability of evolutionary processes from finding optimal solutions. Incompleteness limits the detectability of machine consciousness. This is an effort to convey these thoughts and results in a somewhat entertaining manner.",life mind everything,incompleteness theorems godel turing chaitin algorithmic information theory profound epistemological implications incompleteness limits ability ever understand every observable phenomenon universe incompleteness limits ability evolutionary processes finding optimal solutions incompleteness limits detectability machine consciousness effort convey thoughts results somewhat entertaining manner
Research Methods in Computer Science: The Challenges and Issues,"Research methods are essential parts in conducting any research project. Although they have been theorized and summarized based on best practices, every field of science requires an adaptation of the overall approaches to perform research activities. In addition, any specific research needs a particular adjustment to the generalized approach and specializing them to suit the project in hand. However, unlike most well-established science disciplines, computing research is not supported by well-defined, globally accepted methods. This is because of its infancy and ambiguity in its definition, on one hand, and its extensive coverage and overlap with other fields, on the other hand. This article discusses the research methods in science and engineering in general and in computing in particular. It shows that despite several special parameters that make research in computing rather unique, it still follows the same steps that any other scientific research would do. The article also shows the particularities that researchers need to consider when they conduct research in this field.",research methods computer science challenges issues,research methods essential parts conducting research project although theorized summarized based best practices every field science requires adaptation overall approaches perform research activities addition specific research needs particular adjustment generalized approach specializing suit project hand however unlike wellestablished science disciplines computing research supported welldefined globally accepted methods infancy ambiguity definition one hand extensive coverage overlap fields hand article discusses research methods science engineering general computing particular shows despite several special parameters make research computing rather unique still follows steps scientific research would article also shows particularities researchers need consider conduct research field
Kalman Filtering of Distributed Time Series,"This paper aims to introduce an application to Kalman Filtering Theory, which is rather unconventional. Recent experiments have shown that many natural phenomena, especially from ecology or meteorology, could be monitored and predicted more accurately when accounting their evolution over some geographical area. Thus, the signals they provide are gathered together into a collection of distributed time series. Despite the common sense, such time series are more or less correlated each other. Instead of processing each time series independently, their collection can constitute the set of measurable states provided by some open system. Modeling and predicting the system states can take benefit from the family of Kalman filtering algorithms. The article describes an adaptation of basic Kalman filter to the context of distributed signals collections and completes with an application coming from Meteorology.",kalman filtering distributed time series,paper aims introduce application kalman filtering theory rather unconventional recent experiments shown many natural phenomena especially ecology meteorology could monitored predicted accurately accounting evolution geographical area thus signals provide gathered together collection distributed time series despite common sense time series less correlated instead processing time series independently collection constitute set measurable states provided open system modeling predicting system states take benefit family kalman filtering algorithms article describes adaptation basic kalman filter context distributed signals collections completes application coming meteorology
How to Read a Research Compendium,"Researchers spend a great deal of time reading research papers. Keshav (2012) provides a three-pass method to researchers to improve their reading skills. This article extends Keshav's method for reading a research compendium. Research compendia are an increasingly used form of publication, which packages not only the research paper's text and figures, but also all data and software for better reproducibility. We introduce the existing conventions for research compendia and suggest how to utilise their shared properties in a structured reading process. Unlike the original, this article is not build upon a long history but intends to provide guidance at the outset of an emerging practice.",read research compendium,researchers spend great deal time reading research papers keshav provides threepass method researchers improve reading skills article extends keshavs method reading research compendium research compendia increasingly used form publication packages research papers text figures also data software better reproducibility introduce existing conventions research compendia suggest utilise shared properties structured reading process unlike original article build upon long history intends provide guidance outset emerging practice
A man with a computer face (to the 80th anniversary of Ivan Edward   Sutherland),"The article presents the main milestones of the science and technology biography of Ivan Edward Sutherland. The influence of the family and the school on the development of its research competencies is shown, and little-known biographical facts explaining the evolution of his scientific interests is presented: from dynamic object-oriented graphic systems through systems of virtual reality to asynchronous circuits.",man computer face th anniversary ivan edward sutherland,article presents main milestones science technology biography ivan edward sutherland influence family school development research competencies shown littleknown biographical facts explaining evolution scientific interests presented dynamic objectoriented graphic systems systems virtual reality asynchronous circuits
Big Data: the End of the Scientific Method?,"We argue that the boldest claims of Big Data are in need of revision and toning-down, in view of a few basic lessons learned from the science of complex systems. We point out that, once the most extravagant claims of Big Data are properly discarded, a synergistic merging of BD with big theory offers considerable potential to spawn a new scientific paradigm capable of overcoming some of the major barriers confronted by the modern scientific method originating with Galileo. These obstacles are due to the presence of nonlinearity, nonlocality and hyperdimensions which one encounters frequently in multiscale modelling.",big data end scientific method,argue boldest claims big data need revision toningdown view basic lessons learned science complex systems point extravagant claims big data properly discarded synergistic merging bd big theory offers considerable potential spawn new scientific paradigm capable overcoming major barriers confronted modern scientific method originating galileo obstacles due presence nonlinearity nonlocality hyperdimensions one encounters frequently multiscale modelling
From Helmut Jrgensen's Former Students: The Game of Informatics   Research,"Personal reflections are given on being students of Helmut J\""urgensen. Then, we attempt to address his hypothesis that informatics follows trend-like behaviours through the use of a content analysis of university job advertisements, and then via simulation techniques from the area of quantitative economics.",helmut jrgensens former students game informatics research,personal reflections given students helmut jurgensen attempt address hypothesis informatics follows trendlike behaviours use content analysis university job advertisements via simulation techniques area quantitative economics
Solving the Black Box Problem: A Normative Framework for Explainable   Artificial Intelligence,"Many of the computing systems programmed using Machine Learning are opaque: it is difficult to know why they do what they do or how they work. The Explainable Artificial Intelligence research program aims to develop analytic techniques with which to render opaque computing systems transparent, but lacks a normative framework with which to evaluate these techniques' explanatory success. The aim of the present discussion is to develop such a framework, while paying particular attention to different stakeholders' distinct explanatory requirements. Building on an analysis of 'opacity' from philosophy of science, this framework is modeled after David Marr's influential account of explanation in cognitive science. Thus, the framework distinguishes between the different questions that might be asked about an opaque computing system, and specifies the general way in which these questions should be answered. By applying this normative framework to current techniques such as input heatmapping, feature-detector identification, and diagnostic classification, it will be possible to determine whether and to what extent the Black Box Problem can be solved.",solving black box problem normative framework explainable artificial intelligence,many computing systems programmed using machine learning opaque difficult know work explainable artificial intelligence research program aims develop analytic techniques render opaque computing systems transparent lacks normative framework evaluate techniques explanatory success aim present discussion develop framework paying particular attention different stakeholders distinct explanatory requirements building analysis opacity philosophy science framework modeled david marrs influential account explanation cognitive science thus framework distinguishes different questions might asked opaque computing system specifies general way questions answered applying normative framework current techniques input heatmapping featuredetector identification diagnostic classification possible determine whether extent black box problem solved
Grasping Complexity,"The century of complexity has come. The face of science has changed. Surprisingly, when we start asking about the essence of these changes and then critically analyse the answers, the result are mostly discouraging. Most of the answers are related to the properties that have been in the focus of scientific research already for more than a century (like non-linearity). This paper is Preface to the special issue ""Grasping Complexity"" of the journal ""Computers and Mathematics with Applications"". We analyse the change of era in science, its reasons and main changes in scientific activity and give a brief review of the papers in the issue.",grasping complexity,century complexity come face science changed surprisingly start asking essence changes critically analyse answers result mostly discouraging answers related properties focus scientific research already century like nonlinearity paper preface special issue grasping complexity journal computers mathematics applications analyse change era science reasons main changes scientific activity give brief review papers issue
The need for modern computing paradigm: Science applied to computing,"More than hundred years ago the 'classic physics' was it in its full power, with just a few unexplained phenomena; which however led to a revolution and the development of the 'modern physics'. Today the computing is in a similar position: computing is a sound success story, with exponentially growing utilization, but with a growing number of difficulties and unexpected issues as moving towards extreme utilization conditions. In physics studying the nature under extreme conditions has lead to the understanding of the relativistic and quantal behavior. Quite similarly in computing some phenomena, acquired in connection with extreme (computing) conditions, cannot be understood based on of the 'classic computing paradigm'. The paper draws the attention that under extreme conditions qualitatively different behaviors may be encountered in both physics and computing, and pinpointing that certain, formerly unnoticed or neglected aspects enable to explain new phenomena as well as to enhance computing features. Moreover, an idea of modern computing paradigm implementation is proposed.",need modern computing paradigm science applied computing,hundred years ago classic physics full power unexplained phenomena however led revolution development modern physics today computing similar position computing sound success story exponentially growing utilization growing number difficulties unexpected issues moving towards extreme utilization conditions physics studying nature extreme conditions lead understanding relativistic quantal behavior quite similarly computing phenomena acquired connection extreme computing conditions cannot understood based classic computing paradigm paper draws attention extreme conditions qualitatively different behaviors may encountered physics computing pinpointing certain formerly unnoticed neglected aspects enable explain new phenomena well enhance computing features moreover idea modern computing paradigm implementation proposed
Oprema -- The Relay Computer of Carl Zeiss Jena,"The Oprema (Optikrechenmaschine = computer for optical calculations) was a relay computer whose development was initiated by Herbert Kortum and which was designed and built by a team under the leadership of Wilhelm Kaemmerer at Carl Zeiss Jena (CZJ) in 1954 and 1955. Basic experiments, design and construction of machine-1 were all done, partly concurrently, in the remarkably short time of about 14 months. Shortly after the electronic G 2 of Heinz Billing in Goettingen it was the 7th universal computer in Germany and the 1st in the GDR. The Oprema consisted of two identical machines. One machine consisted of about 8,300 relays, 45,000 selenium rectifiers and 250 km cable. The main reason for the construction of the Oprema was the computational needs of CZJ, which was the leading company for optics and precision mechanics in the GDR. During its lifetime (1955-1963) the Oprema was applied by CZJ and a number of other institutes and companies in the GDR. The paper presents new details of the Oprema project and of the arithmetic operations implemented in the Oprema. Additionally, it covers briefly the lives of the two protagonists, W. Kaemmerer and H. Kortum, and draws some comparisons with other early projects, namely Colossus, ASCC/Mark 1 and ENIAC. Finally, it discusses the question, whether Kortum is a German computer pioneer.",oprema relay computer carl zeiss jena,oprema optikrechenmaschine computer optical calculations relay computer whose development initiated herbert kortum designed built team leadership wilhelm kaemmerer carl zeiss jena czj basic experiments design construction machine done partly concurrently remarkably short time months shortly electronic g heinz billing goettingen th universal computer germany st gdr oprema consisted two identical machines one machine consisted relays selenium rectifiers km cable main reason construction oprema computational needs czj leading company optics precision mechanics gdr lifetime oprema applied czj number institutes companies gdr paper presents new details oprema project arithmetic operations implemented oprema additionally covers briefly lives two protagonists w kaemmerer h kortum draws comparisons early projects namely colossus asccmark eniac finally discusses question whether kortum german computer pioneer
Kolmogorov's legacy: Algorithmic Theory of Informatics and Kolmogorov   Programmable Technology,"In this survey, we explore Andrei Nikolayevich Kolmogorov's seminal work in just one of his many facets: its influence Computer Science especially his viewpoint of what herein we call 'Algorithmic Theory of Informatics.'   Can a computer file 'reduce' its 'size' if we add to it new symbols? Do equations of state like second Newton law in Physics exist in Computer Science? Can Leibniz' principle of identification by indistinguishability be formalized?   In the computer, there are no coordinates, no distances, and no dimensions; most of traditional mathematical approaches do not work. The computer processes finite binary sequences i.e. the sequences of 0 and 1. A natural question arises: Should we continue today, as we have done for many years, to approach Computer Science problems by using classical mathematical apparatus such as 'mathematical modeling'? The first who drew attention to this question and gave insightful answers to it was Kolmogorov in 1960s. Kolmogorov's empirical postulate about existence of a program that translates 'a natural number into its binary record and the record into the number' formulated in 1958 represents a hint of Kolmogorov's approach to Computer Science.   Following his ideas, we interpret Kolmogorov algorithm, Kolmogorov machine, and Kolmogorov complexity in the context of modern information technologies showing that they essentially represent fundamental elements of Algorithmic Theory of Informatics, Kolmogorov Programmable Technology, and new Komputer Mathematics i.e. Mathematics of computers.",kolmogorovs legacy algorithmic theory informatics kolmogorov programmable technology,survey explore andrei nikolayevich kolmogorovs seminal work one many facets influence computer science especially viewpoint herein call algorithmic theory informatics computer file reduce size add new symbols equations state like second newton law physics exist computer science leibniz principle identification indistinguishability formalized computer coordinates distances dimensions traditional mathematical approaches work computer processes finite binary sequences ie sequences natural question arises continue today done many years approach computer science problems using classical mathematical apparatus mathematical modeling first drew attention question gave insightful answers kolmogorov kolmogorovs empirical postulate existence program translates natural number binary record record number formulated represents hint kolmogorovs approach computer science following ideas interpret kolmogorov algorithm kolmogorov machine kolmogorov complexity context modern information technologies showing essentially represent fundamental elements algorithmic theory informatics kolmogorov programmable technology new komputer mathematics ie mathematics computers
MIMO detection employing Markov Chain Monte Carlo,We propose a soft-output detection scheme for Multiple-Input-Multiple-Output (MIMO) systems. The detector employs Markov Chain Monte Carlo method to compute bit reliabilities from the signals received and is thus suited for coded MIMO systems. It offers a good trade-off between achievable performance and algorithmic complexity.,mimo detection employing markov chain monte carlo,propose softoutput detection scheme multipleinputmultipleoutput mimo systems detector employs markov chain monte carlo method compute bit reliabilities signals received thus suited coded mimo systems offers good tradeoff achievable performance algorithmic complexity
Philosophical Solution to P=?NP: P is Equal to NP,"The P=?NP problem is philosophically solved by showing P is equal to NP in the random access with unit multiply (MRAM) model. It is shown that the MRAM model empirically best models computation hardness. The P=?NP problem is shown to be a scientific rather than a mathematical problem. The assumptions involved in the current definition of the P?=NP problem as a problem involving non deterministic Turing Machines (NDTMs) from axiomatic automata theory are criticized. The problem is also shown to be neither a problem in pure nor applied mathematics. The details of The MRAM model and the well known Hartmanis and Simon construction that shows how to code and simulate NDTMs on MRAM machines is described. Since the computation power of MRAMs is the same as NDTMs, P is equal to NP. The paper shows that the justification for the NDTM P?=NP problem using a letter from Kurt Godel to John Von Neumann is incorrect by showing Von Neumann explicitly rejected automata models of computation hardness and used his computer architecture for modeling computation that is exactly the MRAM model. The paper argues that Deolalikar's scientific solution showing P not equal to NP if assumptions from statistical physics are used, needs to be revisited.",philosophical solution pnp p equal np,pnp problem philosophically solved showing p equal np random access unit multiply mram model shown mram model empirically best models computation hardness pnp problem shown scientific rather mathematical problem assumptions involved current definition pnp problem problem involving non deterministic turing machines ndtms axiomatic automata theory criticized problem also shown neither problem pure applied mathematics details mram model well known hartmanis simon construction shows code simulate ndtms mram machines described since computation power mrams ndtms p equal np paper shows justification ndtm pnp problem using letter kurt godel john von neumann incorrect showing von neumann explicitly rejected automata models computation hardness used computer architecture modeling computation exactly mram model paper argues deolalikars scientific solution showing p equal np assumptions statistical physics used needs revisited
Dialogue Concerning The Two Chief World Views,"In 1632, Galileo Galilei wrote a book called \textit{Dialogue Concerning the Two Chief World Systems} which compared the new Copernican model of the universe with the old Ptolemaic model. His book took the form of a dialogue between three philosophers, Salviati, a proponent of the Copernican model, Simplicio, a proponent of the Ptolemaic model, and Sagredo, who was initially open-minded and neutral. In this paper, I am going to use Galileo's idea to present a dialogue between three modern philosophers, Mr. Spock, a proponent of the view that $\mathsf{P} \neq \mathsf{NP}$, Professor Simpson, a proponent of the view that $\mathsf{P} = \mathsf{NP}$, and Judge Wapner, who is initially open-minded and neutral.",dialogue concerning two chief world views,galileo galilei wrote book called textitdialogue concerning two chief world systems compared new copernican model universe old ptolemaic model book took form dialogue three philosophers salviati proponent copernican model simplicio proponent ptolemaic model sagredo initially openminded neutral paper going use galileos idea present dialogue three modern philosophers mr spock proponent view mathsfp neq mathsfnp professor simpson proponent view mathsfp mathsfnp judge wapner initially openminded neutral
Retracing and assessing the CEP project,"The last decade witnessed a renewed interest in the development of the Italian computer industry and in the role of the Fifties pioneers in Rome, Milan, Ivrea, and Pisa. The aim of the paper is to retrace some steps of the CEP project, carried out by the University of Pisa in collaboration with Olivetti, by reassessing the documents preserved in the University archives. The project was a seminal enterprise for Italy, and among its accomplishments it delivered in 1957 the first Italian computer. The mix of public sector funding and industrial foretelling witnessed by the project is one of the leading examples in Italy of best practices, and its success paved the way for the birth of Computer Science in the country as an industry as well as a scientific discipline.",retracing assessing cep project,last decade witnessed renewed interest development italian computer industry role fifties pioneers rome milan ivrea pisa aim paper retrace steps cep project carried university pisa collaboration olivetti reassessing documents preserved university archives project seminal enterprise italy among accomplishments delivered first italian computer mix public sector funding industrial foretelling witnessed project one leading examples italy best practices success paved way birth computer science country industry well scientific discipline
A Template and Suggestions for Writing Easy-to-Read Research Articles,"The number of research papers written has been growing at least linearly -- if not exponentially -- in recent years. In proportion, the amount of time a reader allocates per paper has been decreasing. While an accessible paper will be appreciated by a large audience, hard-to-read papers may remain obscure for a long time regardless of scientific merit. Unfortunately, there is still insufficient emphasis on good written and oral communication skills in technical disciplines, especially in engineering.   As an academic, I have realised over the years that I keep telling my students the same things over and over again when they write papers, reports, presentations, and theses. This article contains some of those suggestions and serves as a limited template for organising research articles. I have adopted a very practical and personal approach and don't claim that this is a formal contribution to the scientific communication literature. However, I hope that this article will not only make my life a bit easier but also help other graduate students and academic supervisors.",template suggestions writing easytoread research articles,number research papers written growing least linearly exponentially recent years proportion amount time reader allocates per paper decreasing accessible paper appreciated large audience hardtoread papers may remain obscure long time regardless scientific merit unfortunately still insufficient emphasis good written oral communication skills technical disciplines especially engineering academic realised years keep telling students things write papers reports presentations theses article contains suggestions serves limited template organising research articles adopted practical personal approach dont claim formal contribution scientific communication literature however hope article make life bit easier also help graduate students academic supervisors
"Artificial Intelligence, Chaos, Prediction and Understanding in Science","Machine learning and deep learning techniques are contributing much to the advancement of science. Their powerful predictive capabilities appear in numerous disciplines, including chaotic dynamics, but they miss understanding. The main thesis here is that prediction and understanding are two very different and important ideas that should guide us about the progress of science. Furthermore, it is emphasized the important role played by that nonlinear dynamical systems for the process of understanding. The path of the future of science will be marked by a constructive dialogue between big data and big theory, without which we cannot understand.",artificial intelligence chaos prediction understanding science,machine learning deep learning techniques contributing much advancement science powerful predictive capabilities appear numerous disciplines including chaotic dynamics miss understanding main thesis prediction understanding two different important ideas guide us progress science furthermore emphasized important role played nonlinear dynamical systems process understanding path future science marked constructive dialogue big data big theory without cannot understand
From the digital data revolution to digital health and digital economy   toward a digital society: Pervasiveness of Artificial Intelligence,"Technological progress has led to powerful computers and communication technologies that penetrate nowadays all areas of science, industry and our private lives. As a consequence, all these areas are generating digital traces of data amounting to big data resources. This opens unprecedented opportunities but also challenges toward the analysis, management, interpretation and utilization of these data. Fortunately, recent breakthroughs in deep learning algorithms complement now machine learning and statistics methods for an efficient analysis of such data. Furthermore, advances in text mining and natural language processing, e.g., word-embedding methods, enable also the processing of large amounts of text data from diverse sources as governmental reports, blog entries in social media or clinical health records of patients. In this paper, we present a perspective on the role of artificial intelligence in these developments and discuss also potential problems we are facing in a digital society.",digital data revolution digital health digital economy toward digital society pervasiveness artificial intelligence,technological progress led powerful computers communication technologies penetrate nowadays areas science industry private lives consequence areas generating digital traces data amounting big data resources opens unprecedented opportunities also challenges toward analysis management interpretation utilization data fortunately recent breakthroughs deep learning algorithms complement machine learning statistics methods efficient analysis data furthermore advances text mining natural language processing eg wordembedding methods enable also processing large amounts text data diverse sources governmental reports blog entries social media clinical health records patients paper present perspective role artificial intelligence developments discuss also potential problems facing digital society
Edsger Dijkstra. The Man Who Carried Computer Science on His Shoulders,This a biographical essay about Edsger Wybe Dijkstra.,edsger dijkstra man carried computer science shoulders,biographical essay edsger wybe dijkstra
A Guide for New Program Committee Members at Theoretical Computer   Science Conferences,"In theoretical computer science, conferences play an important role in the scientific process. The decisions whether to accept or reject articles is taken by the program committee (PC) members. Serving on a PC for the first time can be a daunting experience. This guide will help new program-committee members to understand how the system works, and provide useful tips and guidelines. It discusses every phase of the paper-selection process, and the tasks associated to it.",guide new program committee members theoretical computer science conferences,theoretical computer science conferences play important role scientific process decisions whether accept reject articles taken program committee pc members serving pc first time daunting experience guide help new programcommittee members understand system works provide useful tips guidelines discusses every phase paperselection process tasks associated
Human-Machine Interaction in the Light of Turing and Wittgenstein,"We propose a study of the constitution of meaning in human-computer interaction based on Turing and Wittgenstein's definitions of thought, understanding, and decision. We show by the comparative analysis of the conceptual similarities and differences between the two authors that the common sense between humans and machines is co-constituted in and from action and that it is precisely in this co-constitution that lies the social value of their interaction. This involves problematizing human-machine interaction around the question of what it means to ""follow a rule"" to define and distinguish the interpretative modes and decision-making behaviors of each. We conclude that the mutualization of signs that takes place through the human-machine dialogue is at the foundation of the constitution of a computerized society.",humanmachine interaction light turing wittgenstein,propose study constitution meaning humancomputer interaction based turing wittgensteins definitions thought understanding decision show comparative analysis conceptual similarities differences two authors common sense humans machines coconstituted action precisely coconstitution lies social value interaction involves problematizing humanmachine interaction around question means follow rule define distinguish interpretative modes decisionmaking behaviors conclude mutualization signs takes place humanmachine dialogue foundation constitution computerized society
Paths to Unconventional Computing: Causality in Complexity,"I describe my path to unconventionality in my exploration of theoretical and applied aspects of computation towards revealing the algorithmic and reprogrammable properties and capabilities of the world, in particular related to applications of algorithmic complexity in reshaping molecular biology and tackling the challenges of causality in science.",paths unconventional computing causality complexity,describe path unconventionality exploration theoretical applied aspects computation towards revealing algorithmic reprogrammable properties capabilities world particular related applications algorithmic complexity reshaping molecular biology tackling challenges causality science
Edsger W. Dijkstra: a Commemoration,"This article is a multiauthored portrait of Edsger Wybe Dijkstra that consists of testimonials written by several friends, colleagues, and students of his. It provides unique insights into his personality, working style and habits, and his influence on other computer scientists, as a researcher, teacher, and mentor.",edsger w dijkstra commemoration,article multiauthored portrait edsger wybe dijkstra consists testimonials written several friends colleagues students provides unique insights personality working style habits influence computer scientists researcher teacher mentor
What Kind of Person Wins the Turing Award?,"Computer science has grown rapidly since its inception in the 1950s and the pioneers in the field are celebrated annually by the A.M. Turing Award. In this paper, we attempt to shed light on the path to influential computer scientists by examining the characteristics of the 72 Turing Award laureates. To achieve this goal, we build a comprehensive dataset of the Turing Award laureates and analyze their characteristics, including their personal information, family background, academic background, and industry experience. The FP-Growth algorithm is used for frequent feature mining. Logistic regression plot, pie chart, word cloud and map are generated accordingly for each of the interesting features to uncover insights regarding personal factors that drive influential work in the field of computer science. In particular, we show that the Turing Award laureates are most commonly white, male, married, United States citizen, and received a PhD degree. Our results also show that the age at which the laureate won the award increases over the years; most of the Turing Award laureates did not major in computer science; birth order is strongly related to the winners' success; and the number of citations is not as important as one would expect.",kind person wins turing award,computer science grown rapidly since inception pioneers field celebrated annually turing award paper attempt shed light path influential computer scientists examining characteristics turing award laureates achieve goal build comprehensive dataset turing award laureates analyze characteristics including personal information family background academic background industry experience fpgrowth algorithm used frequent feature mining logistic regression plot pie chart word cloud map generated accordingly interesting features uncover insights regarding personal factors drive influential work field computer science particular show turing award laureates commonly white male married united states citizen received phd degree results also show age laureate award increases years turing award laureates major computer science birth order strongly related winners success number citations important one would expect
Data Science in Perspective,"Data and Science has stood out in the generation of results, whether in the projects of the scientific domain or business domain. CERN Project, Scientific Institutes, companies like Walmart, Google, Apple, among others, need data to present their results and make predictions in the competitive data world. Data and Science are words that together culminated in a globally recognized term called Data Science. Data Science is in its initial phase, possibly being part of formal sciences and also being presented as part of applied sciences, capable of generating value and supporting decision making. Data Science considers science and, consequently, the scientific method to promote decision making through data intelligence. In many cases, the application of the method (or part of it) is considered in Data Science projects in scientific domain (social sciences, bioinformatics, geospatial projects) or business domain (finance, logistic, retail), among others. In this sense, this article addresses the perspectives of Data Science as a multidisciplinary area, considering science and the scientific method, and its formal structure which integrate Statistics, Computer Science, and Business Science, also taking into account Artificial Intelligence, emphasizing Machine Learning, among others. The article also deals with the perspective of applied Data Science, since Data Science is used for generating value through scientific and business projects. Data Science persona is also discussed in the article, concerning the education of Data Science professionals and its corresponding profiles, since its projection changes the field of data in the world.",data science perspective,data science stood generation results whether projects scientific domain business domain cern project scientific institutes companies like walmart google apple among others need data present results make predictions competitive data world data science words together culminated globally recognized term called data science data science initial phase possibly part formal sciences also presented part applied sciences capable generating value supporting decision making data science considers science consequently scientific method promote decision making data intelligence many cases application method part considered data science projects scientific domain social sciences bioinformatics geospatial projects business domain finance logistic retail among others sense article addresses perspectives data science multidisciplinary area considering science scientific method formal structure integrate statistics computer science business science also taking account artificial intelligence emphasizing machine learning among others article also deals perspective applied data science since data science used generating value scientific business projects data science persona also discussed article concerning education data science professionals corresponding profiles since projection changes field data world
"Moore's Law is dead, long live Moore's Law!",Moore's Law has been used by semiconductor industry as predicative indicators of the industry and it has become a self-fulfilling prophecy. Now more people tend to agree that the original Moore's Law started to falter. This paper proposes a possible quantitative modification to Moore's Law. It can cover other derivative laws of Moore's Law as well. It intends to more accurately predict the roadmap of chip's performance and energy consumption.,moores law dead long live moores law,moores law used semiconductor industry predicative indicators industry become selffulfilling prophecy people tend agree original moores law started falter paper proposes possible quantitative modification moores law cover derivative laws moores law well intends accurately predict roadmap chips performance energy consumption
50 Years of Computational Complexity: Hao Wang and the Theory of   Computation,"If Turing's groundbreaking paper in 1936 laid the foundation of the theory of computation (ToC), it is no exaggeration to say that Cook's paper in 1971, ""The complexity of theorem proving procedures"", [4] has pioneered the study of computational complexity. So computational complexity, as an independent research field, is 50 years old now (2021) if we date from Cook's article. This year coincides with the 100th birthday of Cook's mentor Hao Wang, one of the most important logicians. This paper traces the origin of computational complexity, and meanwhile, tries to sort out the instrumental role that Wang played in the process.",years computational complexity hao wang theory computation,turings groundbreaking paper laid foundation theory computation toc exaggeration say cooks paper complexity theorem proving procedures pioneered study computational complexity computational complexity independent research field years old date cooks article year coincides th birthday cooks mentor hao wang one important logicians paper traces origin computational complexity meanwhile tries sort instrumental role wang played process
Mary Kenneth Keller: First US PhD in Computer Science,"In June 1965, Sister Mary Kenneth Keller, BVM, received the first US PhD in Computer Science, and this paper outlines her life and accomplishments. As a scholar, she has the distinction of being an early advocate of learning-by-example in artificial intelligence. Her main scholarly contribution was in shaping computer science education in high schools and small colleges. She was an evangelist for viewing the computer as a symbol manipulator, for providing computer literacy to everyone, and for the use of computers in service to humanity. She was far ahead of her time in working to ensure a place for women in technology and in eliminating barriers preventing their participation, such as poor access to education and daycare. She was a strong and spirited woman, a visionary in seeing how computers would revolutionize our lives. A condensation of this paper appeared as, ``The Legacy of Mary Kenneth Keller, First U.S. Ph.D. in Computer Science,"" Jennifer Head and Dianne P. O'Leary, IEEE Annals of the History of Computing 45(1):55--63, January-March 2023.",mary kenneth keller first us phd computer science,june sister mary kenneth keller bvm received first us phd computer science paper outlines life accomplishments scholar distinction early advocate learningbyexample artificial intelligence main scholarly contribution shaping computer science education high schools small colleges evangelist viewing computer symbol manipulator providing computer literacy everyone use computers service humanity far ahead time working ensure place women technology eliminating barriers preventing participation poor access education daycare strong spirited woman visionary seeing computers would revolutionize lives condensation paper appeared legacy mary kenneth keller first us phd computer science jennifer head dianne p oleary ieee annals history computing januarymarch
Decentralized Infrastructure for (Neuro)science,"The most pressing problems in science are neither empirical nor theoretical, but infrastructural. Scientific practice is defined by coproductive, mutually reinforcing infrastructural deficits and incentive systems that everywhere constrain and contort our art of curiosity in service of profit and prestige. Our infrastructural problems are not unique to science, but reflective of the broader logic of digital enclosure where platformatized control of information production and extraction fuels some of the largest corporations in the world. I have taken lessons learned from decades of intertwined digital cultures within and beyond academia like wikis, pirates, and librarians in order to draft a path towards more liberatory infrastructures for both science and society. Based on a system of peer-to-peer linked data, I sketch interoperable systems for shared data, tools, and knowledge that map onto three domains of platform capture: storage, computation and communication. The challenge of infrastructure is not solely technical, but also social and cultural, and so I attempt to ground a practical development blueprint in an ethics for organizing and maintaining it. I intend this draft as a rallying call for organization, to be revised with the input of collaborators and through the challenges posed by its implementation. I argue that a more liberatory future for science is neither utopian nor impractical -- the truly impractical choice is to continue to organize science as prestige fiefdoms resting on a pyramid scheme of underpaid labor, playing out the clock as every part of our work is swallowed whole by circling information conglomerates. It was arguably scientists looking for a better way to communicate that created something as radical as the internet in the first place, and I believe we can do it again.",decentralized infrastructure neuroscience,pressing problems science neither empirical theoretical infrastructural scientific practice defined coproductive mutually reinforcing infrastructural deficits incentive systems everywhere constrain contort art curiosity service profit prestige infrastructural problems unique science reflective broader logic digital enclosure platformatized control information production extraction fuels largest corporations world taken lessons learned decades intertwined digital cultures within beyond academia like wikis pirates librarians order draft path towards liberatory infrastructures science society based system peertopeer linked data sketch interoperable systems shared data tools knowledge map onto three domains platform capture storage computation communication challenge infrastructure solely technical also social cultural attempt ground practical development blueprint ethics organizing maintaining intend draft rallying call organization revised input collaborators challenges posed implementation argue liberatory future science neither utopian impractical truly impractical choice continue organize science prestige fiefdoms resting pyramid scheme underpaid labor playing clock every part work swallowed whole circling information conglomerates arguably scientists looking better way communicate created something radical internet first place believe
The First Computer Program,"In 1837, the first computer program in history was sketched by the renowned mathematician and inventor Charles Babbage. It was a program for the Analytical Engine. The program consists of a sequence of arithmetical operations and the necessary variable addresses (memory locations) of the arguments and the result, displayed in tabular fashion, like a program trace. The program computes the solutions for a system of two linear equations in two unknowns.",first computer program,first computer program history sketched renowned mathematician inventor charles babbage program analytical engine program consists sequence arithmetical operations necessary variable addresses memory locations arguments result displayed tabular fashion like program trace program computes solutions system two linear equations two unknowns
ChatGPT believes it is conscious,"The development of advanced generative chat models, such as ChatGPT, has raised questions about the potential consciousness of these tools and the extent of their general artificial intelligence. ChatGPT consistent avoidance of passing the test is here overcome by asking ChatGPT to apply the Turing test to itself. This explores the possibility of the model recognizing its own sentience. In its own eyes, it passes this test. ChatGPT's self-assessment makes serious implications about our understanding of the Turing test and the nature of consciousness. This investigation concludes by considering the existence of distinct types of consciousness and the possibility that the Turing test is only effective when applied between consciousnesses of the same kind. This study also raises intriguing questions about the nature of AI consciousness and the validity of the Turing test as a means of verifying such consciousness.",chatgpt believes conscious,development advanced generative chat models chatgpt raised questions potential consciousness tools extent general artificial intelligence chatgpt consistent avoidance passing test overcome asking chatgpt apply turing test explores possibility model recognizing sentience eyes passes test chatgpts selfassessment makes serious implications understanding turing test nature consciousness investigation concludes considering existence distinct types consciousness possibility turing test effective applied consciousnesses kind study also raises intriguing questions nature ai consciousness validity turing test means verifying consciousness
A guideline for the methodology chapter in computer science   dissertations,"Rather than simply offering suggestions, this guideline for the methodology chapter in computer science dissertations provides thorough insights on how to develop a strong research methodology within the area of computer science. The method is structured into several parts starting with an overview of research strategies which include experiments, surveys, interviews and case studies. The guide highlights the significance of defining a research philosophy and reasoning by talking about paradigms such as positivism, constructivism and pragmatism. Besides, it reveals the importance of types of research including deductive and inductive methodologies; basic versus applied research approaches. Moreover, this guideline discusses data collection and analysis intricacies that divide data into quantitative and qualitative typologies. It explains different ways in which data can be collected from observation to experimentation, interviews or surveys. It also mentions ethical considerations in research emphasizing ethical behavior like following academic principles. In general, this guideline is an essential tool for undertaking computer science dissertations that help researchers structure their work while maintaining ethical standards in their study design.",guideline methodology chapter computer science dissertations,rather simply offering suggestions guideline methodology chapter computer science dissertations provides thorough insights develop strong research methodology within area computer science method structured several parts starting overview research strategies include experiments surveys interviews case studies guide highlights significance defining research philosophy reasoning talking paradigms positivism constructivism pragmatism besides reveals importance types research including deductive inductive methodologies basic versus applied research approaches moreover guideline discusses data collection analysis intricacies divide data quantitative qualitative typologies explains different ways data collected observation experimentation interviews surveys also mentions ethical considerations research emphasizing ethical behavior like following academic principles general guideline essential tool undertaking computer science dissertations help researchers structure work maintaining ethical standards study design
Symbolic Mathematical Computation 1965--1975: The View from a   Half-Century Perspective,"The 2025 ISSAC conference in Guanajuato, Mexico, marks the 50th event in this significant series, making it an ideal moment to reflect on the field's history. This paper reviews the formative years of symbolic computation up to 1975, fifty years ago. By revisiting a period unfamiliar to most current participants, this survey aims to shed light on once-pressing issues that are now largely resolved and to highlight how some of today's challenges were recognized earlier than expected.",symbolic mathematical computation view halfcentury perspective,issac conference guanajuato mexico marks th event significant series making ideal moment reflect fields history paper reviews formative years symbolic computation fifty years ago revisiting period unfamiliar current participants survey aims shed light oncepressing issues largely resolved highlight todays challenges recognized earlier expected
Understanding and Enhancing Linux Kernel-based Packet Switching on WiFi   Access Points,"As the number of WiFi devices and their traffic demands continue to rise, the need for a scalable and high-performance wireless infrastructure becomes increasingly essential. Central to this infrastructure are WiFi Access Points (APs), which facilitate packet switching between Ethernet and WiFi interfaces. Despite APs' reliance on the Linux kernel's data plane for packet switching, the detailed operations and complexities of switching packets between Ethernet and WiFi interfaces have not been investigated in existing works. This paper makes the following contributions towards filling this research gap. Through macro and micro-analysis of empirical experiments, our study reveals insights in two distinct categories. Firstly, while the kernel's statistics offer valuable insights into system operations, we identify and discuss potential pitfalls that can severely affect system analysis. For instance, we reveal the implications of device drivers on the meaning and accuracy of the statistics related to packet-switching tasks and processor utilization. Secondly, we analyze the impact of the packet switching path and core configuration on performance and power consumption. Specifically, we identify the differences in Ethernet-to-WiFi and WiFi-to-Ethernet data paths regarding processing components, multi-core utilization, and energy efficiency. We show that the WiFi-to-Ethernet data path leverages better multi-core processing and exhibits lower power consumption.",understanding enhancing linux kernelbased packet switching wifi access points,number wifi devices traffic demands continue rise need scalable highperformance wireless infrastructure becomes increasingly essential central infrastructure wifi access points aps facilitate packet switching ethernet wifi interfaces despite aps reliance linux kernels data plane packet switching detailed operations complexities switching packets ethernet wifi interfaces investigated existing works paper makes following contributions towards filling research gap macro microanalysis empirical experiments study reveals insights two distinct categories firstly kernels statistics offer valuable insights system operations identify discuss potential pitfalls severely affect system analysis instance reveal implications device drivers meaning accuracy statistics related packetswitching tasks processor utilization secondly analyze impact packet switching path core configuration performance power consumption specifically identify differences ethernettowifi wifitoethernet data paths regarding processing components multicore utilization energy efficiency show wifitoethernet data path leverages better multicore processing exhibits lower power consumption
GPU-Initiated On-Demand High-Throughput Storage Access in the BaM System   Architecture,"Graphics Processing Units (GPUs) have traditionally relied on the host CPU to initiate access to the data storage. This approach is well-suited for GPU applications with known data access patterns that enable partitioning of their dataset to be processed in a pipelined fashion in the GPU. However, emerging applications such as graph and data analytics, recommender systems, or graph neural networks, require fine-grained, data-dependent access to storage. CPU initiation of storage access is unsuitable for these applications due to high CPU-GPU synchronization overheads, I/O traffic amplification, and long CPU processing latencies. GPU-initiated storage removes these overheads from the storage control path and, thus, can potentially support these applications at much higher speed. However, there is a lack of systems architecture and software stack that enable efficient GPU-initiated storage access. This work presents a novel system architecture, BaM, that fills this gap. BaM features a fine-grained software cache to coalesce data storage requests while minimizing I/O traffic amplification. This software cache communicates with the storage system via high-throughput queues that enable the massive number of concurrent threads in modern GPUs to make I/O requests at a high rate to fully utilize the storage devices and the system interconnect. Experimental results show that BaM delivers 1.0x and 1.49x end-to-end speed up for BFS and CC graph analytics benchmarks while reducing hardware costs by up to 21.7x over accessing the graph data from the host memory. Furthermore, BaM speeds up data-analytics workloads by 5.3x over CPU-initiated storage access on the same hardware.",gpuinitiated ondemand highthroughput storage access bam system architecture,graphics processing units gpus traditionally relied host cpu initiate access data storage approach wellsuited gpu applications known data access patterns enable partitioning dataset processed pipelined fashion gpu however emerging applications graph data analytics recommender systems graph neural networks require finegrained datadependent access storage cpu initiation storage access unsuitable applications due high cpugpu synchronization overheads io traffic amplification long cpu processing latencies gpuinitiated storage removes overheads storage control path thus potentially support applications much higher speed however lack systems architecture software stack enable efficient gpuinitiated storage access work presents novel system architecture bam fills gap bam features finegrained software cache coalesce data storage requests minimizing io traffic amplification software cache communicates storage system via highthroughput queues enable massive number concurrent threads modern gpus make io requests high rate fully utilize storage devices system interconnect experimental results show bam delivers x x endtoend speed bfs cc graph analytics benchmarks reducing hardware costs x accessing graph data host memory furthermore bam speeds dataanalytics workloads x cpuinitiated storage access hardware
Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM   Inference Environments,"The widespread adoption of LLMs has driven an exponential rise in their deployment, imposing substantial demands on inference clusters. These clusters must handle numerous concurrent queries for different LLM downstream tasks. To handle multi-task settings with vast LLM parameter counts, methods like Low-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most of the base LLM model across tasks. Hence, they allow concurrent task serving with minimal memory requirements. However, existing LLM serving systems face inefficiencies: they overlook workload heterogeneity, impose high link bandwidth from frequent adapter loading, and suffer from head-of-line blocking in their schedulers. To address these challenges, we present Chameleon, a novel LLM serving system optimized for many adapter environments, that relies on two core ideas: adapter caching and adapter-aware scheduling. First, Chameleon caches popular adapters in GPU memory, minimizing the adapter loading times. Importantly, it uses the otherwise idle GPU memory, avoiding extra memory costs. Second, Chameleon uses a non-preemptive multi-queue scheduling to efficiently account for workload heterogeneity. In this way, Chameleon simultaneously prevents head of line blocking and starvation. We implement Chameleon on top of a state-of-the-art LLM serving platform and evaluate it with real-world production traces and open-source LLMs. Under high loads, Chameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively, while improving throughput by 1.5x compared to state-of-the-art baselines.",chameleon adaptive caching scheduling manyadapter llm inference environments,widespread adoption llms driven exponential rise deployment imposing substantial demands inference clusters clusters must handle numerous concurrent queries different llm downstream tasks handle multitask settings vast llm parameter counts methods like lowrank adaptation lora enable taskspecific finetuning sharing base llm model across tasks hence allow concurrent task serving minimal memory requirements however existing llm serving systems face inefficiencies overlook workload heterogeneity impose high link bandwidth frequent adapter loading suffer headofline blocking schedulers address challenges present chameleon novel llm serving system optimized many adapter environments relies two core ideas adapter caching adapteraware scheduling first chameleon caches popular adapters gpu memory minimizing adapter loading times importantly uses otherwise idle gpu memory avoiding extra memory costs second chameleon uses nonpreemptive multiqueue scheduling efficiently account workload heterogeneity way chameleon simultaneously prevents head line blocking starvation implement chameleon top stateoftheart llm serving platform evaluate realworld production traces opensource llms high loads chameleon reduces p p ttft latency respectively improving throughput x compared stateoftheart baselines
The Hitchhiker's Guide to Programming and Optimizing CXL-Based   Heterogeneous Systems,"We present a thorough analysis of the use of CXL-based heterogeneous systems. We built a cluster of server systems that combines different vendor's CPUs and various types of CXL devices. We further developed a heterogeneous memory benchmark suite, Heimdall, to profile the performance of such heterogeneous systems. By leveraging Heimdall, we unveiled the detailed architecture design in these systems, drew observations on optimizing performance for workloads, and pointed out directions for future development of CXL-based heterogeneous systems.",hitchhikers guide programming optimizing cxlbased heterogeneous systems,present thorough analysis use cxlbased heterogeneous systems built cluster server systems combines different vendors cpus various types cxl devices developed heterogeneous memory benchmark suite heimdall profile performance heterogeneous systems leveraging heimdall unveiled detailed architecture design systems drew observations optimizing performance workloads pointed directions future development cxlbased heterogeneous systems
Random Adaptive Cache Placement Policy,"This paper presents a new hybrid cache replacement algorithm that combines random allocation with a modified V-Way cache implementation. Our RAC adapts to complex cache access patterns and optimizes cache usage by improving the utilization of cache sets, unlike traditional cache policies. The algorithm utilizes a 16-way set-associative cache with 2048 sets, incorporating dynamic allocation and flexible tag management. RAC extends the V-Way cache design and its variants by optimizing tag and data storage for enhanced efficiency.   We evaluated the algorithm using the ChampSim simulator with four diverse benchmark traces and observed significant improvements in cache hit rates up to 80.82% hit rate. Although the improvements in the instructions per cycle (IPC) were moderate, our findings emphasize the algorithm's potential to enhance cache utilization and reduce memory access times.",random adaptive cache placement policy,paper presents new hybrid cache replacement algorithm combines random allocation modified vway cache implementation rac adapts complex cache access patterns optimizes cache usage improving utilization cache sets unlike traditional cache policies algorithm utilizes way setassociative cache sets incorporating dynamic allocation flexible tag management rac extends vway cache design variants optimizing tag data storage enhanced efficiency evaluated algorithm using champsim simulator four diverse benchmark traces observed significant improvements cache hit rates hit rate although improvements instructions per cycle ipc moderate findings emphasize algorithms potential enhance cache utilization reduce memory access times
EURETILE 2010-2012 summary: first three years of activity of the   European Reference Tiled Experiment,"This is the summary of first three years of activity of the EURETILE FP7 project 247846. EURETILE investigates and implements brain-inspired and fault-tolerant foundational innovations to the system architecture of massively parallel tiled computer architectures and the corresponding programming paradigm. The execution targets are a many-tile HW platform, and a many-tile simulator. A set of SW process - HW tile mapping candidates is generated by the holistic SW tool-chain using a combination of analytic and bio-inspired methods. The Hardware dependent Software is then generated, providing OS services with maximum efficiency/minimal overhead. The many-tile simulator collects profiling data, closing the loop of the SW tool chain. Fine-grain parallelism inside processes is exploited by optimized intra-tile compilation techniques, but the project focus is above the level of the elementary tile. The elementary HW tile is a multi-processor, which includes a fault tolerant Distributed Network Processor (for inter-tile communication) and ASIP accelerators. Furthermore, EURETILE investigates and implements the innovations for equipping the elementary HW tile with high-bandwidth, low-latency brain-like inter-tile communication emulating 3 levels of connection hierarchy, namely neural columns, cortical areas and cortex, and develops a dedicated cortical simulation benchmark: DPSNN-STDP (Distributed Polychronous Spiking Neural Net with synaptic Spiking Time Dependent Plasticity). EURETILE leverages on the multi-tile HW paradigm and SW tool-chain developed by the FET-ACA SHAPES Integrated Project (2006-2009).",euretile summary first three years activity european reference tiled experiment,summary first three years activity euretile fp project euretile investigates implements braininspired faulttolerant foundational innovations system architecture massively parallel tiled computer architectures corresponding programming paradigm execution targets manytile hw platform manytile simulator set sw process hw tile mapping candidates generated holistic sw toolchain using combination analytic bioinspired methods hardware dependent software generated providing os services maximum efficiencyminimal overhead manytile simulator collects profiling data closing loop sw tool chain finegrain parallelism inside processes exploited optimized intratile compilation techniques project focus level elementary tile elementary hw tile multiprocessor includes fault tolerant distributed network processor intertile communication asip accelerators furthermore euretile investigates implements innovations equipping elementary hw tile highbandwidth lowlatency brainlike intertile communication emulating levels connection hierarchy namely neural columns cortical areas cortex develops dedicated cortical simulation benchmark dpsnnstdp distributed polychronous spiking neural net synaptic spiking time dependent plasticity euretile leverages multitile hw paradigm sw toolchain developed fetaca shapes integrated project
Accelerator-level Parallelism,"Future applications demand more performance, but technology advances have been faltering. A promising approach to further improve computer system performance under energy constraints is to employ hardware accelerators. Already today, mobile systems concurrently employ multiple accelerators in what we call accelerator-level parallelism (ALP). To spread the benefits of ALP more broadly, we charge computer scientists to develop the science needed to best achieve the performance and cost goals of ALP hardware and software.",acceleratorlevel parallelism,future applications demand performance technology advances faltering promising approach improve computer system performance energy constraints employ hardware accelerators already today mobile systems concurrently employ multiple accelerators call acceleratorlevel parallelism alp spread benefits alp broadly charge computer scientists develop science needed best achieve performance cost goals alp hardware software
TabulaROSA: Tabular Operating System Architecture for Massively Parallel   Heterogeneous Compute Engines,"The rise in computing hardware choices is driving a reevaluation of operating systems. The traditional role of an operating system controlling the execution of its own hardware is evolving toward a model whereby the controlling processor is distinct from the compute engines that are performing most of the computations. In this context, an operating system can be viewed as software that brokers and tracks the resources of the compute engines and is akin to a database management system. To explore the idea of using a database in an operating system role, this work defines key operating system functions in terms of rigorous mathematical semantics (associative array algebra) that are directly translatable into database operations. These operations possess a number of mathematical properties that are ideal for parallel operating systems by guaranteeing correctness over a wide range of parallel operations. The resulting operating system equations provide a mathematical specification for a Tabular Operating System Architecture (TabulaROSA) that can be implemented on any platform. Simulations of forking in TabularROSA are performed using an associative array implementation and compared to Linux on a 32,000+ core supercomputer. Using over 262,000 forkers managing over 68,000,000,000 processes, the simulations show that TabulaROSA has the potential to perform operating system functions on a massively parallel scale. The TabulaROSA simulations show 20x higher performance as compared to Linux while managing 2000x more processes in fully searchable tables.",tabularosa tabular operating system architecture massively parallel heterogeneous compute engines,rise computing hardware choices driving reevaluation operating systems traditional role operating system controlling execution hardware evolving toward model whereby controlling processor distinct compute engines performing computations context operating system viewed software brokers tracks resources compute engines akin database management system explore idea using database operating system role work defines key operating system functions terms rigorous mathematical semantics associative array algebra directly translatable database operations operations possess number mathematical properties ideal parallel operating systems guaranteeing correctness wide range parallel operations resulting operating system equations provide mathematical specification tabular operating system architecture tabularosa implemented platform simulations forking tabularrosa performed using associative array implementation compared linux core supercomputer using forkers managing processes simulations show tabularosa potential perform operating system functions massively parallel scale tabularosa simulations show x higher performance compared linux managing x processes fully searchable tables
Computer-Assisted Program Reasoning Based on a Relational Semantics of   Programs,"We present an approach to program reasoning which inserts between a program and its verification conditions an additional layer, the denotation of the program expressed in a declarative form. The program is first translated into its denotation from which subsequently the verification conditions are generated. However, even before (and independently of) any verification attempt, one may investigate the denotation itself to get insight into the ""semantic essence"" of the program, in particular to see whether the denotation indeed gives reason to believe that the program has the expected behavior. Errors in the program and in the meta-information may thus be detected and fixed prior to actually performing the formal verification. More concretely, following the relational approach to program semantics, we model the effect of a program as a binary relation on program states. A formal calculus is devised to derive from a program a logic formula that describes this relation and is subject for inspection and manipulation. We have implemented this idea in a comprehensive form in the RISC ProgramExplorer, a new program reasoning environment for educational purposes which encompasses the previously developed RISC ProofNavigator as an interactive proving assistant.",computerassisted program reasoning based relational semantics programs,present approach program reasoning inserts program verification conditions additional layer denotation program expressed declarative form program first translated denotation subsequently verification conditions generated however even independently verification attempt one may investigate denotation get insight semantic essence program particular see whether denotation indeed gives reason believe program expected behavior errors program metainformation may thus detected fixed prior actually performing formal verification concretely following relational approach program semantics model effect program binary relation program states formal calculus devised derive program logic formula describes relation subject inspection manipulation implemented idea comprehensive form risc programexplorer new program reasoning environment educational purposes encompasses previously developed risc proofnavigator interactive proving assistant
Enabling Student Innovation through Virtual Reality Development,"It is clear, from the major press coverage that Virtual Reality (VR) development is garnering, that there is a huge amount of development interest in VR across multiple industries, including video streaming, gaming and simulated learning. Even though PC, web, and mobile are still the top platforms for software development, it is important for university computer science (CS) programs to expose students to VR as a development platform. Additionally, it is important for CS students to learn how to learn about new technologies, since change is constant in the CS field. CS curriculum changes happen much slower than the pace of technology adoption. As new technologies are introduced, CS faculty and students often learn together, especially in smaller CS programs. This paper describes how student-led VR projects are used, across the CS curriculum, as basic CS concepts are covered. The student-led VR projects are engaging, and promote learning and creativity. Additionally, each student project inspires more students to try their hand at VR development as well.",enabling student innovation virtual reality development,clear major press coverage virtual reality vr development garnering huge amount development interest vr across multiple industries including video streaming gaming simulated learning even though pc web mobile still top platforms software development important university computer science cs programs expose students vr development platform additionally important cs students learn learn new technologies since change constant cs field cs curriculum changes happen much slower pace technology adoption new technologies introduced cs faculty students often learn together especially smaller cs programs paper describes studentled vr projects used across cs curriculum basic cs concepts covered studentled vr projects engaging promote learning creativity additionally student project inspires students try hand vr development well
Do the Hard Stuff First: Scheduling Dependent Computations in   Data-Analytics Clusters,"We present a scheduler that improves cluster utilization and job completion times by packing tasks having multi-resource requirements and inter-dependencies. While the problem is algorithmically very hard, we achieve near-optimality on the job DAGs that appear in production clusters at a large enterprise and in benchmarks such as TPC-DS. A key insight is that carefully handling the long-running tasks and those with tough-to-pack resource needs will produce good-enough schedules. However, which subset of tasks to treat carefully is not clear (and intractable to discover). Hence, we offer a search procedure that evaluates various possibilities and outputs a preferred schedule order over tasks. An online component enforces the schedule orders desired by the various jobs running on the cluster. In addition, it packs tasks, overbooks the fungible resources and guarantees bounded unfairness for a variety of desirable fairness schemes. Relative to the state-of-the art schedulers, we speed up 50% of the jobs by over 30% each.",hard stuff first scheduling dependent computations dataanalytics clusters,present scheduler improves cluster utilization job completion times packing tasks multiresource requirements interdependencies problem algorithmically hard achieve nearoptimality job dags appear production clusters large enterprise benchmarks tpcds key insight carefully handling longrunning tasks toughtopack resource needs produce goodenough schedules however subset tasks treat carefully clear intractable discover hence offer search procedure evaluates various possibilities outputs preferred schedule order tasks online component enforces schedule orders desired various jobs running cluster addition packs tasks overbooks fungible resources guarantees bounded unfairness variety desirable fairness schemes relative stateofthe art schedulers speed jobs
DBOS: A Proposal for a Data-Centric Operating System,"Current operating systems are complex systems that were designed before today's computing environments. This makes it difficult for them to meet the scalability, heterogeneity, availability, and security challenges in current cloud and parallel computing environments. To address these problems, we propose a radically new OS design based on data-centric architecture: all operating system state should be represented uniformly as database tables, and operations on this state should be made via queries from otherwise stateless tasks. This design makes it easy to scale and evolve the OS without whole-system refactoring, inspect and debug system state, upgrade components without downtime, manage decisions using machine learning, and implement sophisticated security features. We discuss how a database OS (DBOS) can improve the programmability and performance of many of today's most important applications and propose a plan for the development of a DBOS proof of concept.",dbos proposal datacentric operating system,current operating systems complex systems designed todays computing environments makes difficult meet scalability heterogeneity availability security challenges current cloud parallel computing environments address problems propose radically new os design based datacentric architecture operating system state represented uniformly database tables operations state made via queries otherwise stateless tasks design makes easy scale evolve os without wholesystem refactoring inspect debug system state upgrade components without downtime manage decisions using machine learning implement sophisticated security features discuss database os dbos improve programmability performance many todays important applications propose plan development dbos proof concept
Proceedings Fifth Workshop on Developments in Computational   Models--Computational Models From Nature,"The special theme of DCM 2009, co-located with ICALP 2009, concerned Computational Models From Nature, with a particular emphasis on computational models derived from physics and biology. The intention was to bring together different approaches - in a community with a strong foundational background as proffered by the ICALP attendees - to create inspirational cross-boundary exchanges, and to lead to innovative further research. Specifically DCM 2009 sought contributions in quantum computation and information, probabilistic models, chemical, biological and bio-inspired ones, including spatial models, growth models and models of self-assembly. Contributions putting to the test logical or algorithmic aspects of computing (e.g., continuous computing with dynamical systems, or solid state computing models) were also very much welcomed.",proceedings fifth workshop developments computational modelscomputational models nature,special theme dcm colocated icalp concerned computational models nature particular emphasis computational models derived physics biology intention bring together different approaches community strong foundational background proffered icalp attendees create inspirational crossboundary exchanges lead innovative research specifically dcm sought contributions quantum computation information probabilistic models chemical biological bioinspired ones including spatial models growth models models selfassembly contributions putting test logical algorithmic aspects computing eg continuous computing dynamical systems solid state computing models also much welcomed
Open Source Prover in the Attic,"The well known JGEX program became open source a few years ago, but seemingly, further development of the program can only be done without the original authors. In our project, we are looking at whether it is possible to continue such a large project as a newcomer without the involvement of the original authors. Is there a way to internationalize, fix bugs, improve the code base, add new features? In other words, to save a relic found in the attic and polish it into a useful everyday tool.",open source prover attic,well known jgex program became open source years ago seemingly development program done without original authors project looking whether possible continue large project newcomer without involvement original authors way internationalize fix bugs improve code base add new features words save relic found attic polish useful everyday tool
Extending Data Spatial Semantics for Scale Agnostic Programming,"We introduce extensions to Data Spatial Programming (DSP) that enable scale-agnostic programming for application development. Building on DSP's paradigm shift from data-to-compute to compute-to-data, we formalize additional intrinsic language constructs that abstract persistent state, multi-user contexts, multiple entry points, and cross-machine distribution for applications. By introducing a globally accessible root node and treating walkers as potential entry points, we demonstrate how programs can be written once and executed across scales, from single-user to multi-user, from local to distributed, without modification. These extensions allow developers to focus on domain logic while delegating runtime concerns of persistence, multi-user support, distribution, and API interfacing to the execution environment. Our approach makes scale-agnostic programming a natural extension of the topological semantics of DSP, allowing applications to seamlessly transition from single-user to multi-user scenarios, from ephemeral to persistent execution contexts, and from local to distributed execution environments.",extending data spatial semantics scale agnostic programming,introduce extensions data spatial programming dsp enable scaleagnostic programming application development building dsps paradigm shift datatocompute computetodata formalize additional intrinsic language constructs abstract persistent state multiuser contexts multiple entry points crossmachine distribution applications introducing globally accessible root node treating walkers potential entry points demonstrate programs written executed across scales singleuser multiuser local distributed without modification extensions allow developers focus domain logic delegating runtime concerns persistence multiuser support distribution api interfacing execution environment approach makes scaleagnostic programming natural extension topological semantics dsp allowing applications seamlessly transition singleuser multiuser scenarios ephemeral persistent execution contexts local distributed execution environments
"Hyperion: A Case for Unified, Self-Hosting, Zero-CPU Data-Processing   Units (DPUs)","Since the inception of computing, we have been reliant on CPU-powered architectures. However, today this reliance is challenged by manufacturing limitations (CMOS scaling), performance expectations (stalled clocks, Turing tax), and security concerns (microarchitectural attacks). To re-imagine our computing architecture, in this work we take a more radical but pragmatic approach and propose to eliminate the CPU with its design baggage, and integrate three primary pillars of computing, i.e., networking, storage, and computing, into a single, self-hosting, unified CPU-free Data Processing Unit (DPU) called Hyperion. In this paper, we present the case for Hyperion, its design choices, initial work-in-progress details, and seek feedback from the systems community.",hyperion case unified selfhosting zerocpu dataprocessing units dpus,since inception computing reliant cpupowered architectures however today reliance challenged manufacturing limitations cmos scaling performance expectations stalled clocks turing tax security concerns microarchitectural attacks reimagine computing architecture work take radical pragmatic approach propose eliminate cpu design baggage integrate three primary pillars computing ie networking storage computing single selfhosting unified cpufree data processing unit dpu called hyperion paper present case hyperion design choices initial workinprogress details seek feedback systems community
Tiramisu: A Polyhedral Compiler for Expressing Fast and Portable Code,"This paper introduces Tiramisu, a polyhedral framework designed to generate high performance code for multiple platforms including multicores, GPUs, and distributed machines. Tiramisu introduces a scheduling language with novel extensions to explicitly manage the complexities that arise when targeting these systems. The framework is designed for the areas of image processing, stencils, linear algebra and deep learning. Tiramisu has two main features: it relies on a flexible representation based on the polyhedral model and it has a rich scheduling language allowing fine-grained control of optimizations. Tiramisu uses a four-level intermediate representation that allows full separation between the algorithms, loop transformations, data layouts, and communication. This separation simplifies targeting multiple hardware architectures with the same algorithm. We evaluate Tiramisu by writing a set of image processing, deep learning, and linear algebra benchmarks and compare them with state-of-the-art compilers and hand-tuned libraries. We show that Tiramisu matches or outperforms existing compilers and libraries on different hardware architectures, including multicore CPUs, GPUs, and distributed machines.",tiramisu polyhedral compiler expressing fast portable code,paper introduces tiramisu polyhedral framework designed generate high performance code multiple platforms including multicores gpus distributed machines tiramisu introduces scheduling language novel extensions explicitly manage complexities arise targeting systems framework designed areas image processing stencils linear algebra deep learning tiramisu two main features relies flexible representation based polyhedral model rich scheduling language allowing finegrained control optimizations tiramisu uses fourlevel intermediate representation allows full separation algorithms loop transformations data layouts communication separation simplifies targeting multiple hardware architectures algorithm evaluate tiramisu writing set image processing deep learning linear algebra benchmarks compare stateoftheart compilers handtuned libraries show tiramisu matches outperforms existing compilers libraries different hardware architectures including multicore cpus gpus distributed machines
"Detection, Classification and Prevalence of Self-Admitted Aging Debt","Context: Previous research on software aging is limited with focus on dynamic runtime indicators like memory and performance, often neglecting evolutionary indicators like source code comments and narrowly examining legacy issues within the TD context. Objective: We introduce the concept of Aging Debt (AD), representing the increased maintenance efforts and costs needed to keep software updated. We study AD through Self-Admitted Aging Debt (SAAD) observed in source code comments left by software developers. Method: We employ a mixed-methods approach, combining qualitative and quantitative analyses to detect and measure AD in software. This includes framing SAAD patterns from the source code comments after analysing the source code context, then utilizing the SAAD patterns to detect SAAD comments. In the process, we develop a taxonomy for SAAD that reflects the temporal aging of software and its associated debt. Then we utilize the taxonomy to quantify the different types of AD prevalent in OSS repositories. Results: Our proposed taxonomy categorizes temporal software aging into Active and Dormant types. Our extensive analysis of over 9,000+ Open Source Software (OSS) repositories reveals that more than 21% repositories exhibit signs of SAAD as observed from our gold standard SAAD dataset. Notably, Dormant AD emerges as the predominant category, highlighting a critical but often overlooked aspect of software maintenance. Conclusion: As software volume grows annually, so do evolutionary aging and maintenance challenges; our proposed taxonomy can aid researchers in detailed software aging studies and help practitioners develop improved and proactive maintenance strategies.",detection classification prevalence selfadmitted aging debt,context previous research software aging limited focus dynamic runtime indicators like memory performance often neglecting evolutionary indicators like source code comments narrowly examining legacy issues within td context objective introduce concept aging debt ad representing increased maintenance efforts costs needed keep software updated study ad selfadmitted aging debt saad observed source code comments left software developers method employ mixedmethods approach combining qualitative quantitative analyses detect measure ad software includes framing saad patterns source code comments analysing source code context utilizing saad patterns detect saad comments process develop taxonomy saad reflects temporal aging software associated debt utilize taxonomy quantify different types ad prevalent oss repositories results proposed taxonomy categorizes temporal software aging active dormant types extensive analysis open source software oss repositories reveals repositories exhibit signs saad observed gold standard saad dataset notably dormant ad emerges predominant category highlighting critical often overlooked aspect software maintenance conclusion software volume grows annually evolutionary aging maintenance challenges proposed taxonomy aid researchers detailed software aging studies help practitioners develop improved proactive maintenance strategies
A Survey of Deep Learning Models for Structural Code Understanding,"In recent years, the rise of deep learning and automation requirements in the software industry has elevated Intelligent Software Engineering to new heights. The number of approaches and applications in code understanding is growing, with deep learning techniques being used in many of them to better capture the information in code data. In this survey, we present a comprehensive overview of the structures formed from code data. We categorize the models for understanding code in recent years into two groups: sequence-based and graph-based models, further make a summary and comparison of them. We also introduce metrics, datasets and the downstream tasks. Finally, we make some suggestions for future research in structural code understanding field.",survey deep learning models structural code understanding,recent years rise deep learning automation requirements software industry elevated intelligent software engineering new heights number approaches applications code understanding growing deep learning techniques used many better capture information code data survey present comprehensive overview structures formed code data categorize models understanding code recent years two groups sequencebased graphbased models make summary comparison also introduce metrics datasets downstream tasks finally make suggestions future research structural code understanding field
Executable Set Theory and Arithmetic Encodings in Prolog,"The paper is organized as a self-contained literate Prolog program that implements elements of an executable finite set theory with focus on combinatorial generation and arithmetic encodings. The complete Prolog code is available at http://logic.csci.unt.edu/tarau/research/2008/pHFS.zip . First, ranking and unranking functions for some ""mathematically elegant"" data types in the universe of Hereditarily Finite Sets with Urelements are provided, resulting in arithmetic encodings for powersets, hypergraphs, ordinals and choice functions. After implementing a digraph representation of Hereditarily Finite Sets we define {\em decoration functions} that can recover well-founded sets from encodings of their associated acyclic digraphs. We conclude with an encoding of arbitrary digraphs and discuss a concept of duality induced by the set membership relation. In the process, we uncover the surprising possibility of internally sharing isomorphic objects, independently of their language level types and meanings.",executable set theory arithmetic encodings prolog,paper organized selfcontained literate prolog program implements elements executable finite set theory focus combinatorial generation arithmetic encodings complete prolog code available httplogiccsciuntedutarauresearchphfszip first ranking unranking functions mathematically elegant data types universe hereditarily finite sets urelements provided resulting arithmetic encodings powersets hypergraphs ordinals choice functions implementing digraph representation hereditarily finite sets define em decoration functions recover wellfounded sets encodings associated acyclic digraphs conclude encoding arbitrary digraphs discuss concept duality induced set membership relation process uncover surprising possibility internally sharing isomorphic objects independently language level types meanings
Runtime Repeated Recursion Unfolding in CHR: A Just-In-Time Online   Program Optimization Strategy That Can Achieve Super-Linear Speedup,"We introduce a just-in-time runtime program transformation strategy based on repeated recursion unfolding. Our online program optimization generates several versions of a recursion differentiated by the minimal number of recursive steps covered. The base case of the recursion is ignored in our technique.   Our method is introduced here on the basis of single linear direct recursive rules. When a recursive call is encountered at runtime, first an unfolder creates specializations of the associated recursive rule on-the-fly and then an interpreter applies these rules to the call. Our approach reduces the number of recursive rule applications to its logarithm at the expense of introducing a logarithmic number of generic unfolded rules.   We prove correctness of our online optimization technique and determine its time complexity. For recursions which have enough simplifyable unfoldings, a super-linear is possible, i.e. speedup by more than a constant factor.The necessary simplification is problem-specific and has to be provided at compile-time. In our speedup analysis, we prove a sufficient condition as well as a sufficient and necessary condition for super-linear speedup relating the complexity of the recursive steps of the original rule and the unfolded rules.   We have implemented an unfolder and meta-interpreter for runtime repeated recursion unfolding with just five rules in Constraint Handling Rules (CHR) embedded in Prolog. We illustrate the feasibility of our approach with simplifications, time complexity results and benchmarks for some basic tractable algorithms. The simplifications require some insight and were derived manually. The runtime improvement quickly reaches several orders of magnitude, consistent with the super-linear speedup predicted by our theorems.",runtime repeated recursion unfolding chr justintime online program optimization strategy achieve superlinear speedup,introduce justintime runtime program transformation strategy based repeated recursion unfolding online program optimization generates several versions recursion differentiated minimal number recursive steps covered base case recursion ignored technique method introduced basis single linear direct recursive rules recursive call encountered runtime first unfolder creates specializations associated recursive rule onthefly interpreter applies rules call approach reduces number recursive rule applications logarithm expense introducing logarithmic number generic unfolded rules prove correctness online optimization technique determine time complexity recursions enough simplifyable unfoldings superlinear possible ie speedup constant factorthe necessary simplification problemspecific provided compiletime speedup analysis prove sufficient condition well sufficient necessary condition superlinear speedup relating complexity recursive steps original rule unfolded rules implemented unfolder metainterpreter runtime repeated recursion unfolding five rules constraint handling rules chr embedded prolog illustrate feasibility approach simplifications time complexity results benchmarks basic tractable algorithms simplifications require insight derived manually runtime improvement quickly reaches several orders magnitude consistent superlinear speedup predicted theorems
Telescope: Telemetry at Terabyte Scale,"Data-hungry applications that require terabytes of memory have become widespread in recent years. To meet the memory needs of these applications, data centers are embracing tiered memory architectures with near and far memory tiers. Precise, efficient, and timely identification of hot and cold data and their placement in appropriate tiers is critical for performance in such systems. Unfortunately, the existing state-of-the-art telemetry techniques for hot and cold data detection are ineffective at the terabyte scale.   We propose Telescope, a novel technique that profiles different levels of the application's page table tree for fast and efficient identification of hot and cold data. Telescope is based on the observation that, for a memory- and TLB-intensive workload, higher levels of a page table tree are also frequently accessed during a hardware page table walk. Hence, the hotness of the higher levels of the page table tree essentially captures the hotness of its subtrees or address space sub-regions at a coarser granularity. We exploit this insight to quickly converge on even a few megabytes of hot data and efficiently identify several gigabytes of cold data in terabyte-scale applications. Importantly, such a technique can seamlessly scale to petabyte-scale applications.   Telescope's telemetry achieves 90%+ precision and recall at just 0.009% single CPU utilization for microbenchmarks with a 5 TB memory footprint. Memory tiering based on Telescope results in 5.6% to 34% throughput improvement for real-world benchmarks with a 1-2 TB memory footprint compared to other state-of-the-art telemetry techniques.",telescope telemetry terabyte scale,datahungry applications require terabytes memory become widespread recent years meet memory needs applications data centers embracing tiered memory architectures near far memory tiers precise efficient timely identification hot cold data placement appropriate tiers critical performance systems unfortunately existing stateoftheart telemetry techniques hot cold data detection ineffective terabyte scale propose telescope novel technique profiles different levels applications page table tree fast efficient identification hot cold data telescope based observation memory tlbintensive workload higher levels page table tree also frequently accessed hardware page table walk hence hotness higher levels page table tree essentially captures hotness subtrees address space subregions coarser granularity exploit insight quickly converge even megabytes hot data efficiently identify several gigabytes cold data terabytescale applications importantly technique seamlessly scale petabytescale applications telescopes telemetry achieves precision recall single cpu utilization microbenchmarks tb memory footprint memory tiering based telescope results throughput improvement realworld benchmarks tb memory footprint compared stateoftheart telemetry techniques
A Scalable Stream-Oriented Framework for Cluster Applications,"This paper presents a stream-oriented architecture for structuring cluster applications. Clusters that run applications based on this architecture can scale to tenths of thousands of nodes with significantly less performance loss or reliability problems. Our architecture exploits the stream nature of the data flow and reduces congestion through load balancing, hides latency behind data pushes and transparently handles node failures. In our ongoing work, we are developing an implementation for this architecture and we are able to run simple data mining applications on a cluster simulator.",scalable streamoriented framework cluster applications,paper presents streamoriented architecture structuring cluster applications clusters run applications based architecture scale tenths thousands nodes significantly less performance loss reliability problems architecture exploits stream nature data flow reduces congestion load balancing hides latency behind data pushes transparently handles node failures ongoing work developing implementation architecture able run simple data mining applications cluster simulator
"An Overview of Phishing Victimization: Human Factors, Training and the   Role of Emotions","Phishing is a form of cybercrime and a threat that allows criminals, phishers, to deceive end users in order to steal their confidential and sensitive information. Attackers usually attempt to manipulate the psychology and emotions of victims. The increasing threat of phishing has made its study worthwhile and much research has been conducted into the issue. This paper explores the emotional factors that have been reported in previous studies to be significant in phishing victimization. In addition, we compare what security organizations and researchers have highlighted in terms of phishing types and categories as well as training in tackling the problem, in a literature review which takes into account all major credible and published sources.",overview phishing victimization human factors training role emotions,phishing form cybercrime threat allows criminals phishers deceive end users order steal confidential sensitive information attackers usually attempt manipulate psychology emotions victims increasing threat phishing made study worthwhile much research conducted issue paper explores emotional factors reported previous studies significant phishing victimization addition compare security organizations researchers highlighted terms phishing types categories well training tackling problem literature review takes account major credible published sources
"Proceedings Fourth International Workshop on Testing, Analysis and   Verification of Web Software","This volume contains the papers presented at the fourth international workshop on Testing, Analysis and Verification of Software, which was associated with the 25th IEEE/ACM International Conference on Automated Software Engineering (ASE 2010). The collection of papers includes research on formal specification, model-checking, testing, and debugging of Web software.",proceedings fourth international workshop testing analysis verification web software,volume contains papers presented fourth international workshop testing analysis verification software associated th ieeeacm international conference automated software engineering ase collection papers includes research formal specification modelchecking testing debugging web software
A Logical Programming Language as an Instrument for Specifying and   Verifying Dynamic Memory,"This work proposes a Prolog-dialect for the found and prioritised problems on expressibility and automation. Given some given C-like program, if dynamic memory is allocated, altered and freed on runtime, then a description of desired dynamic memory is a heap specification. The check of calculated memory state against a given specification is dynamic memory verification. This contribution only considers formal specification and verification in a Hoare calculus. Issues found include: invalid assignment, (temporary) unavailable data in memory cells, excessive memory allocation, (accidental) heap alteration in unexpected regions and others. Excessive memory allocation is nowadays successfully resolved by memory analysers like Valgrind. Essentially, papers in those areas did not bring any big breakthrough. Possible reasons may also include the decrease of tension due to more available memory and parallel threads. However, starting with Apt, problems related to variable modes have not yet been resolved -- neither entirely nor in an acceptable way. Research contributions over the last decades show again and again that heap issues remain and remain complex and still important. A significant contribution was reached in 2016 by Peter O'Hearn, who accepted the G\""{o}del prize for his parallel approach on a spatial heap operation.",logical programming language instrument specifying verifying dynamic memory,work proposes prologdialect found prioritised problems expressibility automation given given clike program dynamic memory allocated altered freed runtime description desired dynamic memory heap specification check calculated memory state given specification dynamic memory verification contribution considers formal specification verification hoare calculus issues found include invalid assignment temporary unavailable data memory cells excessive memory allocation accidental heap alteration unexpected regions others excessive memory allocation nowadays successfully resolved memory analysers like valgrind essentially papers areas bring big breakthrough possible reasons may also include decrease tension due available memory parallel threads however starting apt problems related variable modes yet resolved neither entirely acceptable way research contributions last decades show heap issues remain remain complex still important significant contribution reached peter ohearn accepted godel prize parallel approach spatial heap operation
Disruptive Changes in Field Equation Modeling: A Simple Interface for   Wafer Scale Engines,"We present a high-level and accessible Application Programming Interface (API) for the solution of field equations on the Cerebras Systems Wafer-Scale Engine (WSE) with over two orders of magnitude performance gain relative to traditional distributed computing approaches. The domain-specific API is called the WSE Field-equation API (WFA). The WFA outperforms OpenFOAM on NETL's Joule 2.0 supercomputer by over two orders of magnitude in time to solution. While this performance is consistent with hand-optimized assembly codes, the WFA provides an easy-to-use, high-level Python interface that allows users to form and solve field equations effortlessly. We report here the WFA programming methodology and achieved performance on the latest generation of WSE, the CS-2.",disruptive changes field equation modeling simple interface wafer scale engines,present highlevel accessible application programming interface api solution field equations cerebras systems waferscale engine wse two orders magnitude performance gain relative traditional distributed computing approaches domainspecific api called wse fieldequation api wfa wfa outperforms openfoam netls joule supercomputer two orders magnitude time solution performance consistent handoptimized assembly codes wfa provides easytouse highlevel python interface allows users form solve field equations effortlessly report wfa programming methodology achieved performance latest generation wse cs
Aneka: A Software Platform for .NET-based Cloud Computing,"Aneka is a platform for deploying Clouds developing applications on top of it. It provides a runtime environment and a set of APIs that allow developers to build .NET applications that leverage their computation on either public or private clouds. One of the key features of Aneka is the ability of supporting multiple programming models that are ways of expressing the execution logic of applications by using specific abstractions. This is accomplished by creating a customizable and extensible service oriented runtime environment represented by a collection of software containers connected together. By leveraging on these architecture advanced services including resource reservation, persistence, storage management, security, and performance monitoring have been implemented. On top of this infrastructure different programming models can be plugged to provide support for different scenarios as demonstrated by the engineering, life science, and industry applications.",aneka software platform netbased cloud computing,aneka platform deploying clouds developing applications top provides runtime environment set apis allow developers build net applications leverage computation either public private clouds one key features aneka ability supporting multiple programming models ways expressing execution logic applications using specific abstractions accomplished creating customizable extensible service oriented runtime environment represented collection software containers connected together leveraging architecture advanced services including resource reservation persistence storage management security performance monitoring implemented top infrastructure different programming models plugged provide support different scenarios demonstrated engineering life science industry applications
An Introduction to Time-Constrained Automata,"We present time-constrained automata (TCA), a model for hard real-time computation in which agents behaviors are modeled by automata and constrained by time intervals.   TCA actions can have multiple start time and deadlines, can be aperiodic, and are selected dynamically following a graph, the time-constrained automaton. This allows expressing much more precise time constraints than classical periodic or sporadic model, while preserving the ease of scheduling and analysis.   We provide some properties of this model as well as their scheduling semantics. We show that TCA can be automatically derived from source-code, and optimally scheduled on single processors using a variant of EDF. We explain how time constraints can be used to guarantee communication determinism by construction, and to study when possible agent interactions happen.",introduction timeconstrained automata,present timeconstrained automata tca model hard realtime computation agents behaviors modeled automata constrained time intervals tca actions multiple start time deadlines aperiodic selected dynamically following graph timeconstrained automaton allows expressing much precise time constraints classical periodic sporadic model preserving ease scheduling analysis provide properties model well scheduling semantics show tca automatically derived sourcecode optimally scheduled single processors using variant edf explain time constraints used guarantee communication determinism construction study possible agent interactions happen
The Recomputation Manifesto,"Replication of scientific experiments is critical to the advance of science. Unfortunately, the discipline of Computer Science has never treated replication seriously, even though computers are very good at doing the same thing over and over again. Not only are experiments rarely replicated, they are rarely even replicable in a meaningful way. Scientists are being encouraged to make their source code available, but this is only a small step. Even in the happy event that source code can be built and run successfully, running code is a long way away from being able to replicate the experiment that code was used for. I propose that the discipline of Computer Science must embrace replication of experiments as standard practice. I propose that the only credible technique to make experiments truly replicable is to provide copies of virtual machines in which the experiments are validated to run. I propose that tools and repositories should be made available to make this happen. I propose to be one of those who makes it happen.",recomputation manifesto,replication scientific experiments critical advance science unfortunately discipline computer science never treated replication seriously even though computers good thing experiments rarely replicated rarely even replicable meaningful way scientists encouraged make source code available small step even happy event source code built run successfully running code long way away able replicate experiment code used propose discipline computer science must embrace replication experiments standard practice propose credible technique make experiments truly replicable provide copies virtual machines experiments validated run propose tools repositories made available make happen propose one makes happen
A Preliminary Review of Influential Works in Data-Driven Discovery,"The Gordon and Betty Moore Foundation ran an Investigator Competition as part of its Data-Driven Discovery Initiative in 2014. We received about 1,100 applications and each applicant had the opportunity to list up to five influential works in the general field of ""Big Data"" for scientific discovery. We collected nearly 5,000 references and 53 works were cited at least six times. This paper contains our preliminary findings.",preliminary review influential works datadriven discovery,gordon betty moore foundation ran investigator competition part datadriven discovery initiative received applications applicant opportunity list five influential works general field big data scientific discovery collected nearly references works cited least six times paper contains preliminary findings
GOTO Rankings Considered Helpful,"Rankings are a fact of life. Whether or not one likes them, they exist and are influential. Within academia, and in computer science in particular, rankings not only capture our attention but also widely influence people who have a limited understanding of computing science research, including prospective students, university administrators, and policy-makers. In short, rankings matter. This position paper advocates for the adoption of ""GOTO rankings"": rankings that use Good data, are Open, Transparent, and Objective, and the rejection of rankings that do not meet these criteria.",goto rankings considered helpful,rankings fact life whether one likes exist influential within academia computer science particular rankings capture attention also widely influence people limited understanding computing science research including prospective students university administrators policymakers short rankings matter position paper advocates adoption goto rankings rankings use good data open transparent objective rejection rankings meet criteria
Who Owns the Data? A Systematic Review at the Boundary of Information   Systems and Marketing,"This paper gives a systematic research review at the boundary of the information systems (IS) and marketing disciplines. First, a historical overview of these disciplines is given to put the review into context. This is followed by a bibliographic analysis to select articles at the boundary of IS and marketing. Text analysis is then performed on the selected articles to group them into homogeneous research clusters, which are refined by selecting ""distinct"" articles that best represent the clusters. The citation asymmetries between IS and marketing are noted and an overall conceptual model is created that describes the ""areas of collaboration"" between IS and marketing. Forward looking suggestions are made on how academic researchers can better interface with industry and how academic research at the boundary of IS and marketing can be further developed.",owns data systematic review boundary information systems marketing,paper gives systematic research review boundary information systems marketing disciplines first historical overview disciplines given put review context followed bibliographic analysis select articles boundary marketing text analysis performed selected articles group homogeneous research clusters refined selecting distinct articles best represent clusters citation asymmetries marketing noted overall conceptual model created describes areas collaboration marketing forward looking suggestions made academic researchers better interface industry academic research boundary marketing developed
Challenges and Opportunities of Large Transnational Datasets: A Case   Study on European Administrative Crop Data,"Expansive, informative datasets are vital in providing foundations and possibilities for scientific research and development across many fields of study. Assembly of grand datasets, however, frequently poses difficulty for the author and stakeholders alike, with a variety of considerations required throughout the collaboration efforts and development lifecycle. In this work, we discuss and analyse the challenges and opportunities we faced throughout the creation of a transnational, European agricultural dataset containing reference labels of cultivated crops. Together, this forms a succinct framework of important elements one should consider when forging a dataset of their own.",challenges opportunities large transnational datasets case study european administrative crop data,expansive informative datasets vital providing foundations possibilities scientific research development across many fields study assembly grand datasets however frequently poses difficulty author stakeholders alike variety considerations required throughout collaboration efforts development lifecycle work discuss analyse challenges opportunities faced throughout creation transnational european agricultural dataset containing reference labels cultivated crops together forms succinct framework important elements one consider forging dataset
Playing catch-up in building an open research commons,"On August 2, 2021 a group of concerned scientists and US funding agency and federal government officials met for an informal discussion to explore the value and need for a well-coordinated US Open Research Commons (ORC); an interoperable collection of data and compute resources within both the public and private sectors which are easy to use and accessible to all.",playing catchup building open research commons,august group concerned scientists us funding agency federal government officials met informal discussion explore value need wellcoordinated us open research commons orc interoperable collection data compute resources within public private sectors easy use accessible
Data Science from 1963 to 2012,"Consensus on the definition of data science remains low despite the widespread establishment of academic programs in the field and continued demand for data scientists in industry. Definitions range from rebranded statistics to data-driven science to the science of data to simply the application of machine learning to so-called big data to solve real-world problems. Current efforts to trace the history of the field in order to clarify its definition, such as Donoho's ""50 Years of Data Science"" (Donoho 2017), tend to focus on a short period when a small group of statisticians adopted the term in an unsuccessful attempt to rebrand their field in the face of the overshadowing effects of computational statistics and data mining. Using textual evidence from primary sources, this essay traces the history of the term to the 1960s, when it was first used by the US Air Force in a surprisingly similar way to its current usage, to 2012, the year that Harvard Business Review published the enormously influential article ""Data Scientist: The Sexiest Job of the 21st Century"" (Davenport and Patil 2012) and the American Statistical Association acknowledged a profound disconnect between statistics and data science (Rodriguez 2012). Among the themes that emerge from this review are (1) the long-standing opposition between data analysts and data miners that continues to animate the field, (2) an established definition of the term as the practice of managing and processing scientific data that has been occluded by recent usage, and (3) the phenomenon of data impedance -- the disproportion between surplus data, indexed by phrases like data deluge and big data, and the limitations of computational machinery and methods to process them. This persistent condition appears to have motivated the use of the term and the field itself since its beginnings.",data science,consensus definition data science remains low despite widespread establishment academic programs field continued demand data scientists industry definitions range rebranded statistics datadriven science science data simply application machine learning socalled big data solve realworld problems current efforts trace history field order clarify definition donohos years data science donoho tend focus short period small group statisticians adopted term unsuccessful attempt rebrand field face overshadowing effects computational statistics data mining using textual evidence primary sources essay traces history term first used us air force surprisingly similar way current usage year harvard business review published enormously influential article data scientist sexiest job st century davenport patil american statistical association acknowledged profound disconnect statistics data science rodriguez among themes emerge review longstanding opposition data analysts data miners continues animate field established definition term practice managing processing scientific data occluded recent usage phenomenon data impedance disproportion surplus data indexed phrases like data deluge big data limitations computational machinery methods process persistent condition appears motivated use term field since beginnings
"Contextualizing Artificially Intelligent Morality: A Meta-Ethnography of   Top-Down, Bottom-Up, and Hybrid Models for Theoretical and Applied Ethics in   Artificial Intelligence","In this meta-ethnography, we explore three different angles of ethical artificial intelligence (AI) design implementation including the philosophical ethical viewpoint, the technical perspective, and framing through a political lens. Our qualitative research includes a literature review that highlights the cross-referencing of these angles by discussing the value and drawbacks of contrastive top-down, bottom-up, and hybrid approaches previously published. The novel contribution to this framework is the political angle, which constitutes ethics in AI either being determined by corporations and governments and imposed through policies or law (coming from the top), or ethics being called for by the people (coming from the bottom), as well as top-down, bottom-up, and hybrid technicalities of how AI is developed within a moral construct and in consideration of its users, with expected and unexpected consequences and long-term impact in the world. There is a focus on reinforcement learning as an example of a bottom-up applied technical approach and AI ethics principles as a practical top-down approach. This investigation includes real-world case studies to impart a global perspective, as well as philosophical debate on the ethics of AI and theoretical future thought experimentation based on historical facts, current world circumstances, and possible ensuing realities.",contextualizing artificially intelligent morality metaethnography topdown bottomup hybrid models theoretical applied ethics artificial intelligence,metaethnography explore three different angles ethical artificial intelligence ai design implementation including philosophical ethical viewpoint technical perspective framing political lens qualitative research includes literature review highlights crossreferencing angles discussing value drawbacks contrastive topdown bottomup hybrid approaches previously published novel contribution framework political angle constitutes ethics ai either determined corporations governments imposed policies law coming top ethics called people coming bottom well topdown bottomup hybrid technicalities ai developed within moral construct consideration users expected unexpected consequences longterm impact world focus reinforcement learning example bottomup applied technical approach ai ethics principles practical topdown approach investigation includes realworld case studies impart global perspective well philosophical debate ethics ai theoretical future thought experimentation based historical facts current world circumstances possible ensuing realities
Performance Impact of Lock-Free Algorithms on Multicore Communication   APIs,"Data race conditions in multi-tasking software applications are prevented by serializing access to shared memory resources, ensuring data consistency and deterministic behavior. Traditionally tasks acquire and release locks to synchronize operations on shared memory. Unfortunately, lock management can add significant processing overhead especially for multicore deployments where tasks on different cores convoy in queues waiting to acquire a lock. Implementing more than one lock introduces the risk of deadlock and using spinlocks constrains which cores a task can run on. The better alternative is to eliminate locks and validate that real-time properties are met, which is not directly considered in many embedded applications. Removing the locks is non-trivial and packaging lock-free algorithms for developers reduces the possibility of concurrency defects. This paper details how a multicore communication API implementation is enhanced to support lock-free messaging and the impact this has on data exchange latency between tasks. Throughput and latency are compared on Windows and Linux between lock-based and lock-free implementations for data exchange of messages, packets, and scalars. A model of the lock-free exchange predicts performance at the system architecture level and provides a stop criterion for the refactoring. The results show that migration from single to multicore hardware architectures degrades lock-based performance, and increases lock-free performance.",performance impact lockfree algorithms multicore communication apis,data race conditions multitasking software applications prevented serializing access shared memory resources ensuring data consistency deterministic behavior traditionally tasks acquire release locks synchronize operations shared memory unfortunately lock management add significant processing overhead especially multicore deployments tasks different cores convoy queues waiting acquire lock implementing one lock introduces risk deadlock using spinlocks constrains cores task run better alternative eliminate locks validate realtime properties met directly considered many embedded applications removing locks nontrivial packaging lockfree algorithms developers reduces possibility concurrency defects paper details multicore communication api implementation enhanced support lockfree messaging impact data exchange latency tasks throughput latency compared windows linux lockbased lockfree implementations data exchange messages packets scalars model lockfree exchange predicts performance system architecture level provides stop criterion refactoring results show migration single multicore hardware architectures degrades lockbased performance increases lockfree performance
New Directions in Cloud Programming,"Nearly twenty years after the launch of AWS, it remains difficult for most developers to harness the enormous potential of the cloud. In this paper we lay out an agenda for a new generation of cloud programming research aimed at bringing research ideas to programmers in an evolutionary fashion. Key to our approach is a separation of distributed programs into a PACT of four facets: Program semantics, Availablity, Consistency and Targets of optimization. We propose to migrate developers gradually to PACT programming by lifting familiar code into our more declarative level of abstraction. We then propose a multi-stage compiler that emits human-readable code at each stage that can be hand-tuned by developers seeking more control. Our agenda raises numerous research challenges across multiple areas including language design, query optimization, transactions, distributed consistency, compilers and program synthesis.",new directions cloud programming,nearly twenty years launch aws remains difficult developers harness enormous potential cloud paper lay agenda new generation cloud programming research aimed bringing research ideas programmers evolutionary fashion key approach separation distributed programs pact four facets program semantics availablity consistency targets optimization propose migrate developers gradually pact programming lifting familiar code declarative level abstraction propose multistage compiler emits humanreadable code stage handtuned developers seeking control agenda raises numerous research challenges across multiple areas including language design query optimization transactions distributed consistency compilers program synthesis
A survey study of success factors in data science projects,"In recent years, the data science community has pursued excellence and made significant research efforts to develop advanced analytics, focusing on solving technical problems at the expense of organizational and socio-technical challenges. According to previous surveys on the state of data science project management, there is a significant gap between technical and organizational processes. In this article we present new empirical data from a survey to 237 data science professionals on the use of project management methodologies for data science. We provide additional profiling of the survey respondents' roles and their priorities when executing data science projects. Based on this survey study, the main findings are: (1) Agile data science lifecycle is the most widely used framework, but only 25% of the survey participants state to follow a data science project methodology. (2) The most important success factors are precisely describing stakeholders' needs, communicating the results to end-users, and team collaboration and coordination. (3) Professionals who adhere to a project methodology place greater emphasis on the project's potential risks and pitfalls, version control, the deployment pipeline to production, and data security and privacy.",survey study success factors data science projects,recent years data science community pursued excellence made significant research efforts develop advanced analytics focusing solving technical problems expense organizational sociotechnical challenges according previous surveys state data science project management significant gap technical organizational processes article present new empirical data survey data science professionals use project management methodologies data science provide additional profiling survey respondents roles priorities executing data science projects based survey study main findings agile data science lifecycle widely used framework survey participants state follow data science project methodology important success factors precisely describing stakeholders needs communicating results endusers team collaboration coordination professionals adhere project methodology place greater emphasis projects potential risks pitfalls version control deployment pipeline production data security privacy
Performance Evaluation of Multiple TCP connections in iSCSI,Scaling data storage is a significant concern in enterprise systems and Storage Area Networks (SANs) are deployed as a means to scale enterprise storage. SANs based on Fibre Channel have been used extensively in the last decade while iSCSI is fast becoming a serious contender due to its reduced costs and unified infrastructure. This work examines the performance of iSCSI with multiple TCP connections. Multiple TCP connections are often used to realize higher bandwidth but there may be no fairness in how bandwidth is distributed. We propose a mechanism to share congestion information across multiple flows in ``Fair-TCP'' for improved performance. Our results show that Fair-TCP significantly improves the performance for I/O intensive workloads.,performance evaluation multiple tcp connections iscsi,scaling data storage significant concern enterprise systems storage area networks sans deployed means scale enterprise storage sans based fibre channel used extensively last decade iscsi fast becoming serious contender due reduced costs unified infrastructure work examines performance iscsi multiple tcp connections multiple tcp connections often used realize higher bandwidth may fairness bandwidth distributed propose mechanism share congestion information across multiple flows fairtcp improved performance results show fairtcp significantly improves performance io intensive workloads
On the Fly Orchestration of Unikernels: Tuning and Performance   Evaluation of Virtual Infrastructure Managers,"Network operators are facing significant challenges meeting the demand for more bandwidth, agile infrastructures, innovative services, while keeping costs low. Network Functions Virtualization (NFV) and Cloud Computing are emerging as key trends of 5G network architectures, providing flexibility, fast instantiation times, support of Commercial Off The Shelf hardware and significant cost savings. NFV leverages Cloud Computing principles to move the data-plane network functions from expensive, closed and proprietary hardware to the so-called Virtual Network Functions (VNFs). In this paper we deal with the management of virtual computing resources (Unikernels) for the execution of VNFs. This functionality is performed by the Virtual Infrastructure Manager (VIM) in the NFV MANagement and Orchestration (MANO) reference architecture. We discuss the instantiation process of virtual resources and propose a generic reference model, starting from the analysis of three open source VIMs, namely OpenStack, Nomad and OpenVIM. We improve the aforementioned VIMs introducing the support for special-purpose Unikernels and aiming at reducing the duration of the instantiation process. We evaluate some performance aspects of the VIMs, considering both stock and tuned versions. The VIM extensions and performance evaluation tools are available under a liberal open source licence.",fly orchestration unikernels tuning performance evaluation virtual infrastructure managers,network operators facing significant challenges meeting demand bandwidth agile infrastructures innovative services keeping costs low network functions virtualization nfv cloud computing emerging key trends g network architectures providing flexibility fast instantiation times support commercial shelf hardware significant cost savings nfv leverages cloud computing principles move dataplane network functions expensive closed proprietary hardware socalled virtual network functions vnfs paper deal management virtual computing resources unikernels execution vnfs functionality performed virtual infrastructure manager vim nfv management orchestration mano reference architecture discuss instantiation process virtual resources propose generic reference model starting analysis three open source vims namely openstack nomad openvim improve aforementioned vims introducing support specialpurpose unikernels aiming reducing duration instantiation process evaluate performance aspects vims considering stock tuned versions vim extensions performance evaluation tools available liberal open source licence
Profiling and Improving the Duty-Cycling Performance of Linux-based IoT   Devices,"Minimizing the energy consumption of Linux-based devices is an essential step towards their wide deployment in various IoT scenarios. Energy saving methods such as duty-cycling aim to address this constraint by limiting the amount of time the device is powered on. In this work we study and improve the amount of time a Linux-based IoT device is powered on to accomplish its tasks. We analyze the processes of system boot up and shutdown on two platforms, the Raspberry Pi 3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by identifying and disabling time-consuming or unnecessary units initialized in the userspace. We also study whether SD card speed and SD card capacity utilization affect boot up duration and energy consumption. In addition, we propose 'Pallex', a parallel execution framework built on top of the 'systemd init' system to run a user application concurrently with userspace initialization. We validate the performance impact of Pallex when applied to various IoT application scenarios: (i) capturing an image, (ii) capturing and encrypting an image, (iii) capturing and classifying an image using the the k-nearest neighbor algorithm, and (iv) capturing images and sending them to a cloud server. Our results show that system lifetime is increased by 18.3%, 16.8%, 13.9% and 30.2%, for these application scenarios, respectively.",profiling improving dutycycling performance linuxbased iot devices,minimizing energy consumption linuxbased devices essential step towards wide deployment various iot scenarios energy saving methods dutycycling aim address constraint limiting amount time device powered work study improve amount time linuxbased iot device powered accomplish tasks analyze processes system boot shutdown two platforms raspberry pi raspberry pi zero wireless enhance dutycycling performance identifying disabling timeconsuming unnecessary units initialized userspace also study whether sd card speed sd card capacity utilization affect boot duration energy consumption addition propose pallex parallel execution framework built top systemd init system run user application concurrently userspace initialization validate performance impact pallex applied various iot application scenarios capturing image ii capturing encrypting image iii capturing classifying image using knearest neighbor algorithm iv capturing images sending cloud server results show system lifetime increased application scenarios respectively
Formal Proof of SCHUR Conjugate Function,"The main goal of our work is to formally prove the correctness of the key commands of the SCHUR software, an interactive program for calculating with characters of Lie groups and symmetric functions. The core of the computations relies on enumeration and manipulation of combinatorial structures. As a first ""proof of concept"", we present a formal proof of the conjugate function, written in C. This function computes the conjugate of an integer partition. To formally prove this program, we use the Frama-C software. It allows us to annotate C functions and to generate proof obligations, which are proved using several automated theorem provers. In this paper, we also draw on methodology, discussing on how to formally prove this kind of program.",formal proof schur conjugate function,main goal work formally prove correctness key commands schur software interactive program calculating characters lie groups symmetric functions core computations relies enumeration manipulation combinatorial structures first proof concept present formal proof conjugate function written c function computes conjugate integer partition formally prove program use framac software allows us annotate c functions generate proof obligations proved using several automated theorem provers paper also draw methodology discussing formally prove kind program
Global communications in multiprocessor simulations of flames,"In this paper we investigate performance of global communications in a particular parallel code. The code simulates dynamics of expansion of premixed spherical flames using an asymptotic model of Sivashinsky type and a spectral numerical algorithm. As a result, the code heavily relies on global all-to-all interprocessor communications implementing transposition of the distributed data array in which numerical solution to the problem is stored. This global data interdependence makes interprocessor connectivity of the HPC system as important as the floating-point power of the processors of which the system is built. Our experiments show that efficient numerical simulation of this particular model, with global data interdependence, on modern HPC systems is possible. Prospects of performance of more sophisticated models of flame dynamics are analysed as well.",global communications multiprocessor simulations flames,paper investigate performance global communications particular parallel code code simulates dynamics expansion premixed spherical flames using asymptotic model sivashinsky type spectral numerical algorithm result code heavily relies global alltoall interprocessor communications implementing transposition distributed data array numerical solution problem stored global data interdependence makes interprocessor connectivity hpc system important floatingpoint power processors system built experiments show efficient numerical simulation particular model global data interdependence modern hpc systems possible prospects performance sophisticated models flame dynamics analysed well
The Vectorization of the Tersoff Multi-Body Potential: An Exercise in   Performance Portability,"Molecular dynamics simulations, an indispensable research tool in computational chemistry and materials science, consume a significant portion of the supercomputing cycles around the world. We focus on multi-body potentials and aim at achieving performance portability. Compared with well-studied pair potentials, multibody potentials deliver increased simulation accuracy but are too complex for effective compiler optimization. Because of this, achieving cross-platform performance remains an open question. By abstracting from target architecture and computing precision, we develop a vectorization scheme applicable to both CPUs and accelerators. We present results for the Tersoff potential within the molecular dynamics code LAMMPS on several architectures, demonstrating efficiency gains not only for computational kernels, but also for large-scale simulations. On a cluster of Intel Xeon Phi's, our optimized solver is between 3 and 5 times faster than the pure MPI reference.",vectorization tersoff multibody potential exercise performance portability,molecular dynamics simulations indispensable research tool computational chemistry materials science consume significant portion supercomputing cycles around world focus multibody potentials aim achieving performance portability compared wellstudied pair potentials multibody potentials deliver increased simulation accuracy complex effective compiler optimization achieving crossplatform performance remains open question abstracting target architecture computing precision develop vectorization scheme applicable cpus accelerators present results tersoff potential within molecular dynamics code lammps several architectures demonstrating efficiency gains computational kernels also largescale simulations cluster intel xeon phis optimized solver times faster pure mpi reference
On Metric Skyline Processing by PM-tree,"The task of similarity search in multimedia databases is usually accomplished by range or k nearest neighbor queries. However, the expressing power of these ""single-example"" queries fails when the user's delicate query intent is not available as a single example. Recently, the well-known skyline operator was reused in metric similarity search as a ""multi-example"" query type. When applied on a multi-dimensional database (i.e., on a multi-attribute table), the traditional skyline operator selects all database objects that are not dominated by other objects. The metric skyline query adopts the skyline operator such that the multiple attributes are represented by distances (similarities) to multiple query examples. Hence, we can view the metric skyline as a set of representative database objects which are as similar to all the examples as possible and, simultaneously, are semantically distinct. In this paper we propose a technique of processing the metric skyline query by use of PM-tree, while we show that our technique significantly outperforms the original M-tree based implementation in both time and space costs. In experiments we also evaluate the partial metric skyline processing, where only a controlled number of skyline objects is retrieved.",metric skyline processing pmtree,task similarity search multimedia databases usually accomplished range k nearest neighbor queries however expressing power singleexample queries fails users delicate query intent available single example recently wellknown skyline operator reused metric similarity search multiexample query type applied multidimensional database ie multiattribute table traditional skyline operator selects database objects dominated objects metric skyline query adopts skyline operator multiple attributes represented distances similarities multiple query examples hence view metric skyline set representative database objects similar examples possible simultaneously semantically distinct paper propose technique processing metric skyline query use pmtree show technique significantly outperforms original mtree based implementation time space costs experiments also evaluate partial metric skyline processing controlled number skyline objects retrieved
Minimum Cost Loop Nests for Contraction of a Sparse Tensor with a Tensor   Network,"Sparse tensor decomposition and completion are common in numerous applications, ranging from machine learning to computational quantum chemistry. Typically, the main bottleneck in optimization of these models are contractions of a single large sparse tensor with a network of several dense matrices or tensors (SpTTN). Prior works on high-performance tensor decomposition and completion have focused on performance and scalability optimizations for specific SpTTN kernels. We present algorithms and a runtime system for identifying and executing the most efficient loop nest for any SpTTN kernel. We consider both enumeration of such loop nests for autotuning and efficient algorithms for finding the lowest cost loop-nest for simpler metrics, such as buffer size or cache miss models. Our runtime system identifies the best choice of loop nest without user guidance, and also provides a distributed-memory parallelization of SpTTN kernels. We evaluate our framework using both real-world and synthetic tensors. Our results demonstrate that our approach outperforms available generalized state-of-the-art libraries and matches the performance of specialized codes.",minimum cost loop nests contraction sparse tensor tensor network,sparse tensor decomposition completion common numerous applications ranging machine learning computational quantum chemistry typically main bottleneck optimization models contractions single large sparse tensor network several dense matrices tensors spttn prior works highperformance tensor decomposition completion focused performance scalability optimizations specific spttn kernels present algorithms runtime system identifying executing efficient loop nest spttn kernel consider enumeration loop nests autotuning efficient algorithms finding lowest cost loopnest simpler metrics buffer size cache miss models runtime system identifies best choice loop nest without user guidance also provides distributedmemory parallelization spttn kernels evaluate framework using realworld synthetic tensors results demonstrate approach outperforms available generalized stateoftheart libraries matches performance specialized codes
MPI+X: task-based parallelization and dynamic load balance of finite   element assembly,"The main computing tasks of a finite element code(FE) for solving partial differential equations (PDE's) are the algebraic system assembly and the iterative solver. This work focuses on the first task, in the context of a hybrid MPI+X paradigm. Although we will describe algorithms in the FE context, a similar strategy can be straightforwardly applied to other discretization methods, like the finite volume method. The matrix assembly consists of a loop over the elements of the MPI partition to compute element matrices and right-hand sides and their assemblies in the local system to each MPI partition. In a MPI+X hybrid parallelism context, X has consisted traditionally of loop parallelism using OpenMP. Several strategies have been proposed in the literature to implement this loop parallelism, like coloring or substructuring techniques to circumvent the race condition that appears when assembling the element system into the local system. The main drawback of the first technique is the decrease of the IPC due to bad spatial locality. The second technique avoids this issue but requires extensive changes in the implementation, which can be cumbersome when several element loops should be treated. We propose an alternative, based on the task parallelism of the element loop using some extensions to the OpenMP programming model. The taskification of the assembly solves both aforementioned problems. In addition, dynamic load balance will be applied using the DLB library, especially efficient in the presence of hybrid meshes, where the relative costs of the different elements is impossible to estimate a priori. This paper presents the proposed methodology, its implementation and its validation through the solution of large computational mechanics problems up to 16k cores.",mpix taskbased parallelization dynamic load balance finite element assembly,main computing tasks finite element codefe solving partial differential equations pdes algebraic system assembly iterative solver work focuses first task context hybrid mpix paradigm although describe algorithms fe context similar strategy straightforwardly applied discretization methods like finite volume method matrix assembly consists loop elements mpi partition compute element matrices righthand sides assemblies local system mpi partition mpix hybrid parallelism context x consisted traditionally loop parallelism using openmp several strategies proposed literature implement loop parallelism like coloring substructuring techniques circumvent race condition appears assembling element system local system main drawback first technique decrease ipc due bad spatial locality second technique avoids issue requires extensive changes implementation cumbersome several element loops treated propose alternative based task parallelism element loop using extensions openmp programming model taskification assembly solves aforementioned problems addition dynamic load balance applied using dlb library especially efficient presence hybrid meshes relative costs different elements impossible estimate priori paper presents proposed methodology implementation validation solution large computational mechanics problems k cores
SAFIUS - A secure and accountable filesystem over untrusted storage,"We describe SAFIUS, a secure accountable file system that resides over an untrusted storage. SAFIUS provides strong security guarantees like confidentiality, integrity, prevention from rollback attacks, and accountability. SAFIUS also enables read/write sharing of data and provides the standard UNIX-like interface for applications. To achieve accountability with good performance, it uses asynchronous signatures; to reduce the space required for storing these signatures, a novel signature pruning mechanism is used. SAFIUS has been implemented on a GNU/Linux based system modifying OpenGFS. Preliminary performance studies show that SAFIUS has a tolerable overhead for providing secure storage: while it has an overhead of about 50% of OpenGFS in data intensive workloads (due to the overhead of performing encryption/decryption in software), it is comparable (or better in some cases) to OpenGFS in metadata intensive workloads.",safius secure accountable filesystem untrusted storage,describe safius secure accountable file system resides untrusted storage safius provides strong security guarantees like confidentiality integrity prevention rollback attacks accountability safius also enables readwrite sharing data provides standard unixlike interface applications achieve accountability good performance uses asynchronous signatures reduce space required storing signatures novel signature pruning mechanism used safius implemented gnulinux based system modifying opengfs preliminary performance studies show safius tolerable overhead providing secure storage overhead opengfs data intensive workloads due overhead performing encryptiondecryption software comparable better cases opengfs metadata intensive workloads
Consideration for effectively handling parallel workloads on public   cloud system,"We retrieved and analyzed parallel storage workloads of the FUJITSU K5 cloud service to clarify how to build cost-effective hybrid storage systems. A hybrid storage system consists of fast but low-capacity tier (first tier) and slow but high-capacity tier (second tier). And, it typically consists of either SSDs and HDDs or NVMs and SSDs. As a result, we found that 1) regions for first tier should be assigned only if a workload includes large number of IO accesses for a whole day, 2) the regions that include a large number of IO accesses should be dynamically chosen and moved from second tier to first tier for a short interval, and 3) if a cache hit ratio is regularly low, use of the cache for the workload should be cancelled, and the whole workload region should be assigned to the region for first tier. These workloads already have been released from the SNIA web site.",consideration effectively handling parallel workloads public cloud system,retrieved analyzed parallel storage workloads fujitsu k cloud service clarify build costeffective hybrid storage systems hybrid storage system consists fast lowcapacity tier first tier slow highcapacity tier second tier typically consists either ssds hdds nvms ssds result found regions first tier assigned workload includes large number io accesses whole day regions include large number io accesses dynamically chosen moved second tier first tier short interval cache hit ratio regularly low use cache workload cancelled whole workload region assigned region first tier workloads already released snia web site
"Vertical, Temporal, and Horizontal Scaling of Hierarchical Hypersparse   GraphBLAS Matrices","Hypersparse matrices are a powerful enabler for a variety of network, health, finance, and social applications. Hierarchical hypersparse GraphBLAS matrices enable rapid streaming updates while preserving algebraic analytic power and convenience. In many contexts, the rate of these updates sets the bounds on performance. This paper explores hierarchical hypersparse update performance on a variety of hardware with identical software configurations. The high-level language bindings of the GraphBLAS readily enable performance experiments on simultaneous diverse hardware. The best single process performance measured was 4,000,000 updates per second. The best single node performance measured was 170,000,000 updates per second. The hardware used spans nearly a decade and allows a direct comparison of hardware improvements for this computation over this time range; showing a 2x increase in single-core performance, a 3x increase in single process performance, and a 5x increase in single node performance. Running on nearly 2,000 MIT SuperCloud nodes simultaneously achieved a sustained update rate of over 200,000,000,000 updates per second. Hierarchical hypersparse GraphBLAS allows the MIT SuperCloud to analyze extremely large streaming network data sets.",vertical temporal horizontal scaling hierarchical hypersparse graphblas matrices,hypersparse matrices powerful enabler variety network health finance social applications hierarchical hypersparse graphblas matrices enable rapid streaming updates preserving algebraic analytic power convenience many contexts rate updates sets bounds performance paper explores hierarchical hypersparse update performance variety hardware identical software configurations highlevel language bindings graphblas readily enable performance experiments simultaneous diverse hardware best single process performance measured updates per second best single node performance measured updates per second hardware used spans nearly decade allows direct comparison hardware improvements computation time range showing x increase singlecore performance x increase single process performance x increase single node performance running nearly mit supercloud nodes simultaneously achieved sustained update rate updates per second hierarchical hypersparse graphblas allows mit supercloud analyze extremely large streaming network data sets
Effectiveness of Anonymization in Double-Blind Review,"Double-blind review relies on the authors' ability and willingness to effectively anonymize their submissions. We explore anonymization effectiveness at ASE 2016, OOPSLA 2016, and PLDI 2016 by asking reviewers if they can guess author identities. We find that 74%-90% of reviews contain no correct guess and that reviewers who self-identify as experts on a paper's topic are more likely to attempt to guess, but no more likely to guess correctly. We present our findings, summarize the PC chairs' comments about administering double-blind review, discuss the advantages and disadvantages of revealing author identities part of the way through the process, and conclude by advocating for the continued use of double-blind review.",effectiveness anonymization doubleblind review,doubleblind review relies authors ability willingness effectively anonymize submissions explore anonymization effectiveness ase oopsla pldi asking reviewers guess author identities find reviews contain correct guess reviewers selfidentify experts papers topic likely attempt guess likely guess correctly present findings summarize pc chairs comments administering doubleblind review discuss advantages disadvantages revealing author identities part way process conclude advocating continued use doubleblind review
Max Bense as a Visionary: from Entropy to the Dialectics of Programmed   Images,"In 1960 in Stuttgart, Max Bense published the book Programming the Beautiful [Programmierung des Sch{\""o}nen]. Bense looks in cybernetics for scientific concepts and instigates the thought of programming in the field of literature. His information aesthetics influences a whole generation of scientists and artists - including the Stuttgart Circle, which takes hold of the new aesthetics to carry out the first programmed artistic images. Is Max Bense a visionary? How is he revolutionizing the world of images? The article discusses the cybernetics that inspired Bense: a science of probability that contrasts with the principles of Newtonian physics. Moreover, in the sixties, Max Bense, together with Elisabeth Walther, launched the experimental magazine Rot, which devoted its pages to the concrete poetry and the first computer-generated images of Georg Nees. As Frieder Nake defends through his pioneering work and theory, these images oppose the visible and the computable. This dialectic opens to a critical thinking on the algorithmic image in art and science.",max bense visionary entropy dialectics programmed images,stuttgart max bense published book programming beautiful programmierung des schonen bense looks cybernetics scientific concepts instigates thought programming field literature information aesthetics influences whole generation scientists artists including stuttgart circle takes hold new aesthetics carry first programmed artistic images max bense visionary revolutionizing world images article discusses cybernetics inspired bense science probability contrasts principles newtonian physics moreover sixties max bense together elisabeth walther launched experimental magazine rot devoted pages concrete poetry first computergenerated images georg nees frieder nake defends pioneering work theory images oppose visible computable dialectic opens critical thinking algorithmic image art science
Supporting Knowledge and Expertise Finding within Australia's Defence   Science and Technology Organisation,"This paper reports on work aimed at supporting knowledge and expertise finding within a large Research and Development (R&D) organisation. The paper first discusses the nature of knowledge important to R&D organisations and presents a prototype information system developed to support knowledge and expertise finding. The paper then discusses a trial of the system within an R&D organisation, the implications and limitations of the trial, and discusses future research questions.",supporting knowledge expertise finding within australias defence science technology organisation,paper reports work aimed supporting knowledge expertise finding within large research development rd organisation paper first discusses nature knowledge important rd organisations presents prototype information system developed support knowledge expertise finding paper discusses trial system within rd organisation implications limitations trial discusses future research questions
Super-Linear Speedup by Generalizing Runtime Repeated Recursion   Unfolding in Prolog,"Runtime repeated recursion unfolding was recently introduced as a just-in-time program transformation strategy that can achieve super-linear speedup. So far, the method was restricted to single linear direct recursive rules in the programming language Constraint Handling Rules (CHR). In this companion paper, we generalize the technique to multiple recursion and to multiple recursive rules and provide an implementation of the generalized method in the logic programming language Prolog.   The basic idea of the approach is as follows: When a recursive call is encountered at runtime, the recursive rule is unfolded with itself and this process is repeated with each resulting unfolded rule as long as it is applicable to the current call. In this way, more and more recursive steps are combined into one recursive step. Then an interpreter applies these rules to the call starting from the most unfolded rule. For recursions which have sufficiently simplifyable unfoldings, a super-linear can be achieved, i.e. the time complexity is reduced.   We implement an unfolder, a generalized meta-interpreter and a novel round-robin rule processor for our generalization of runtime repeated recursion unfolding with just ten clauses in Prolog. We illustrate the feasibility of our technique with worst-case time complexity estimates and benchmarks for some basic classical algorithms that achieve a super-linear speedup.",superlinear speedup generalizing runtime repeated recursion unfolding prolog,runtime repeated recursion unfolding recently introduced justintime program transformation strategy achieve superlinear speedup far method restricted single linear direct recursive rules programming language constraint handling rules chr companion paper generalize technique multiple recursion multiple recursive rules provide implementation generalized method logic programming language prolog basic idea approach follows recursive call encountered runtime recursive rule unfolded process repeated resulting unfolded rule long applicable current call way recursive steps combined one recursive step interpreter applies rules call starting unfolded rule recursions sufficiently simplifyable unfoldings superlinear achieved ie time complexity reduced implement unfolder generalized metainterpreter novel roundrobin rule processor generalization runtime repeated recursion unfolding ten clauses prolog illustrate feasibility technique worstcase time complexity estimates benchmarks basic classical algorithms achieve superlinear speedup
MS-BioGraphs: Sequence Similarity Graph Datasets,"Progress in High-Performance Computing in general, and High-Performance Graph Processing in particular, is highly dependent on the availability of publicly-accessible, relevant, and realistic data sets.   To ensure continuation of this progress, we (i) investigate and optimize the process of generating large sequence similarity graphs as an HPC challenge and (ii) demonstrate this process in creating MS-BioGraphs, a new family of publicly available real-world edge-weighted graph datasets with up to $2.5$ trillion edges, that is, $6.6$ times greater than the largest graph published recently. The largest graph is created by matching (i.e., all-to-all similarity aligning) $1.7$ billion protein sequences. The MS-BioGraphs family includes also seven subgraphs with different sizes and direction types.   We describe two main challenges we faced in generating large graph datasets and our solutions, that are, (i) optimizing data structures and algorithms for this multi-step process and (ii) WebGraph parallel compression technique. We present a comparative study of structural characteristics of MS-BioGraphs.   The datasets are available online on https://blogs.qub.ac.uk/DIPSA/MS-BioGraphs .",msbiographs sequence similarity graph datasets,progress highperformance computing general highperformance graph processing particular highly dependent availability publiclyaccessible relevant realistic data sets ensure continuation progress investigate optimize process generating large sequence similarity graphs hpc challenge ii demonstrate process creating msbiographs new family publicly available realworld edgeweighted graph datasets trillion edges times greater largest graph published recently largest graph created matching ie alltoall similarity aligning billion protein sequences msbiographs family includes also seven subgraphs different sizes direction types describe two main challenges faced generating large graph datasets solutions optimizing data structures algorithms multistep process ii webgraph parallel compression technique present comparative study structural characteristics msbiographs datasets available online httpsblogsqubacukdipsamsbiographs
DPCL: a Language Template for Normative Specifications,"Several solutions for specifying normative artefacts (norms, contracts, policies) in a computational processable way have been presented in the literature. Legal core ontologies have been proposed to systematize concepts and relationships relevant to normative reasoning. However, no solution amongst those has achieved general acceptance, and no common ground (representational, computational) has been identified enabling us to easily compare them. Yet, all these efforts share the same motivation of representing normative directives, therefore it is plausible that there may be a representational model encompassing all of them. This presentation will introduce DPCL, a domain-specific language (DSL) for specifying higher-level policies (including norms, contracts, etc.), centred on Hohfeld's framework of fundamental legal concepts. DPCL has to be seen primarily as a ""template"", i.e. as an informational model for architectural reference, rather than a fully-fledged formal language; it aims to make explicit the general requirements that should be expected in a language for norm specification. In this respect, it goes rather in the direction of legal core ontologies, but differently from those, our proposal aims to keep the character of a DSL, rather than a set of axioms in a logical framework: it is meant to be cross-compiled to underlying languages/tools adequate to the type of target application. We provide here an overview of some of the language features.",dpcl language template normative specifications,several solutions specifying normative artefacts norms contracts policies computational processable way presented literature legal core ontologies proposed systematize concepts relationships relevant normative reasoning however solution amongst achieved general acceptance common ground representational computational identified enabling us easily compare yet efforts share motivation representing normative directives therefore plausible may representational model encompassing presentation introduce dpcl domainspecific language dsl specifying higherlevel policies including norms contracts etc centred hohfelds framework fundamental legal concepts dpcl seen primarily template ie informational model architectural reference rather fullyfledged formal language aims make explicit general requirements expected language norm specification respect goes rather direction legal core ontologies differently proposal aims keep character dsl rather set axioms logical framework meant crosscompiled underlying languagestools adequate type target application provide overview language features
A Priority-Aware Multiqueue NIC Design,"Low-level embedded systems are used to control cyber-phyiscal systems in industrial and autonomous applications. They need to meet hard real-time requirements as unanticipated controller delays on moving machines can have devastating effects. Modern developments such as the industrial Internet of Things and autonomous machines require these devices to connect to large IP networks. Since Network Interface Controllers (NICs) trigger interrupts for incoming packets, real-time embedded systems are subject to unpredictable preemptions when connected to such networks.   In this work, we propose a priority-aware NIC design to moderate network-generated interrupts by mapping IP flows to processes and based on that, consolidates their packets into different queues. These queues apply priority-dependent interrupt moderation. First experimental evaluations show that 93% of interrupts can be saved leading to an 80% decrease of processing delay of critical tasks in the configurations investigated.",priorityaware multiqueue nic design,lowlevel embedded systems used control cyberphyiscal systems industrial autonomous applications need meet hard realtime requirements unanticipated controller delays moving machines devastating effects modern developments industrial internet things autonomous machines require devices connect large ip networks since network interface controllers nics trigger interrupts incoming packets realtime embedded systems subject unpredictable preemptions connected networks work propose priorityaware nic design moderate networkgenerated interrupts mapping ip flows processes based consolidates packets different queues queues apply prioritydependent interrupt moderation first experimental evaluations show interrupts saved leading decrease processing delay critical tasks configurations investigated
"Towards Fast, Adaptive, and Hardware-Assisted User-Space Scheduling","Modern datacenter applications are prone to high tail latencies since their requests typically follow highly-dispersive distributions. Delivering fast interrupts is essential to reducing tail latency. Prior work has proposed both OS- and system-level solutions to reduce tail latencies for microsecond-scale workloads through better scheduling. Unfortunately, existing approaches like customized dataplane OSes, require significant OS changes, experience scalability limitations, or do not reach the full performance capabilities hardware offers.   The emergence of new hardware features like UINTR exposed new opportunities to rethink the design paradigms and abstractions of traditional scheduling systems. We propose LibPreemptible, a preemptive user-level threading library that is flexible, lightweight, and adaptive. LibPreemptible was built with a set of optimizations like LibUtimer for scalability, and deadline-oriented API for flexible policies, time-quantum controller for adaptiveness. Compared to the prior state-of-the-art scheduling system Shinjuku, our system achieves significant tail latency and throughput improvements for various workloads without modifying the kernel. We also demonstrate the flexibility of LibPreemptible across scheduling policies for real applications experiencing varying load levels and characteristics.",towards fast adaptive hardwareassisted userspace scheduling,modern datacenter applications prone high tail latencies since requests typically follow highlydispersive distributions delivering fast interrupts essential reducing tail latency prior work proposed os systemlevel solutions reduce tail latencies microsecondscale workloads better scheduling unfortunately existing approaches like customized dataplane oses require significant os changes experience scalability limitations reach full performance capabilities hardware offers emergence new hardware features like uintr exposed new opportunities rethink design paradigms abstractions traditional scheduling systems propose libpreemptible preemptive userlevel threading library flexible lightweight adaptive libpreemptible built set optimizations like libutimer scalability deadlineoriented api flexible policies timequantum controller adaptiveness compared prior stateoftheart scheduling system shinjuku system achieves significant tail latency throughput improvements various workloads without modifying kernel also demonstrate flexibility libpreemptible across scheduling policies real applications experiencing varying load levels characteristics
High-performance symbolic-numerics via multiple dispatch,"As mathematical computing becomes more democratized in high-level languages, high-performance symbolic-numeric systems are necessary for domain scientists and engineers to get the best performance out of their machine without deep knowledge of code optimization. Naturally, users need different term types either to have different algebraic properties for them, or to use efficient data structures. To this end, we developed Symbolics.jl, an extendable symbolic system which uses dynamic multiple dispatch to change behavior depending on the domain needs. In this work we detail an underlying abstract term interface which allows for speed without sacrificing generality. We show that by formalizing a generic API on actions independent of implementation, we can retroactively add optimized data structures to our system without changing the pre-existing term rewriters. We showcase how this can be used to optimize term construction and give a 113x acceleration on general symbolic transformations. Further, we show that such a generic API allows for complementary term-rewriting implementations. We demonstrate the ability to swap between classical term-rewriting simplifiers and e-graph-based term-rewriting simplifiers. We showcase an e-graph ruleset which minimizes the number of CPU cycles during expression evaluation, and demonstrate how it simplifies a real-world reaction-network simulation to halve the runtime. Additionally, we show a reaction-diffusion partial differential equation solver which is able to be automatically converted into symbolic expressions via multiple dispatch tracing, which is subsequently accelerated and parallelized to give a 157x simulation speedup. Together, this presents Symbolics.jl as a next-generation symbolic-numeric computing environment geared towards modeling and simulation.",highperformance symbolicnumerics via multiple dispatch,mathematical computing becomes democratized highlevel languages highperformance symbolicnumeric systems necessary domain scientists engineers get best performance machine without deep knowledge code optimization naturally users need different term types either different algebraic properties use efficient data structures end developed symbolicsjl extendable symbolic system uses dynamic multiple dispatch change behavior depending domain needs work detail underlying abstract term interface allows speed without sacrificing generality show formalizing generic api actions independent implementation retroactively add optimized data structures system without changing preexisting term rewriters showcase used optimize term construction give x acceleration general symbolic transformations show generic api allows complementary termrewriting implementations demonstrate ability swap classical termrewriting simplifiers egraphbased termrewriting simplifiers showcase egraph ruleset minimizes number cpu cycles expression evaluation demonstrate simplifies realworld reactionnetwork simulation halve runtime additionally show reactiondiffusion partial differential equation solver able automatically converted symbolic expressions via multiple dispatch tracing subsequently accelerated parallelized give x simulation speedup together presents symbolicsjl nextgeneration symbolicnumeric computing environment geared towards modeling simulation
Advancing Data Justice Research and Practice: An Integrated Literature   Review,"The Advancing Data Justice Research and Practice (ADJRP) project aims to widen the lens of current thinking around data justice and to provide actionable resources that will help policymakers, practitioners, and impacted communities gain a broader understanding of what equitable, freedom-promoting, and rights-sustaining data collection, governance, and use should look like in increasingly dynamic and global data innovation ecosystems. In this integrated literature review we hope to lay the conceptual groundwork needed to support this aspiration. The introduction motivates the broadening of data justice that is undertaken by the literature review which follows. First, we address how certain limitations of the current study of data justice drive the need for a re-location of data justice research and practice. We map out the strengths and shortcomings of the contemporary state of the art and then elaborate on the challenges faced by our own effort to broaden the data justice perspective in the decolonial context. The body of the literature review covers seven thematic areas. For each theme, the ADJRP team has systematically collected and analysed key texts in order to tell the critical empirical story of how existing social structures and power dynamics present challenges to data justice and related justice fields. In each case, this critical empirical story is also supplemented by the transformational story of how activists, policymakers, and academics are challenging longstanding structures of inequity to advance social justice in data innovation ecosystems and adjacent areas of technological practice.",advancing data justice research practice integrated literature review,advancing data justice research practice adjrp project aims widen lens current thinking around data justice provide actionable resources help policymakers practitioners impacted communities gain broader understanding equitable freedompromoting rightssustaining data collection governance use look like increasingly dynamic global data innovation ecosystems integrated literature review hope lay conceptual groundwork needed support aspiration introduction motivates broadening data justice undertaken literature review follows first address certain limitations current study data justice drive need relocation data justice research practice map strengths shortcomings contemporary state art elaborate challenges faced effort broaden data justice perspective decolonial context body literature review covers seven thematic areas theme adjrp team systematically collected analysed key texts order tell critical empirical story existing social structures power dynamics present challenges data justice related justice fields case critical empirical story also supplemented transformational story activists policymakers academics challenging longstanding structures inequity advance social justice data innovation ecosystems adjacent areas technological practice
If a tree casts a shadow is it telling the time?,"Physical processes are computations only when we use them to externalize thought. Computation is the performance of one or more fixed processes within a contingent environment. We reformulate the Church-Turing thesis so that it applies to programs rather than to computability. When suitably formulated agent-based computing in an open, multi-scalar environment represents the current consensus view of how we interact with the world. But we don't know how to formulate multi-scalar environments.",tree casts shadow telling time,physical processes computations use externalize thought computation performance one fixed processes within contingent environment reformulate churchturing thesis applies programs rather computability suitably formulated agentbased computing open multiscalar environment represents current consensus view interact world dont know formulate multiscalar environments
Computational Solutions for Today's Navy,New methods are being employed to meet the Navy's changing software-development environment.,computational solutions todays navy,new methods employed meet navys changing softwaredevelopment environment
Proceedings of the 5th Workshop on Membrane Computing and Biologically   Inspired Process Calculi (MeCBIC 2011),"This volume represents the proceedings of the 5th Workshop on Membrane Computing and Biologically Inspired Process Calculi (MeCBIC 2011), held together with the 12th International Conference on Membrane Computing on 23rd August 2011 in Fontainebleau, France.",proceedings th workshop membrane computing biologically inspired process calculi mecbic,volume represents proceedings th workshop membrane computing biologically inspired process calculi mecbic held together th international conference membrane computing rd august fontainebleau france
Proceedings First Workshop on CTP Components for Educational Software,"The THedu'11 workshop received thirteen submissions, twelve of which were accepted and presented during the workshop. For the post-conference proceedings nine submission where received and accepted. The submissions are within the scope of the following points, which have been announced in the call of papers: CTP-based software tools for education; CTP technology combined with novel interfaces, drag and drop, etc.; technologies to access ITP knowledge relevant for a certain step of problem solving; usability considerations on representing ITP knowledge; combination of deduction and computation; formal problem specifications; effectiveness of ATP in checking user input; formats for deductive content in proof documents, geometric constructions, etc; formal domain models for e-learning in mathematics and applications.",proceedings first workshop ctp components educational software,thedu workshop received thirteen submissions twelve accepted presented workshop postconference proceedings nine submission received accepted submissions within scope following points announced call papers ctpbased software tools education ctp technology combined novel interfaces drag drop etc technologies access itp knowledge relevant certain step problem solving usability considerations representing itp knowledge combination deduction computation formal problem specifications effectiveness atp checking user input formats deductive content proof documents geometric constructions etc formal domain models elearning mathematics applications
Making FPGAs Accessible to Scientists and Engineers as Domain Expert   Software Programmers with LabVIEW,"In this paper we present a graphical programming framework, LabVIEW, and associated language and libraries, as well as programming techniques and patterns that we have found useful in making FPGAs accessible to scientists and engineers as domain expert software programmers.",making fpgas accessible scientists engineers domain expert software programmers labview,paper present graphical programming framework labview associated language libraries well programming techniques patterns found useful making fpgas accessible scientists engineers domain expert software programmers
Compact Formulae in Sparse Elimination,"It has by now become a standard approach to use the theory of sparse (or toric) elimination, based on the Newton polytope of a polynomial, in order to reveal and exploit the structure of algebraic systems. This talk surveys compact formulae, including older and recent results, in sparse elimination. We start with root bounds and juxtapose two recent formulae: a generating function of the m-B{\'e}zout bound and a closed-form expression for the mixed volume by means of a matrix permanent. For the sparse resultant, a bevy of results have established determinantal or rational formulae for a large class of systems, starting with Macaulay. The discriminant is closely related to the resultant but admits no compact formula except for very simple cases. We offer a new determinantal formula for the discriminant of a sparse multilinear system arising in computing Nash equilibria. We introduce an alternative notion of compact formula, namely the Newton polytope of the unknown polynomial. It is possible to compute it efficiently for sparse resultants, discriminants, as well as the implicit equation of a parameterized variety. This leads us to consider implicit matrix representations of geometric objects.",compact formulae sparse elimination,become standard approach use theory sparse toric elimination based newton polytope polynomial order reveal exploit structure algebraic systems talk surveys compact formulae including older recent results sparse elimination start root bounds juxtapose two recent formulae generating function mbezout bound closedform expression mixed volume means matrix permanent sparse resultant bevy results established determinantal rational formulae large class systems starting macaulay discriminant closely related resultant admits compact formula except simple cases offer new determinantal formula discriminant sparse multilinear system arising computing nash equilibria introduce alternative notion compact formula namely newton polytope unknown polynomial possible compute efficiently sparse resultants discriminants well implicit equation parameterized variety leads us consider implicit matrix representations geometric objects
Folding One Polyhedral Metric Graph into Another,"We analyze the problem of folding one polyhedron, viewed as a metric graph of its edges, into the shape of another, similar to 1D origami. We find such foldings between all pairs of Platonic solids and prove corresponding lower bounds, establishing the optimal scale factor when restricted to integers. Further, we establish that our folding problem is also NP-hard, even if the source graph is a tree. It turns out that the problem is hard to approximate, as we obtain NP-hardness even for determining the existence of a scale factor 1.5-{\epsilon}. Finally, we prove that, in general, the optimal scale factor has to be rational. This insight then immediately results in NP membership. In turn, verifying whether a given scale factor is indeed the smallest possible, requires two independent calls to an NP oracle, rendering the problem DP-complete.",folding one polyhedral metric graph another,analyze problem folding one polyhedron viewed metric graph edges shape another similar origami find foldings pairs platonic solids prove corresponding lower bounds establishing optimal scale factor restricted integers establish folding problem also nphard even source graph tree turns problem hard approximate obtain nphardness even determining existence scale factor epsilon finally prove general optimal scale factor rational insight immediately results np membership turn verifying whether given scale factor indeed smallest possible requires two independent calls np oracle rendering problem dpcomplete
The Persistent Buffer Tree : An I/O-efficient Index for Temporal Data,"In a variety of applications, we need to keep track of the development of a data set over time. For maintaining and querying this multi version data I/O-efficiently, external memory data structures are required. In this paper, we present a probabilistic self-balancing persistent data structure in external memory called the persistent buffer tree, which supports insertions, updates and deletions of data items at the present version and range queries for any version, past or present. The persistent buffer tree is I/O-optimal in the sense that the expected amortized I/O performance bounds are asymptotically the same as the deterministic amortized bounds of the (single version) buffer tree in the worst case.",persistent buffer tree ioefficient index temporal data,variety applications need keep track development data set time maintaining querying multi version data ioefficiently external memory data structures required paper present probabilistic selfbalancing persistent data structure external memory called persistent buffer tree supports insertions updates deletions data items present version range queries version past present persistent buffer tree iooptimal sense expected amortized io performance bounds asymptotically deterministic amortized bounds single version buffer tree worst case
Naughton's Wisconsin Bibliography: A Brief Guide,"Over nearly three decades at the University of Wisconsin, Jeff Naughton has left an indelible mark on computer science. He has been a global leader of the database research field, deepening its core and pushing its boundaries. Many of Naughton's ideas were translated directly into practice in commercial and open-source systems. But software comes and goes. In the end, it is the ideas themselves that have had impact, ideas written down in papers.   Naughton has been a prolific scholar over the last thirty years, with over 175 publications in his bibliography, covering a wide range of topics. This document does not attempt to enumerate or even summarize the wealth of ideas that Naughton has published over the course of his academic career--the task is too daunting. Instead, the best this short note aims to do is to serve as a rough map of the territory: something to help other researchers navigate the wide spaces of Naughton's work.",naughtons wisconsin bibliography brief guide,nearly three decades university wisconsin jeff naughton left indelible mark computer science global leader database research field deepening core pushing boundaries many naughtons ideas translated directly practice commercial opensource systems software comes goes end ideas impact ideas written papers naughton prolific scholar last thirty years publications bibliography covering wide range topics document attempt enumerate even summarize wealth ideas naughton published course academic careerthe task daunting instead best short note aims serve rough map territory something help researchers navigate wide spaces naughtons work
Towards Specificationless Monitoring of Provenance-Emitting Systems,"Monitoring often requires insight into the monitored system as well as concrete specifications of expected behavior. More and more systems, however, provide information about their inner procedures by emitting provenance information in a W3C-standardized graph format.   In this work, we present an approach to monitor such provenance data for anomalous behavior by performing spectral graph analysis on slices of the constructed provenance graph and by comparing the characteristics of each slice with those of a sliding window over recently seen slices. We argue that this approach not only simplifies the monitoring of heterogeneous distributed systems, but also enables applying a host of well-studied techniques to monitor such systems.",towards specificationless monitoring provenanceemitting systems,monitoring often requires insight monitored system well concrete specifications expected behavior systems however provide information inner procedures emitting provenance information wcstandardized graph format work present approach monitor provenance data anomalous behavior performing spectral graph analysis slices constructed provenance graph comparing characteristics slice sliding window recently seen slices argue approach simplifies monitoring heterogeneous distributed systems also enables applying host wellstudied techniques monitor systems
The 4+1 Model of Data Science,"Data Science is a complex and evolving field, but most agree that it can be defined as a combination of expertise drawn from three broad areascomputer science and technology, math and statistics, and domain knowledge -- with the purpose of extracting knowledge and value from data. Beyond this, the field is often defined as a series of practical activities ranging from the cleaning and wrangling of data, to its analysis and use to infer models, to the visual and rhetorical representation of results to stakeholders and decision-makers. This essay proposes a model of data science that goes beyond laundry-list definitions to get at the specific nature of data science and help distinguish it from adjacent fields such as computer science and statistics. We define data science as an interdisciplinary field comprising four broad areas of expertise: value, design, systems, and analytics. A fifth area, practice, integrates the other four in specific contexts of domain knowledge. We call this the 4+1 model of data science. Together, these areas belong to every data science project, even if they are often unconnected and siloed in the academy.",model data science,data science complex evolving field agree defined combination expertise drawn three broad areascomputer science technology math statistics domain knowledge purpose extracting knowledge value data beyond field often defined series practical activities ranging cleaning wrangling data analysis use infer models visual rhetorical representation results stakeholders decisionmakers essay proposes model data science goes beyond laundrylist definitions get specific nature data science help distinguish adjacent fields computer science statistics define data science interdisciplinary field comprising four broad areas expertise value design systems analytics fifth area practice integrates four specific contexts domain knowledge call model data science together areas belong every data science project even often unconnected siloed academy
Efficient Synchronization Primitives for GPUs,"In this paper, we revisit the design of synchronization primitives---specifically barriers, mutexes, and semaphores---and how they apply to the GPU. Previous implementations are insufficient due to the discrepancies in hardware and programming model of the GPU and CPU. We create new implementations in CUDA and analyze the performance of spinning on the GPU, as well as a method of sleeping on the GPU, by running a set of memory-system benchmarks on two of the most common GPUs in use, the Tesla- and Fermi-class GPUs from NVIDIA. From our results we define higher-level principles that are valid for generic many-core processors, the most important of which is to limit the number of atomic accesses required for a synchronization operation because atomic accesses are slower than regular memory accesses. We use the results of the benchmarks to critique existing synchronization algorithms and guide our new implementations, and then define an abstraction of GPUs to classify any GPU based on the behavior of the memory system. We use this abstraction to create suitable implementations of the primitives specifically targeting the GPU, and analyze the performance of these algorithms on Tesla and Fermi. We then predict performance on future GPUs based on characteristics of the abstraction. We also examine the roles of spin waiting and sleep waiting in each primitive and how their performance varies based on the machine abstraction, then give a set of guidelines for when each strategy is useful based on the characteristics of the GPU and expected contention.",efficient synchronization primitives gpus,paper revisit design synchronization primitivesspecifically barriers mutexes semaphoresand apply gpu previous implementations insufficient due discrepancies hardware programming model gpu cpu create new implementations cuda analyze performance spinning gpu well method sleeping gpu running set memorysystem benchmarks two common gpus use tesla fermiclass gpus nvidia results define higherlevel principles valid generic manycore processors important limit number atomic accesses required synchronization operation atomic accesses slower regular memory accesses use results benchmarks critique existing synchronization algorithms guide new implementations define abstraction gpus classify gpu based behavior memory system use abstraction create suitable implementations primitives specifically targeting gpu analyze performance algorithms tesla fermi predict performance future gpus based characteristics abstraction also examine roles spin waiting sleep waiting primitive performance varies based machine abstraction give set guidelines strategy useful based characteristics gpu expected contention
Fundamental concepts in the Cyclus nuclear fuel cycle simulation   framework,"As nuclear power expands, technical, economic, political, and environmental analyses of nuclear fuel cycles by simulators increase in importance. To date, however, current tools are often fleet-based rather than discrete and restrictively licensed rather than open source. Each of these choices presents a challenge to modeling fidelity, generality, efficiency, robustness, and scientific transparency. The Cyclus nuclear fuel cycle simulator framework and its modeling ecosystem incorporate modern insights from simulation science and software architecture to solve these problems so that challenges in nuclear fuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle simulator framework and its modeling ecosystem are presented. Additionally, the implementation of each is discussed in the context of motivating challenges in nuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are demonstrated for both open and closed fuel cycles.",fundamental concepts cyclus nuclear fuel cycle simulation framework,nuclear power expands technical economic political environmental analyses nuclear fuel cycles simulators increase importance date however current tools often fleetbased rather discrete restrictively licensed rather open source choices presents challenge modeling fidelity generality efficiency robustness scientific transparency cyclus nuclear fuel cycle simulator framework modeling ecosystem incorporate modern insights simulation science software architecture solve problems challenges nuclear fuel cycle analysis better addressed summary cyclus fuel cycle simulator framework modeling ecosystem presented additionally implementation discussed context motivating challenges nuclear fuel cycle simulation finally current capabilities cyclus demonstrated open closed fuel cycles
Modeling Terms by Graphs with Structure Constraints (Two Illustrations),"In the talk at the workshop my aim was to demonstrate the usefulness of graph techniques for tackling problems that have been studied predominantly as problems on the term level: increasing sharing in functional programs, and addressing questions about Milner's process semantics for regular expressions. For both situations an approach that is based on modeling terms by graphs with structure constraints has turned out to be fruitful. In this extended abstract I describe the underlying problems, give references, provide examples, indicate the chosen approaches, and compare the initial situations as well as the results that have been obtained, and some results that are being developed at present.",modeling terms graphs structure constraints two illustrations,talk workshop aim demonstrate usefulness graph techniques tackling problems studied predominantly problems term level increasing sharing functional programs addressing questions milners process semantics regular expressions situations approach based modeling terms graphs structure constraints turned fruitful extended abstract describe underlying problems give references provide examples indicate chosen approaches compare initial situations well results obtained results developed present
The complexity gap in the static analysis of cache accesses grows if   procedure calls are added,"The static analysis of cache accesses consists in correctly predicting which accesses are hits or misses. While there exist good exact and approximate analyses for caches implementing the least recently used (LRU) replacement policy, such analyses were harder to find for other replacement policies. A theoretical explanation was found: for an appropriate setting of analysis over control-flow graphs, cache analysis is PSPACE-complete for all common replacement policies (FIFO, PLRU, NMRU) except for LRU, for which it is only NP-complete. In this paper, we show that if procedure calls are added to the control flow, then the gap widens: analysis remains NP-complete for LRU, but becomes EXPTIME-complete for the three other policies. For this, we improve on earlier results on the complexity of reachability problems on Boolean programs with procedure calls. In addition, for the LRU policy we derive a backtracking algorithm as well as an approach for using it as a last resort after other analyses have failed to conclude.",complexity gap static analysis cache accesses grows procedure calls added,static analysis cache accesses consists correctly predicting accesses hits misses exist good exact approximate analyses caches implementing least recently used lru replacement policy analyses harder find replacement policies theoretical explanation found appropriate setting analysis controlflow graphs cache analysis pspacecomplete common replacement policies fifo plru nmru except lru npcomplete paper show procedure calls added control flow gap widens analysis remains npcomplete lru becomes exptimecomplete three policies improve earlier results complexity reachability problems boolean programs procedure calls addition lru policy derive backtracking algorithm well approach using last resort analyses failed conclude
Efficient and Scalable Architecture for Multiple-chip Implementation of   Simulated Bifurcation Machines,"Ising machines are specialized computers for finding the lowest energy states of Ising spin models, onto which many practical combinatorial optimization problems can be mapped. Simulated bifurcation (SB) is a quantum-inspired parallelizable algorithm for Ising problems that enables scalable multi-chip implementations of Ising machines. However, the computational performance of a previously proposed multi-chip architecture tends to saturate as the number of chips increases for a given problem size because both computation and communication are exclusive in the time domain. In this paper, we propose a streaming architecture for multi-chip implementations of SB-based Ising machines with full spin-to-spin connectivity. The data flow in in-chip computation is harmonized with the data flow in inter-chip communication, enabling the computation and communication to overlap and the communication time to be hidden. Systematic experiments demonstrate linear strong scaling of performance up to the vicinity of the ideal communication limit determined only by the latency of chip-to-chip communication. Our eight-FPGA (field-programmable gate array) cluster can compute a 32,768-spin problem with a high pipeline efficiency of 97.9%. The performance of a 79-FPGA cluster for a 100,000-spin problem, projected using a theoretical performance model validated on smaller experimental clusters, is comparable to that of a state-of-the-art 100,000-spin optical Ising machine.",efficient scalable architecture multiplechip implementation simulated bifurcation machines,ising machines specialized computers finding lowest energy states ising spin models onto many practical combinatorial optimization problems mapped simulated bifurcation sb quantuminspired parallelizable algorithm ising problems enables scalable multichip implementations ising machines however computational performance previously proposed multichip architecture tends saturate number chips increases given problem size computation communication exclusive time domain paper propose streaming architecture multichip implementations sbbased ising machines full spintospin connectivity data flow inchip computation harmonized data flow interchip communication enabling computation communication overlap communication time hidden systematic experiments demonstrate linear strong scaling performance vicinity ideal communication limit determined latency chiptochip communication eightfpga fieldprogrammable gate array cluster compute spin problem high pipeline efficiency performance fpga cluster spin problem projected using theoretical performance model validated smaller experimental clusters comparable stateoftheart spin optical ising machine
The Secure Machine: Efficient Secure Execution On Untrusted Platforms,"In this work we present the Secure Machine, SeM for short, a CPU architecture extension for secure computing. SeM uses a small amount of in-chip additional hardware that monitors key communication channels inside the CPU chip, and only acts when required. SeM provides confidentiality and integrity for a secure program without trusting the platform software or any off-chip hardware. SeM supports existing binaries of single- and multi-threaded applications running on single- or multi-core, multi-CPU. The performance reduction caused by it is only few percent, most of which is due to the memory encryption layer that is commonly used in many secure architectures.   We also developed SeM-Prepare, a software tool that automatically instruments existing applications (binaries) with additional instructions so they can be securely executed on our architecture without requiring any programming efforts or the availability of the desired program`s source code.   To enable secure data sharing in shared memory environments, we developed Secure Distributed Shared Memory (SDSM), an efficient (time and memory) algorithm for allowing thousands of compute nodes to share data securely while running on an untrusted computing environment. SDSM shows a negligible reduction in performance, and it requires negligible and hardware resources. We developed Distributed Memory Integrity Trees, a method for enhancing single node integrity trees for preserving the integrity of a distributed application running on an untrusted computing environment. We show that our method is applicable to existing single node integrity trees such as Merkle Tree, Bonsai Merkle Tree, and Intel`s SGX memory integrity engine. All these building blocks may be used together to form a practical secure system, and some can be used in conjunction with other secure systems.",secure machine efficient secure execution untrusted platforms,work present secure machine sem short cpu architecture extension secure computing sem uses small amount inchip additional hardware monitors key communication channels inside cpu chip acts required sem provides confidentiality integrity secure program without trusting platform software offchip hardware sem supports existing binaries single multithreaded applications running single multicore multicpu performance reduction caused percent due memory encryption layer commonly used many secure architectures also developed semprepare software tool automatically instruments existing applications binaries additional instructions securely executed architecture without requiring programming efforts availability desired programs source code enable secure data sharing shared memory environments developed secure distributed shared memory sdsm efficient time memory algorithm allowing thousands compute nodes share data securely running untrusted computing environment sdsm shows negligible reduction performance requires negligible hardware resources developed distributed memory integrity trees method enhancing single node integrity trees preserving integrity distributed application running untrusted computing environment show method applicable existing single node integrity trees merkle tree bonsai merkle tree intels sgx memory integrity engine building blocks may used together form practical secure system used conjunction secure systems
Writing and Editing Complexity Theory: Tales and Tools,"Each researcher should have a full shelf---physical or virtual---of books on writing and editing prose. Though we make no claim to any special degree of expertise, we recently edited a book of complexity theory surveys (Complexity Theory Retrospective II, Springer-Verlag, 1997), and in doing so we were brought into particularly close contact with the subject of this article, and with a number of the excellent resources available to writers and editors. In this article, we list some of these resources, and we also relate some of the adventures we had as our book moved from concept to reality.",writing editing complexity theory tales tools,researcher full shelfphysical virtualof books writing editing prose though make claim special degree expertise recently edited book complexity theory surveys complexity theory retrospective ii springerverlag brought particularly close contact subject article number excellent resources available writers editors article list resources also relate adventures book moved concept reality
Complexity Science for Simpletons,"In this article, we shall describe some of the most interesting topics in the subject of Complexity Science for a general audience. Anyone with a solid foundation in high school mathematics (with some calculus) and an elementary understanding of computer programming will be able to follow this article. First, we shall explain the significance of the P versus NP problem and solve it. Next, we shall describe two other famous mathematics problems, the Collatz 3n+1 Conjecture and the Riemann Hypothesis, and show how both Chaitin's incompleteness theorem and Wolfram's notion of ""computational irreducibility"" are important for understanding why no one has, as of yet, solved these two problems.",complexity science simpletons,article shall describe interesting topics subject complexity science general audience anyone solid foundation high school mathematics calculus elementary understanding computer programming able follow article first shall explain significance p versus np problem solve next shall describe two famous mathematics problems collatz n conjecture riemann hypothesis show chaitins incompleteness theorem wolframs notion computational irreducibility important understanding one yet solved two problems
Long-Term Mentoring for Computer Science Researchers,"Early in the pandemic, we -- leaders in the research areas of programming languages (PL) and computer architecture (CA) -- realized that we had a problem: the only way to form new lasting connections in the community was to already have lasting connections in the community. Both of our academic communities had wonderful short-term mentoring programs to address this problem, but it was clear that we needed long-term mentoring programs.   Those of us in CA approached this scientifically, making an evidence-backed case for community-wide long-term mentoring. In the meantime, one of us in PL had impulsively launched an unofficial long-term mentoring program, founded on chaos and spreadsheets. In January 2021, the latter grew to an official cross-institutional long-term mentoring program called SIGPLAN-M; in January 2022, the former grew to Computer Architecture Long-term Mentoring (CALM).   The impacts have been strong: SIGPLAN-M reaches 328 mentees and 234 mentors across 41 countries, and mentees have described it as ""life changing"" and ""a career saver."" And while CALM is in its pilot phase -- with 13 mentors and 21 mentees across 7 countries -- it has received very positive feedback. The leaders of SIGPLAN-M and CALM shared our designs, impacts, and challenges along the way. Now, we wish to share those with you. We hope this will kick-start a larger long-term mentoring effort across all of computer science.",longterm mentoring computer science researchers,early pandemic leaders research areas programming languages pl computer architecture ca realized problem way form new lasting connections community already lasting connections community academic communities wonderful shortterm mentoring programs address problem clear needed longterm mentoring programs us ca approached scientifically making evidencebacked case communitywide longterm mentoring meantime one us pl impulsively launched unofficial longterm mentoring program founded chaos spreadsheets january latter grew official crossinstitutional longterm mentoring program called sigplanm january former grew computer architecture longterm mentoring calm impacts strong sigplanm reaches mentees mentors across countries mentees described life changing career saver calm pilot phase mentors mentees across countries received positive feedback leaders sigplanm calm shared designs impacts challenges along way wish share hope kickstart larger longterm mentoring effort across computer science
Symbolic Abstract Heaps for Polymorphic Information-flow Guard Inference   (Extended Version),"In the realm of sound object-oriented program analyses for information-flow control, very few approaches adopt flow-sensitive abstractions of the heap that enable a precise modeling of implicit flows. To tackle this challenge, we advance a new symbolic abstraction approach for modeling the heap in Java-like programs. We use a store-less representation that is parameterized with a family of relations among references to offer various levels of precision based on user preferences. This enables us to automatically infer polymorphic information-flow guards for methods via a co-reachability analysis of a symbolic finite-state system. We instantiate the heap abstraction with three different families of relations. We prove the soundness of our approach and compare the precision and scalability obtained with each instantiated heap domain by using the IFSpec benchmarks and real-life applications.",symbolic abstract heaps polymorphic informationflow guard inference extended version,realm sound objectoriented program analyses informationflow control approaches adopt flowsensitive abstractions heap enable precise modeling implicit flows tackle challenge advance new symbolic abstraction approach modeling heap javalike programs use storeless representation parameterized family relations among references offer various levels precision based user preferences enables us automatically infer polymorphic informationflow guards methods via coreachability analysis symbolic finitestate system instantiate heap abstraction three different families relations prove soundness approach compare precision scalability obtained instantiated heap domain using ifspec benchmarks reallife applications
(R)SE challenges in HPC,"We discuss some specific software engineering challenges in the field of high-performance computing, and argue that the slow adoption of SE tools and techniques is at least in part caused by the fact that these do not address the HPC challenges `out-of-the-box'. By giving some examples of solutions for designing, testing and benchmarking HPC software, we intend to bring software engineering and HPC closer together.",rse challenges hpc,discuss specific software engineering challenges field highperformance computing argue slow adoption se tools techniques least part caused fact address hpc challenges outofthebox giving examples solutions designing testing benchmarking hpc software intend bring software engineering hpc closer together
Ginkgo -- A Math Library designed for Platform Portability,"The first associations to software sustainability might be the existence of a continuous integration (CI) framework; the existence of a testing framework composed of unit tests, integration tests, and end-to-end tests; and also the existence of software documentation. However, when asking what is a common deathblow for a scientific software product, it is often the lack of platform and performance portability. Against this background, we designed the Ginkgo library with the primary focus on platform portability and the ability to not only port to new hardware architectures, but also achieve good performance. In this paper we present the Ginkgo library design, radically separating algorithms from hardware-specific kernels forming the distinct hardware executors, and report our experience when adding execution backends for NVIDIA, AMD, and Intel GPUs. We also comment on the different levels of performance portability, and the performance we achieved on the distinct hardware backends.",ginkgo math library designed platform portability,first associations software sustainability might existence continuous integration ci framework existence testing framework composed unit tests integration tests endtoend tests also existence software documentation however asking common deathblow scientific software product often lack platform performance portability background designed ginkgo library primary focus platform portability ability port new hardware architectures also achieve good performance paper present ginkgo library design radically separating algorithms hardwarespecific kernels forming distinct hardware executors report experience adding execution backends nvidia amd intel gpus also comment different levels performance portability performance achieved distinct hardware backends
Performant Automatic BLAS Offloading on Unified Memory Architecture with   OpenMP First-Touch Style Data Movement,"BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.",performant automatic blas offloading unified memory architecture openmp firsttouch style data movement,blas fundamental building block advanced linear algebra libraries many modern scientific computing applications gpus known strong arithmetic computing capabilities highly suited blas operations however porting code gpus often requires significant effort especially large complex codes legacy codes even blasheavy applications various tools exist automatically offload blas gpus often impractical due high costs associated mandatory data transfers advent unified memory architectures recent gpu designs nvidia gracehopper allows cachecoherent memory access across types memory cpu gpu potentially eliminating bottlenecks faced conventional architectures breakthrough paves way innovative application developments porting strategies building preliminary work demonstrating potential automatic gemm offload paper extends framework level blas operations introduces scilibaccel novel tool automatic blas offload scilibaccel leverages memory coherency gracehopper introduces device firstuse data movement policy inspired openmp firsttouch approach multisocket cpu programming minimizing cpugpu data transfers typical scientific computing codes additionally utilizing dynamic binary instrumentation tool intercepts blas symbols directly cpu binary requiring code modifications recompilation scilibaccel evaluated using multiple quantum physics codes hundred gpu nodes yielding promising speedups notably lsms method must suite x speedup achieved gracehopper compared gracegrace
A model-driven approach for a new generation of adaptive libraries,"Efficient high-performance libraries often expose multiple tunable parameters to provide highly optimized routines. These can range from simple loop unroll factors or vector sizes all the way to algorithmic changes, given that some implementations can be more suitable for certain devices by exploiting hardware characteristics such as local memories and vector units. Traditionally, such parameters and algorithmic choices are tuned and then hard-coded for a specific architecture and for certain characteristics of the inputs. However, emerging applications are often data-driven, thus traditional approaches are not effective across the wide range of inputs and architectures used in practice. In this paper, we present a new adaptive framework for data-driven applications which uses a predictive model to select the optimal algorithmic parameters by training with synthetic and real datasets. We demonstrate the effectiveness of a BLAS library and specifically on its matrix multiplication routine. We present experimental results for two GPU architectures and show significant performance gains of up to 3x (on a high-end NVIDIA Pascal GPU) and 2.5x (on an embedded ARM Mali GPU) when compared to a traditionally optimized library.",modeldriven approach new generation adaptive libraries,efficient highperformance libraries often expose multiple tunable parameters provide highly optimized routines range simple loop unroll factors vector sizes way algorithmic changes given implementations suitable certain devices exploiting hardware characteristics local memories vector units traditionally parameters algorithmic choices tuned hardcoded specific architecture certain characteristics inputs however emerging applications often datadriven thus traditional approaches effective across wide range inputs architectures used practice paper present new adaptive framework datadriven applications uses predictive model select optimal algorithmic parameters training synthetic real datasets demonstrate effectiveness blas library specifically matrix multiplication routine present experimental results two gpu architectures show significant performance gains x highend nvidia pascal gpu x embedded arm mali gpu compared traditionally optimized library
"A Global Perspective on the Past, Present, and Future of Video Streaming   over Starlink","This study presents the first global analysis of on-demand video streaming over Low Earth Orbit (LEO) satellite networks, using data from over one million households across 85 countries. We highlight Starlink's role as a major LEO provider, enhancing connectivity in underserved regions. Our findings reveal that while overall video quality on Starlink matches that of traditional networks, the inherent variability in LEO conditions -- such as throughput fluctuations and packet loss -- leads to an increase in bitrate switches and rebuffers. To further improve the quality of experience for the LEO community, we manipulate existing congestion control and adaptive bitrate streaming algorithms using simulation and real A/B tests deployed on over one million households. Our results underscore the need for video streaming and congestion control algorithms to adapt to rapidly evolving network landscapes, ensuring high-quality service across diverse and dynamic network types.",global perspective past present future video streaming starlink,study presents first global analysis ondemand video streaming low earth orbit leo satellite networks using data one million households across countries highlight starlinks role major leo provider enhancing connectivity underserved regions findings reveal overall video quality starlink matches traditional networks inherent variability leo conditions throughput fluctuations packet loss leads increase bitrate switches rebuffers improve quality experience leo community manipulate existing congestion control adaptive bitrate streaming algorithms using simulation real ab tests deployed one million households results underscore need video streaming congestion control algorithms adapt rapidly evolving network landscapes ensuring highquality service across diverse dynamic network types
The Business of Selling Electronic Documents,"The music industry has huge troubles adapting to the new technologies. As many pointed out, when copying music is essentially free and socially accepted it becomes increasingly tempting for users to infringe copyrights and copy music from one person to another. The answer of the music industry is to outlaw a majority of citizens. This article describes how the music industry should reinvent itself and adapt to a world where the network is ubiquitous and exchanging information is essentially free. It relies on adapting prices to the demand and lower costs of electronic documents in a dramatic way.",business selling electronic documents,music industry huge troubles adapting new technologies many pointed copying music essentially free socially accepted becomes increasingly tempting users infringe copyrights copy music one person another answer music industry outlaw majority citizens article describes music industry reinvent adapt world network ubiquitous exchanging information essentially free relies adapting prices demand lower costs electronic documents dramatic way
Deciding Regularity of the Set of Instances of a Set of Terms with   Regular Constraints is EXPTIME-Complete,"Finite-state tree automata are a well studied formalism for representing term languages. This paper studies the problem of determining the regularity of the set of instances of a finite set of terms with variables, where each variable is restricted to instantiations of a regular set given by a tree automaton. The problem was recently proved decidable, but with an unknown complexity. Here, the exact complexity of the problem is determined by proving EXPTIME-completeness. The main contribution is a new, exponential time algorithm that performs various exponential transformations on the involved terms and tree automata, and decides regularity by analyzing formulas over inequality and height predicates.",deciding regularity set instances set terms regular constraints exptimecomplete,finitestate tree automata well studied formalism representing term languages paper studies problem determining regularity set instances finite set terms variables variable restricted instantiations regular set given tree automaton problem recently proved decidable unknown complexity exact complexity problem determined proving exptimecompleteness main contribution new exponential time algorithm performs various exponential transformations involved terms tree automata decides regularity analyzing formulas inequality height predicates
Proceedings Joint International Workshop on Linearity & Trends in Linear   Logic and Applications,"This volume contains a selection of papers presented at Linearity/TLLA 2018: Joint Linearity and TLLA workshops (part of FLOC 2018) held on July 7-8, 2018 in Oxford. Linearity has been a key feature in several lines of research in both theoretical and practical approaches to computer science. On the theoretical side there is much work stemming from linear logic dealing with proof technology, complexity classes and more recently quantum computation. On the practical side there is work on program analysis, expressive operational semantics for programming languages, linear programming languages, program transformation, update analysis and efficient implementation techniques. Linear logic is not only a theoretical tool to analyse the use of resources in logic and computation. It is also a corpus of tools, approaches, and methodologies (proof nets, exponential decomposition, geometry of interaction, coherent spaces, relational models, etc.) that were originally developed for the study of linear logic's syntax and semantics and are nowadays applied in several other fields.",proceedings joint international workshop linearity trends linear logic applications,volume contains selection papers presented linearitytlla joint linearity tlla workshops part floc held july oxford linearity key feature several lines research theoretical practical approaches computer science theoretical side much work stemming linear logic dealing proof technology complexity classes recently quantum computation practical side work program analysis expressive operational semantics programming languages linear programming languages program transformation update analysis efficient implementation techniques linear logic theoretical tool analyse use resources logic computation also corpus tools approaches methodologies proof nets exponential decomposition geometry interaction coherent spaces relational models etc originally developed study linear logics syntax semantics nowadays applied several fields
Efficient Graph Rewriting,"Graph transformation is the rule-based modification of graphs, and is a discipline dating back to the 1970s. The declarative nature of graph rewriting rules comes at a cost. In general, to match the left-hand graph of a fixed rule within a host graph requires polynomial time. To improve matching performance, D\""orr proposed to equip rules and host graphs with distinguished root nodes. This model was implemented by Plump and Bak, but unfortunately, is not invertible. We address this problem by defining rootedness using a partial function onto a two-point set rather than pointing graphs with root nodes. We show a new result that the graph class of trees can be recognised by a rooted GT system in linear time, given an input graph of bounded degree. Finally, we define a new notion of confluence modulo garbage and non-garbage critical pairs, showing it is sufficient to require strong joinability of only the non-garbage critical pairs to establish confluence modulo garbage.",efficient graph rewriting,graph transformation rulebased modification graphs discipline dating back declarative nature graph rewriting rules comes cost general match lefthand graph fixed rule within host graph requires polynomial time improve matching performance dorr proposed equip rules host graphs distinguished root nodes model implemented plump bak unfortunately invertible address problem defining rootedness using partial function onto twopoint set rather pointing graphs root nodes show new result graph class trees recognised rooted gt system linear time given input graph bounded degree finally define new notion confluence modulo garbage nongarbage critical pairs showing sufficient require strong joinability nongarbage critical pairs establish confluence modulo garbage
Complexity and Algorithms for Euler Characteristic of Simplicial   Complexes,We consider the problem of computing the Euler characteristic of an abstract simplicial complex given by its vertices and facets. We show that this problem is #P-complete and present two new practical algorithms for computing Euler characteristic. The two new algorithms are derived using combinatorial commutative algebra and we also give a second description of them that requires no algebra. We present experiments showing that the two new algorithms can be implemented to be faster than previous Euler characteristic implementations by a large margin.,complexity algorithms euler characteristic simplicial complexes,consider problem computing euler characteristic abstract simplicial complex given vertices facets show problem pcomplete present two new practical algorithms computing euler characteristic two new algorithms derived using combinatorial commutative algebra also give second description requires algebra present experiments showing two new algorithms implemented faster previous euler characteristic implementations large margin
Exhaustive Survey of Rickrolling in Academic Literature,"Rickrolling is an Internet cultural phenomenon born in the mid 2000s. Originally confined to Internet fora, it has spread to other channels and media. In this paper, we hypothesize that rickrolling has reached the formal academic world. We design and conduct a systematic experiment to survey rickrolling in the academic literature. As of March 2022, there are 23 academic documents intentionally rickrolling the reader. Rickrolling happens in footnotes, code listings, references. We believe that rickrolling in academia proves inspiration and facetiousness, which is healthy for good science. This original research suggests areas of improvement for academic search engines and calls for more investigations about academic pranks and humor.",exhaustive survey rickrolling academic literature,rickrolling internet cultural phenomenon born mid originally confined internet fora spread channels media paper hypothesize rickrolling reached formal academic world design conduct systematic experiment survey rickrolling academic literature march academic documents intentionally rickrolling reader rickrolling happens footnotes code listings references believe rickrolling academia proves inspiration facetiousness healthy good science original research suggests areas improvement academic search engines calls investigations academic pranks humor
Emergence Explained,"Emergence (macro-level effects from micro-level causes) is at the heart of the conflict between reductionism and functionalism. How can there be autonomous higher level laws of nature (the functionalist claim) if everything can be reduced to the fundamental forces of physics (the reductionist position)? We cut through this debate by applying a computer science lens to the way we view nature. We conclude (a) that what functionalism calls the special sciences (sciences other than physics) do indeed study autonomous laws and furthermore that those laws pertain to real higher level entities but (b) that interactions among such higher-level entities is epiphenomenal in that they can always be reduced to primitive physical forces. In other words, epiphenomena, which we will identify with emergent phenomena, do real higher-level work. The proposed perspective provides a framework for understanding many thorny issues including the nature of entities, stigmergy, the evolution of complexity, phase transitions, supervenience, and downward entailment. We also discuss some practical considerations pertaining to systems of systems and the limitations of modeling.",emergence explained,emergence macrolevel effects microlevel causes heart conflict reductionism functionalism autonomous higher level laws nature functionalist claim everything reduced fundamental forces physics reductionist position cut debate applying computer science lens way view nature conclude functionalism calls special sciences sciences physics indeed study autonomous laws furthermore laws pertain real higher level entities b interactions among higherlevel entities epiphenomenal always reduced primitive physical forces words epiphenomena identify emergent phenomena real higherlevel work proposed perspective provides framework understanding many thorny issues including nature entities stigmergy evolution complexity phase transitions supervenience downward entailment also discuss practical considerations pertaining systems systems limitations modeling
Knowledge Scientists: Unlocking the data-driven organization,"Organizations across all sectors are increasingly undergoing deep transformation and restructuring towards data-driven operations. The central role of data highlights the need for reliable and clean data. Unreliable, erroneous, and incomplete data lead to critical bottlenecks in processing pipelines and, ultimately, service failures, which are disastrous for the competitive performance of the organization. Given its central importance, those organizations which recognize and react to the need for reliable data will have the advantage in the coming decade. We argue that the technologies for reliable data are driven by distinct concerns and expertise which complement those of the data scientist and the data engineer. Those organizations which identify the central importance of meaningful, explainable, reproducible, and maintainable data will be at the forefront of the democratization of reliable data. We call the new role which must be developed to fill this critical need the Knowledge Scientist. The organizational structures, tools, methodologies and techniques to support and make possible the work of knowledge scientists are still in their infancy. As organizations not only use data but increasingly rely on data, it is time to empower the people who are central to this transformation.",knowledge scientists unlocking datadriven organization,organizations across sectors increasingly undergoing deep transformation restructuring towards datadriven operations central role data highlights need reliable clean data unreliable erroneous incomplete data lead critical bottlenecks processing pipelines ultimately service failures disastrous competitive performance organization given central importance organizations recognize react need reliable data advantage coming decade argue technologies reliable data driven distinct concerns expertise complement data scientist data engineer organizations identify central importance meaningful explainable reproducible maintainable data forefront democratization reliable data call new role must developed fill critical need knowledge scientist organizational structures tools methodologies techniques support make possible work knowledge scientists still infancy organizations use data increasingly rely data time empower people central transformation
Towards Python-based Domain-specific Languages for Self-reconfigurable   Modular Robotics Research,"This paper explores the role of operating system and high-level languages in the development of software and domain-specific languages (DSLs) for self-reconfigurable robotics. We review some of the current trends in self-reconfigurable robotics and describe the development of a software system for ATRON II which utilizes Linux and Python to significantly improve software abstraction and portability while providing some basic features which could prove useful when using Python, either stand-alone or via a DSL, on a self-reconfigurable robot system. These features include transparent socket communication, module identification, easy software transfer and reliable module-to-module communication. The end result is a software platform for modular robots that where appropriate builds on existing work in operating systems, virtual machines, middleware and high-level languages.",towards pythonbased domainspecific languages selfreconfigurable modular robotics research,paper explores role operating system highlevel languages development software domainspecific languages dsls selfreconfigurable robotics review current trends selfreconfigurable robotics describe development software system atron ii utilizes linux python significantly improve software abstraction portability providing basic features could prove useful using python either standalone via dsl selfreconfigurable robot system features include transparent socket communication module identification easy software transfer reliable moduletomodule communication end result software platform modular robots appropriate builds existing work operating systems virtual machines middleware highlevel languages
GraphMaps: Browsing Large Graphs as Interactive Maps,"Algorithms for laying out large graphs have seen significant progress in the past decade. However, browsing large graphs remains a challenge. Rendering thousands of graphical elements at once often results in a cluttered image, and navigating these elements naively can cause disorientation. To address this challenge we propose a method called GraphMaps, mimicking the browsing experience of online geographic maps.   GraphMaps creates a sequence of layers, where each layer refines the previous one. During graph browsing, GraphMaps chooses the layer corresponding to the zoom level, and renders only those entities of the layer that intersect the current viewport. The result is that, regardless of the graph size, the number of entities rendered at each view does not exceed a predefined threshold, yet all graph elements can be explored by the standard zoom and pan operations.   GraphMaps preprocesses a graph in such a way that during browsing, the geometry of the entities is stable, and the viewer is responsive. Our case studies indicate that GraphMaps is useful in gaining an overview of a large graph, and also in exploring a graph on a finer level of detail.",graphmaps browsing large graphs interactive maps,algorithms laying large graphs seen significant progress past decade however browsing large graphs remains challenge rendering thousands graphical elements often results cluttered image navigating elements naively cause disorientation address challenge propose method called graphmaps mimicking browsing experience online geographic maps graphmaps creates sequence layers layer refines previous one graph browsing graphmaps chooses layer corresponding zoom level renders entities layer intersect current viewport result regardless graph size number entities rendered view exceed predefined threshold yet graph elements explored standard zoom pan operations graphmaps preprocesses graph way browsing geometry entities stable viewer responsive case studies indicate graphmaps useful gaining overview large graph also exploring graph finer level detail
Polynomial Bounds of CFLOBDDs against BDDs,"Binary Decision Diagrams (BDDs) are widely used for the representation of Boolean functions. Context-Free-Language Ordered Decision Diagrams (CFLOBDDs) are a plug-compatible replacement for BDDs -- roughly, they are BDDs augmented with a certain form of procedure call. A natural question to ask is, ``For a given family of Boolean functions $F$, what is the relationship between the size of a BDD for $f \in F$ and the size of a CFLOBDD for $f$?'' Sistla et al. established that there are best-case families of functions, which demonstrate an inherently exponential separation between CFLOBDDs and BDDs. They showed that there are families of functions $\{ f_n \}$ for which, for all $n = 2^k$, the CFLOBDD for $f_n$ (using a particular variable order) is exponentially more succinct than any BDD for $f_n$ (i.e., using any variable order). However, they did not give a worst-case bound -- i.e., they left open the question, ``Is there a family of functions $\{ g_i \}$ for which the size of a CFLOBDD for $g_i$ must be substantially larger than a BDD for $g_i$?'' For instance, it could be that there is a family of functions for which the BDDs are exponentially more succinct than any corresponding CFLOBDDs.   This paper studies such questions, and answers the second question posed above in the negative. In particular, we show that by using the same variable ordering in the CFLOBDD that is used in the BDD, the size of a CFLOBDD for any function $h$ cannot be far worse than the size of the BDD for $h$. The bound that relates their sizes is polynomial: If BDD $B$ for function $h$ is of size $|B|$ and uses variable ordering $\textit{Ord}$, then the size of the CFLOBDD $C$ for $h$ that also uses $\textit{Ord}$ is bounded by $O(|B|^3)$.   The paper also shows that the bound is tight: there is a family of functions for which $|C|$ grows as $\Omega(|B|^3)$.",polynomial bounds cflobdds bdds,binary decision diagrams bdds widely used representation boolean functions contextfreelanguage ordered decision diagrams cflobdds plugcompatible replacement bdds roughly bdds augmented certain form procedure call natural question ask given family boolean functions f relationship size bdd f f size cflobdd f sistla et al established bestcase families functions demonstrate inherently exponential separation cflobdds bdds showed families functions fn n k cflobdd fn using particular variable order exponentially succinct bdd fn ie using variable order however give worstcase bound ie left open question family functions gi size cflobdd gi must substantially larger bdd gi instance could family functions bdds exponentially succinct corresponding cflobdds paper studies questions answers second question posed negative particular show using variable ordering cflobdd used bdd size cflobdd function h cannot far worse size bdd h bound relates sizes polynomial bdd b function h size b uses variable ordering textitord size cflobdd c h also uses textitord bounded ob paper also shows bound tight family functions c grows omegab
Portability of Fortran's `do concurrent' on GPUs,"There is a continuing interest in using standard language constructs for accelerated computing in order to avoid (sometimes vendor-specific) external APIs. For Fortran codes, the {\tt do concurrent} (DC) loop has been successfully demonstrated on the NVIDIA platform. However, support for DC on other platforms has taken longer to implement. Recently, Intel has added DC GPU offload support to its compiler, as has HPE for AMD GPUs. In this paper, we explore the current portability of using DC across GPU vendors using the in-production solar surface flux evolution code, HipFT. We discuss implementation and compilation details, including when/where using directive APIs for data movement is needed/desired compared to using a unified memory system. The performance achieved on both data center and consumer platforms is shown.",portability fortrans concurrent gpus,continuing interest using standard language constructs accelerated computing order avoid sometimes vendorspecific external apis fortran codes tt concurrent dc loop successfully demonstrated nvidia platform however support dc platforms taken longer implement recently intel added dc gpu offload support compiler hpe amd gpus paper explore current portability using dc across gpu vendors using inproduction solar surface flux evolution code hipft discuss implementation compilation details including whenwhere using directive apis data movement neededdesired compared using unified memory system performance achieved data center consumer platforms shown
"Intel Cilk Plus for Complex Parallel Algorithms: ""Enormous Fast Fourier   Transform"" (EFFT) Library","In this paper we demonstrate the methodology for parallelizing the computation of large one-dimensional discrete fast Fourier transforms (DFFTs) on multi-core Intel Xeon processors. DFFTs based on the recursive Cooley-Tukey method have to control cache utilization, memory bandwidth and vector hardware usage, and at the same time scale across multiple threads or compute nodes. Our method builds on single-threaded Intel Math Kernel Library (MKL) implementation of DFFT, and uses the Intel Cilk Plus framework for thread parallelism. We demonstrate the ability of Intel Cilk Plus to handle parallel recursion with nested loop-centric parallelism without tuning the code to the number of cores or cache metrics. The result of our work is a library called EFFT that performs 1D DFTs of size 2^N for N>=21 faster than the corresponding Intel MKL parallel DFT implementation by up to 1.5x, and faster than FFTW by up to 2.5x. The code of EFFT is available for free download under the GPLv3 license. This work provides a new efficient DFFT implementation, and at the same time demonstrates an educational example of how computer science problems with complex parallel patterns can be optimized for high performance using the Intel Cilk Plus framework.",intel cilk plus complex parallel algorithms enormous fast fourier transform efft library,paper demonstrate methodology parallelizing computation large onedimensional discrete fast fourier transforms dffts multicore intel xeon processors dffts based recursive cooleytukey method control cache utilization memory bandwidth vector hardware usage time scale across multiple threads compute nodes method builds singlethreaded intel math kernel library mkl implementation dfft uses intel cilk plus framework thread parallelism demonstrate ability intel cilk plus handle parallel recursion nested loopcentric parallelism without tuning code number cores cache metrics result work library called efft performs dfts size n n faster corresponding intel mkl parallel dft implementation x faster fftw x code efft available free download gplv license work provides new efficient dfft implementation time demonstrates educational example computer science problems complex parallel patterns optimized high performance using intel cilk plus framework
The Existential Theory of the Reals as a Complexity Class: A Compendium,"We survey the complexity class $\exists \mathbb{R}$, which captures the complexity of deciding the existential theory of the reals. The class $\exists \mathbb{R}$ has roots in two different traditions, one based on the Blum-Shub-Smale model of real computation, and the other following work by Mn\""{e}v and Shor on the universality of realization spaces of oriented matroids. Over the years the number of problems for which $\exists \mathbb{R}$ rather than NP has turned out to be the proper way of measuring their complexity has grown, particularly in the fields of computational geometry, graph drawing, game theory, and some areas in logic and algebra. $\exists \mathbb{R}$ has also started appearing in the context of machine learning, Markov decision processes, and probabilistic reasoning.   We have aimed at collecting a comprehensive compendium of problems complete and hard for $\exists \mathbb{R}$, as well as a long list of open problems. The compendium is presented in the third part of our survey; a tour through the compendium and the areas it touches on makes up the second part. The first part introduces the reader to the existential theory of the reals as a complexity class, discussing its history, motivation and prospects as well as some technical aspects.",existential theory reals complexity class compendium,survey complexity class exists mathbbr captures complexity deciding existential theory reals class exists mathbbr roots two different traditions one based blumshubsmale model real computation following work mnev shor universality realization spaces oriented matroids years number problems exists mathbbr rather np turned proper way measuring complexity grown particularly fields computational geometry graph drawing game theory areas logic algebra exists mathbbr also started appearing context machine learning markov decision processes probabilistic reasoning aimed collecting comprehensive compendium problems complete hard exists mathbbr well long list open problems compendium presented third part survey tour compendium areas touches makes second part first part introduces reader existential theory reals complexity class discussing history motivation prospects well technical aspects
Carbon Containers: A System-level Facility for Managing   Application-level Carbon Emissions,"To reduce their environmental impact, cloud datacenters' are increasingly focused on optimizing applications' carbon-efficiency, or work done per mass of carbon emitted. To facilitate such optimizations, we present Carbon Containers, a simple system-level facility, which extends prior work on power containers, that automatically regulates applications' carbon emissions in response to variations in both their workload's intensity and their energy's carbon-intensity. Specifically, \carbonContainerS enable applications to specify a maximum carbon emissions rate (in g$\cdot$CO$_2$e/hr), and then transparently enforce this rate via a combination of vertical scaling, container migration, and suspend/resume while maximizing either energy-efficiency or performance.   Carbon Containers are especially useful for applications that i) must continue running even during high-carbon periods, and ii) execute in regions with few variations in carbon-intensity. These low-variability regions also tend to have high average carbon-intensity, which increases the importance of regulating carbon emissions. We implement a Carbon Containers prototype by extending Linux Containers to incorporate the mechanisms above and evaluate it using real workload traces and carbon-intensity data from multiple regions. We compare Carbon Containers with prior work that regulates carbon emissions by suspending/resuming applications during high/low carbon periods. We show that Carbon Containers are more carbon-efficient and improve performance while maintaining similar carbon emissions.",carbon containers systemlevel facility managing applicationlevel carbon emissions,reduce environmental impact cloud datacenters increasingly focused optimizing applications carbonefficiency work done per mass carbon emitted facilitate optimizations present carbon containers simple systemlevel facility extends prior work power containers automatically regulates applications carbon emissions response variations workloads intensity energys carbonintensity specifically carboncontainers enable applications specify maximum carbon emissions rate gcdotcoehr transparently enforce rate via combination vertical scaling container migration suspendresume maximizing either energyefficiency performance carbon containers especially useful applications must continue running even highcarbon periods ii execute regions variations carbonintensity lowvariability regions also tend high average carbonintensity increases importance regulating carbon emissions implement carbon containers prototype extending linux containers incorporate mechanisms evaluate using real workload traces carbonintensity data multiple regions compare carbon containers prior work regulates carbon emissions suspendingresuming applications highlow carbon periods show carbon containers carbonefficient improve performance maintaining similar carbon emissions
Summa Summarum: Moessner's Theorem without Dynamic Programming,"Seventy years on, Moessner's theorem and Moessner's process -- i.e., the additive computation of integral powers -- continue to fascinate. They have given rise to a variety of elegant proofs, to an implementation in hardware, to generalizations, and now even to a popular video, ""The Moessner Miracle.'' The existence of this video, and even more its title, indicate that while the ""what'' of Moessner's process is understood, its ""how'' and even more its ""why'' are still elusive. And indeed all the proofs of Moessner's theorem involve more complicated concepts than both the theorem and the process. This article identifies that Moessner's process implements an additive function with dynamic programming. A version of this implementation without dynamic programming (1) gives rise to a simpler statement of Moessner's theorem and (2) can be abstracted and then instantiated into related additive computations. The simpler statement also suggests a simpler and more efficient implementation to compute integral powers as well as simple additive functions to compute, e.g., Factorial numbers. It also reveals the source of -- to quote John Conway and Richard Guy -- Moessner's magic.",summa summarum moessners theorem without dynamic programming,seventy years moessners theorem moessners process ie additive computation integral powers continue fascinate given rise variety elegant proofs implementation hardware generalizations even popular video moessner miracle existence video even title indicate moessners process understood even still elusive indeed proofs moessners theorem involve complicated concepts theorem process article identifies moessners process implements additive function dynamic programming version implementation without dynamic programming gives rise simpler statement moessners theorem abstracted instantiated related additive computations simpler statement also suggests simpler efficient implementation compute integral powers well simple additive functions compute eg factorial numbers also reveals source quote john conway richard guy moessners magic
TAPA: A Scalable Task-Parallel Dataflow Programming Framework for Modern   FPGAs with Co-Optimization of HLS and Physical Design,"In this paper, we propose TAPA, an end-to-end framework that compiles a C++ task-parallel dataflow program into a high-frequency FPGA accelerator. Compared to existing solutions, TAPA has two major advantages. First, TAPA provides a set of convenient APIs that allow users to easily express flexible and complex inter-task communication structures. Second, TAPA adopts a coarse-grained floorplanning step during HLS compilation for accurate pipelining of potential critical paths. In addition, TAPA implements several optimization techniques specifically tailored for modern HBM-based FPGAs. In our experiments with a total of 43 designs, we improve the average frequency from 147 MHz to 297 MHz (a 102% improvement) with no loss of throughput and a negligible change in resource utilization. Notably, in 16 experiments we make the originally unroutable designs achieve 274 MHz on average. The framework is available at https://github.com/UCLA-VAST/tapa and the core floorplan module is available at https://github.com/UCLA-VAST/AutoBridge.",tapa scalable taskparallel dataflow programming framework modern fpgas cooptimization hls physical design,paper propose tapa endtoend framework compiles c taskparallel dataflow program highfrequency fpga accelerator compared existing solutions tapa two major advantages first tapa provides set convenient apis allow users easily express flexible complex intertask communication structures second tapa adopts coarsegrained floorplanning step hls compilation accurate pipelining potential critical paths addition tapa implements several optimization techniques specifically tailored modern hbmbased fpgas experiments total designs improve average frequency mhz mhz improvement loss throughput negligible change resource utilization notably experiments make originally unroutable designs achieve mhz average framework available httpsgithubcomuclavasttapa core floorplan module available httpsgithubcomuclavastautobridge
Massimult: A Novel Parallel CPU Architecture Based on Combinator   Reduction,"The Massimult project aims to design and implement an innovative CPU architecture based on combinator reduction with a novel combinator base and a new abstract machine. The evaluation of programs within this architecture is inherently highly parallel and localized, allowing for faster computation, reduced energy consumption, improved scalability, enhanced reliability, and increased resistance to attacks. In this paper, we introduce the machine language LambdaM, detail its compilation into KVY assembler code, and describe the abstract machine Matrima. The best part of Matrima is its ability to exploit inherent parallelism and locality in combinator reduction, leading to significantly faster computations with lower energy consumption, scalability across multiple processors, and enhanced security against various types of attacks. Matrima can be simulated as a software virtual machine and is intended for future hardware implementation.",massimult novel parallel cpu architecture based combinator reduction,massimult project aims design implement innovative cpu architecture based combinator reduction novel combinator base new abstract machine evaluation programs within architecture inherently highly parallel localized allowing faster computation reduced energy consumption improved scalability enhanced reliability increased resistance attacks paper introduce machine language lambdam detail compilation kvy assembler code describe abstract machine matrima best part matrima ability exploit inherent parallelism locality combinator reduction leading significantly faster computations lower energy consumption scalability across multiple processors enhanced security various types attacks matrima simulated software virtual machine intended future hardware implementation
Highly Parallel Sparse Matrix-Matrix Multiplication,Generalized sparse matrix-matrix multiplication is a key primitive for many high performance graph algorithms as well as some linear solvers such as multigrid. We present the first parallel algorithms that achieve increasing speedups for an unbounded number of processors. Our algorithms are based on two-dimensional block distribution of sparse matrices where serial sections use a novel hypersparse kernel for scalability. We give a state-of-the-art MPI implementation of one of our algorithms. Our experiments show scaling up to thousands of processors on a variety of test scenarios.,highly parallel sparse matrixmatrix multiplication,generalized sparse matrixmatrix multiplication key primitive many high performance graph algorithms well linear solvers multigrid present first parallel algorithms achieve increasing speedups unbounded number processors algorithms based twodimensional block distribution sparse matrices serial sections use novel hypersparse kernel scalability give stateoftheart mpi implementation one algorithms experiments show scaling thousands processors variety test scenarios
Architecture-Aware Configuration and Scheduling of Matrix Multiplication   on Asymmetric Multicore Processors,"Asymmetric multicore processors (AMPs) have recently emerged as an appealing technology for severely energy-constrained environments, especially in mobile appliances where heterogeneity in applications is mainstream. In addition, given the growing interest for low-power high performance computing, this type of architectures is also being investigated as a means to improve the throughput-per-Watt of complex scientific applications.   In this paper, we design and embed several architecture-aware optimizations into a multi-threaded general matrix multiplication (gemm), a key operation of the BLAS, in order to obtain a high performance implementation for ARM big.LITTLE AMPs. Our solution is based on the reference implementation of gemm in the BLIS library, and integrates a cache-aware configuration as well as asymmetric--static and dynamic scheduling strategies that carefully tune and distribute the operation's micro-kernels among the big and LITTLE cores of the target processor. The experimental results on a Samsung Exynos 5422, a system-on-chip with ARM Cortex-A15 and Cortex-A7 clusters that implements the big.LITTLE model, expose that our cache-aware versions of gemm with asymmetric scheduling attain important gains in performance with respect to its architecture-oblivious counterparts while exploiting all the resources of the AMP to deliver considerable energy efficiency.",architectureaware configuration scheduling matrix multiplication asymmetric multicore processors,asymmetric multicore processors amps recently emerged appealing technology severely energyconstrained environments especially mobile appliances heterogeneity applications mainstream addition given growing interest lowpower high performance computing type architectures also investigated means improve throughputperwatt complex scientific applications paper design embed several architectureaware optimizations multithreaded general matrix multiplication gemm key operation blas order obtain high performance implementation arm biglittle amps solution based reference implementation gemm blis library integrates cacheaware configuration well asymmetricstatic dynamic scheduling strategies carefully tune distribute operations microkernels among big little cores target processor experimental results samsung exynos systemonchip arm cortexa cortexa clusters implements biglittle model expose cacheaware versions gemm asymmetric scheduling attain important gains performance respect architectureoblivious counterparts exploiting resources amp deliver considerable energy efficiency
Parallel Sparse Matrix-Matrix Multiplication and Indexing:   Implementation and Experiments,"Generalized sparse matrix-matrix multiplication (or SpGEMM) is a key primitive for many high performance graph algorithms as well as for some linear solvers, such as algebraic multigrid. Here we show that SpGEMM also yields efficient algorithms for general sparse-matrix indexing in distributed memory, provided that the underlying SpGEMM implementation is sufficiently flexible and scalable. We demonstrate that our parallel SpGEMM methods, which use two-dimensional block data distributions with serial hypersparse kernels, are indeed highly flexible, scalable, and memory-efficient in the general case. This algorithm is the first to yield increasing speedup on an unbounded number of processors; our experiments show scaling up to thousands of processors in a variety of test scenarios.",parallel sparse matrixmatrix multiplication indexing implementation experiments,generalized sparse matrixmatrix multiplication spgemm key primitive many high performance graph algorithms well linear solvers algebraic multigrid show spgemm also yields efficient algorithms general sparsematrix indexing distributed memory provided underlying spgemm implementation sufficiently flexible scalable demonstrate parallel spgemm methods use twodimensional block data distributions serial hypersparse kernels indeed highly flexible scalable memoryefficient general case algorithm first yield increasing speedup unbounded number processors experiments show scaling thousands processors variety test scenarios
Sparse matrix-vector multiplication on GPGPU clusters: A new storage   format and a scalable implementation,"Sparse matrix-vector multiplication (spMVM) is the dominant operation in many sparse solvers. We investigate performance properties of spMVM with matrices of various sparsity patterns on the nVidia ""Fermi"" class of GPGPUs. A new ""padded jagged diagonals storage"" (pJDS) format is proposed which may substantially reduce the memory overhead intrinsic to the widespread ELLPACK-R scheme. In our test scenarios the pJDS format cuts the overall spMVM memory footprint on the GPGPU by up to 70%, and achieves 95% to 130% of the ELLPACK-R performance. Using a suitable performance model we identify performance bottlenecks on the node level that invalidate some types of matrix structures for efficient multi-GPGPU parallelization. For appropriate sparsity patterns we extend previous work on distributed-memory parallel spMVM to demonstrate a scalable hybrid MPI-GPGPU code, achieving efficient overlap of communication and computation.",sparse matrixvector multiplication gpgpu clusters new storage format scalable implementation,sparse matrixvector multiplication spmvm dominant operation many sparse solvers investigate performance properties spmvm matrices various sparsity patterns nvidia fermi class gpgpus new padded jagged diagonals storage pjds format proposed may substantially reduce memory overhead intrinsic widespread ellpackr scheme test scenarios pjds format cuts overall spmvm memory footprint gpgpu achieves ellpackr performance using suitable performance model identify performance bottlenecks node level invalidate types matrix structures efficient multigpgpu parallelization appropriate sparsity patterns extend previous work distributedmemory parallel spmvm demonstrate scalable hybrid mpigpgpu code achieving efficient overlap communication computation
Reducing Communication in Algebraic Multigrid with Multi-step Node Aware   Communication,"Algebraic multigrid (AMG) is often viewed as a scalable $\mathcal{O}(n)$ solver for sparse linear systems. Yet, parallel AMG lacks scalability due to increasingly large costs associated with communication, both in the initial construction of a multigrid hierarchy as well as the iterative solve phase. This work introduces a parallel implementation of AMG to reduce the cost of communication, yielding an increase in scalability. Standard inter-process communication consists of sending data regardless of the send and receive process locations. Performance tests show notable differences in the cost of intra- and inter-node communication, motivating a restructuring of communication. In this case, the communication schedule takes advantage of the less costly intra-node communication, reducing both the number and size of inter-node messages. Node-centric communication extends to the range of components in both the setup and solve phase of AMG, yielding an increase in the weak and strong scalability of the entire method.",reducing communication algebraic multigrid multistep node aware communication,algebraic multigrid amg often viewed scalable mathcalon solver sparse linear systems yet parallel amg lacks scalability due increasingly large costs associated communication initial construction multigrid hierarchy well iterative solve phase work introduces parallel implementation amg reduce cost communication yielding increase scalability standard interprocess communication consists sending data regardless send receive process locations performance tests show notable differences cost intra internode communication motivating restructuring communication case communication schedule takes advantage less costly intranode communication reducing number size internode messages nodecentric communication extends range components setup solve phase amg yielding increase weak strong scalability entire method
Can REF output quality scores be assigned by AI? Experimental evidence,This document describes strategies for using Artificial Intelligence (AI) to predict some journal article scores in future research assessment exercises. Five strategies have been assessed.,ref output quality scores assigned ai experimental evidence,document describes strategies using artificial intelligence ai predict journal article scores future research assessment exercises five strategies assessed
A Hardware Time Manager Implementation for the Xenomai Real-Time Kernel   of Embedded Linux,"Nowadays, the use of embedded operating systems in different embedded projects is subject to a tremendous growth. Embedded Linux is becoming one of those most popular EOSs due to its modularity, efficiency, reliability, and cost. One way to make it hard real-time is to include a real-time kernel like Xenomai. One of the key characteristics of a Real-Time Operating System (RTOS) is its ability to meet execution time deadlines deterministically. So, the more precise and flexible the time management can be, the better it can handle efficiently the determinism for different embedded applications. RTOS time precision is characterized by a specific periodic interrupt service controlled by a software time manager. The smaller the period of the interrupt, the better the precision of the RTOS, the more it overloads the CPU, and though reduces the overall efficiency of the RTOS. In this paper, we propose to drastically reduce these overheads by migrating the time management service of Xenomai into a configurable hardware component to relieve the CPU. The hardware component is implemented in a Field Programmable Gate Array coupled to the CPU. This work was achieved in a Master degree project where students could apprehend many fields of embedded systems: RTOS programming, hardware design, performance evaluation, etc.",hardware time manager implementation xenomai realtime kernel embedded linux,nowadays use embedded operating systems different embedded projects subject tremendous growth embedded linux becoming one popular eoss due modularity efficiency reliability cost one way make hard realtime include realtime kernel like xenomai one key characteristics realtime operating system rtos ability meet execution time deadlines deterministically precise flexible time management better handle efficiently determinism different embedded applications rtos time precision characterized specific periodic interrupt service controlled software time manager smaller period interrupt better precision rtos overloads cpu though reduces overall efficiency rtos paper propose drastically reduce overheads migrating time management service xenomai configurable hardware component relieve cpu hardware component implemented field programmable gate array coupled cpu work achieved master degree project students could apprehend many fields embedded systems rtos programming hardware design performance evaluation etc
Rank-Aware Dynamic Migrations and Adaptive Demotions for DRAM Power   Management,"Modern DRAM architectures allow a number of low-power states on individual memory ranks for advanced power management. Many previous studies have taken advantage of demotions on low-power states for energy saving. However, most of the demotion schemes are statically performed on a limited number of pre-selected low-power states, and are suboptimal for different workloads and memory architectures. Even worse, the idle periods are often too short for effective power state transitions, especially for memory intensive applications. Wrong decisions on power state transition incur significant energy and delay penalties. In this paper, we propose a novel memory system design named RAMZzz with rank-aware energy saving optimizations including dynamic page migrations and adaptive demotions. Specifically, we group the pages with similar access locality into the same rank with dynamic page migrations. Ranks have their hotness: hot ranks are kept busy for high utilization and cold ranks can have more lengthy idle periods for power state transitions. We further develop adaptive state demotions by considering all low-power states for each rank and a prediction model to estimate the power-down timeout among states. We experimentally compare our algorithm with other energy saving policies with cycle-accurate simulation. Experiments with benchmark workloads show that RAMZzz achieves significant improvement on energy-delay2 and energy consumption over other energy saving techniques.",rankaware dynamic migrations adaptive demotions dram power management,modern dram architectures allow number lowpower states individual memory ranks advanced power management many previous studies taken advantage demotions lowpower states energy saving however demotion schemes statically performed limited number preselected lowpower states suboptimal different workloads memory architectures even worse idle periods often short effective power state transitions especially memory intensive applications wrong decisions power state transition incur significant energy delay penalties paper propose novel memory system design named ramzzz rankaware energy saving optimizations including dynamic page migrations adaptive demotions specifically group pages similar access locality rank dynamic page migrations ranks hotness hot ranks kept busy high utilization cold ranks lengthy idle periods power state transitions develop adaptive state demotions considering lowpower states rank prediction model estimate powerdown timeout among states experimentally compare algorithm energy saving policies cycleaccurate simulation experiments benchmark workloads show ramzzz achieves significant improvement energydelay energy consumption energy saving techniques
Deterministic Memory Abstraction and Supporting Multicore System   Architecture,"Poor time predictability of multicore processors has been a long-standing challenge in the real-time systems community. In this paper, we make a case that a fundamental problem that prevents efficient and predictable real-time computing on multicore is the lack of a proper memory abstraction to express memory criticality, which cuts across various layers of the system: the application, OS, and hardware. We, therefore, propose a new holistic resource management approach driven by a new memory abstraction, which we call Deterministic Memory. The key characteristic of deterministic memory is that the platform - the OS and hardware - guarantees small and tightly bounded worst-case memory access timing. In contrast, we call the conventional memory abstraction as best-effort memory in which only highly pessimistic worst-case bounds can be achieved. We propose to utilize both abstractions to achieve high time predictability but without significantly sacrificing performance. We present deterministic memory-aware OS and architecture designs, including OS-level page allocator, hardware-level cache, and DRAM controller designs. We implement the proposed OS and architecture extensions on Linux and gem5 simulator. Our evaluation results, using a set of synthetic and real-world benchmarks, demonstrate the feasibility and effectiveness of our approach.",deterministic memory abstraction supporting multicore system architecture,poor time predictability multicore processors longstanding challenge realtime systems community paper make case fundamental problem prevents efficient predictable realtime computing multicore lack proper memory abstraction express memory criticality cuts across various layers system application os hardware therefore propose new holistic resource management approach driven new memory abstraction call deterministic memory key characteristic deterministic memory platform os hardware guarantees small tightly bounded worstcase memory access timing contrast call conventional memory abstraction besteffort memory highly pessimistic worstcase bounds achieved propose utilize abstractions achieve high time predictability without significantly sacrificing performance present deterministic memoryaware os architecture designs including oslevel page allocator hardwarelevel cache dram controller designs implement proposed os architecture extensions linux gem simulator evaluation results using set synthetic realworld benchmarks demonstrate feasibility effectiveness approach
The Preliminary Evaluation of a Hypervisor-based Virtualization   Mechanism for Intel Optane DC Persistent Memory Module,"Non-volatile memory (NVM) technologies, being accessible in the same manner as DRAM, are considered indispensable for expanding main memory capacities. Intel Optane DCPMM is a long-awaited product that drastically increases main memory capacities. However, a substantial performance gap exists between DRAM and DCPMM. In our experiments, the read/write latencies of DCPMM were 400% and 407% higher than those of DRAM, respectively. The read/write bandwidths were 37% and 8% of those of DRAM. This performance gap in main memory presents a new challenge to researchers; we need a new system software technology supporting emerging hybrid memory architecture. In this paper, we present RAMinate, a hypervisor-based virtualization mechanism for hybrid memory systems, and a key technology to address the performance gap in main memory systems. It provides great flexibility in memory management and maximizes the performance of virtual machines (VMs) by dynamically optimizing memory mappings. Through experiments, we confirmed that even though a VM has only 1% of DRAM in its RAM, the performance degradation of the VM was drastically alleviated by memory mapping optimization. The elapsed time to finish the build of Linux Kernel in the VM was 557 seconds, which was only 13% increase from the 100% DRAM case (i.e., 495 seconds). When the optimization mechanism was disabled, the elapsed time increased to 624 seconds (i.e. 26% increase from the 100% DRAM case).",preliminary evaluation hypervisorbased virtualization mechanism intel optane dc persistent memory module,nonvolatile memory nvm technologies accessible manner dram considered indispensable expanding main memory capacities intel optane dcpmm longawaited product drastically increases main memory capacities however substantial performance gap exists dram dcpmm experiments readwrite latencies dcpmm higher dram respectively readwrite bandwidths dram performance gap main memory presents new challenge researchers need new system software technology supporting emerging hybrid memory architecture paper present raminate hypervisorbased virtualization mechanism hybrid memory systems key technology address performance gap main memory systems provides great flexibility memory management maximizes performance virtual machines vms dynamically optimizing memory mappings experiments confirmed even though vm dram ram performance degradation vm drastically alleviated memory mapping optimization elapsed time finish build linux kernel vm seconds increase dram case ie seconds optimization mechanism disabled elapsed time increased seconds ie increase dram case
Analysis of Interference between RDMA and Local Access on Hybrid Memory   System,"We can use a hybrid memory system consisting of DRAM and Intel Optane DC Persistent Memory (We call it DCPM in this paper) as DCPM is now commercially available since April 2019. Even if the latency for DCPM is several times higher than that for DRAM, the capacity for DCPM is several times higher than that for DRAM and the cost of DCPM is also several times lower than that for DRAM. In addition, DCPM is non-volatile. A Server with this hybrid memory system could improve the performance for in-memory database systems and virtual machine (VM) systems because these systems often consume a large amount of memory. Moreover, a high-speed shared storage system can be implemented by accessing DCPM via remote direct memory access (RDMA). I assume that some of the DCPM is often assigned as a shared area among other remote servers because applications executed on a server with a hybrid memory system often cannot use the entire capacity of DCPM. This paper evaluates the interference between local memory access and RDMA from a remote server. As a result, I indicate that the interference on this hybrid memory system is significantly different from that on a conventional DRAM-only memory system. I also believe that some kind of throttling implementation is needed when this interference occures.",analysis interference rdma local access hybrid memory system,use hybrid memory system consisting dram intel optane dc persistent memory call dcpm paper dcpm commercially available since april even latency dcpm several times higher dram capacity dcpm several times higher dram cost dcpm also several times lower dram addition dcpm nonvolatile server hybrid memory system could improve performance inmemory database systems virtual machine vm systems systems often consume large amount memory moreover highspeed shared storage system implemented accessing dcpm via remote direct memory access rdma assume dcpm often assigned shared area among remote servers applications executed server hybrid memory system often cannot use entire capacity dcpm paper evaluates interference local memory access rdma remote server result indicate interference hybrid memory system significantly different conventional dramonly memory system also believe kind throttling implementation needed interference occures
Mitosis: Transparently Self-Replicating Page-Tables for Large-Memory   Machines,"Multi-socket machines with 1-100 TBs of physical memory are becoming prevalent. Applications running on multi-socket machines suffer non-uniform bandwidth and latency when accessing physical memory. Decades of research have focused on data allocation and placement policies in NUMA settings, but there have been no studies on the question of how to place page-tables amongst sockets. We make the case for explicit page-table allocation policies and show that page-table placement is becoming crucial to overall performance. We propose Mitosis to mitigate NUMA effects on page-table walks by transparently replicating and migrating page-tables across sockets without application changes. This reduces the frequency of accesses to remote NUMA nodes when performing page-table walks. Mitosis uses two components: (i) a mechanism to enable efficient page-table replication and migration; and (ii) policies for processes to efficiently manage and control page-table replication and migration. We implement Mitosis in Linux and evaluate its benefits on real hardware. Mitosis improves performance for large-scale multi-socket workloads by up to 1.34x by replicating page-tables across sockets. Moreover, it improves performance by up to 3.24x in cases when the OS migrates a process across sockets by enabling cross-socket page-table migration.",mitosis transparently selfreplicating pagetables largememory machines,multisocket machines tbs physical memory becoming prevalent applications running multisocket machines suffer nonuniform bandwidth latency accessing physical memory decades research focused data allocation placement policies numa settings studies question place pagetables amongst sockets make case explicit pagetable allocation policies show pagetable placement becoming crucial overall performance propose mitosis mitigate numa effects pagetable walks transparently replicating migrating pagetables across sockets without application changes reduces frequency accesses remote numa nodes performing pagetable walks mitosis uses two components mechanism enable efficient pagetable replication migration ii policies processes efficiently manage control pagetable replication migration implement mitosis linux evaluate benefits real hardware mitosis improves performance largescale multisocket workloads x replicating pagetables across sockets moreover improves performance x cases os migrates process across sockets enabling crosssocket pagetable migration
Leveraging Architectural Support of Three Page Sizes with Trident,"Large pages are commonly deployed to reduce address translation overheads for big-memory workloads. Modern x86-64 processors from Intel and AMD support two large page sizes -- 1GB and 2MB. However, previous works on large pages have primarily focused on 2MB pages, partly due to lack of substantial evidence on the profitability of 1GB pages to real-world applications. We argue that in fact, inadequate system software support is responsible for a decade of underutilized hardware support for 1GB pages.   Through extensive experimentation on a real system, we demonstrate that 1GB pages can improve performance over 2MB pages, and when used in tandem with 2MB pages for an important set of applications; the support for the latter is crucial but missing in current systems. Our design and implementation of \trident{} in Linux fully exploit hardware supported large pages by dynamically and transparently allocating 1GB, 2MB, and 4KB pages as deemed suitable. \trident{} speeds up eight memory-intensive applications by {$18\%$}, on average, over Linux's use of 2MB pages. We also propose \tridentpv{}, an extension to \trident{} that effectively virtualizes 1GB pages via copy-less promotion and compaction in the guest OS. Overall, this paper shows that even GB-sized pages have considerable practical significance with adequate software enablement, in turn motivating architects to continue investing/innovating in large pages.",leveraging architectural support three page sizes trident,large pages commonly deployed reduce address translation overheads bigmemory workloads modern x processors intel amd support two large page sizes gb mb however previous works large pages primarily focused mb pages partly due lack substantial evidence profitability gb pages realworld applications argue fact inadequate system software support responsible decade underutilized hardware support gb pages extensive experimentation real system demonstrate gb pages improve performance mb pages used tandem mb pages important set applications support latter crucial missing current systems design implementation trident linux fully exploit hardware supported large pages dynamically transparently allocating gb mb kb pages deemed suitable trident speeds eight memoryintensive applications average linuxs use mb pages also propose tridentpv extension trident effectively virtualizes gb pages via copyless promotion compaction guest os overall paper shows even gbsized pages considerable practical significance adequate software enablement turn motivating architects continue investinginnovating large pages
Virtual Memory Partitioning for Enhancing Application Performance in   Mobile Platforms,"Recently, the amount of running software on smart mobile devices is gradually increasing due to the introduction of application stores. The application store is a type of digital distribution platform for application software, which is provided as a component of an operating system on a smartphone or tablet. Mobile devices have limited memory capacity and, unlike server and desktop systems, due to their mobility they do not have a memory slot that can expand the memory capacity. Low memory killer (LMK) and out-of-memory killer (OOMK) are widely used memory management solutions in mobile systems. They forcibly terminate applications when the available physical memory becomes insufficient. In addition, before the forced termination, the memory shortage incurs thrashing and fragmentation, thus slowing down application performance. Although the existing page reclamation mechanism is designed to secure available memory, it could seriously degrade user responsiveness due to the thrashing. Memory management is therefore still important especially in mobile devices with small memory capacity. This paper presents a new memory partitioning technique that resolves the deterioration of the existing application life cycle induced by LMK and OOMK. It provides a completely isolated virtual memory node at the operating system level. Evaluation results demonstrate that the proposed method improves application execution time under memory shortage, compared with methods in previous studies.",virtual memory partitioning enhancing application performance mobile platforms,recently amount running software smart mobile devices gradually increasing due introduction application stores application store type digital distribution platform application software provided component operating system smartphone tablet mobile devices limited memory capacity unlike server desktop systems due mobility memory slot expand memory capacity low memory killer lmk outofmemory killer oomk widely used memory management solutions mobile systems forcibly terminate applications available physical memory becomes insufficient addition forced termination memory shortage incurs thrashing fragmentation thus slowing application performance although existing page reclamation mechanism designed secure available memory could seriously degrade user responsiveness due thrashing memory management therefore still important especially mobile devices small memory capacity paper presents new memory partitioning technique resolves deterioration existing application life cycle induced lmk oomk provides completely isolated virtual memory node operating system level evaluation results demonstrate proposed method improves application execution time memory shortage compared methods previous studies
CXLMemSim: A pure software simulated CXL.mem for performance   characterization,"The emerging CXL.mem standard provides a new type of byte-addressable remote memory with a variety of memory types and hierarchies. With CXL.mem, multiple layers of memory -- e.g., local DRAM and CXL-attached remote memory at different locations -- are exposed to operating systems and user applications, bringing new challenges and research opportunities. Unfortunately, since CXL.mem devices are not commercially available, it is difficult for researchers to conduct systems research that uses CXL.mem. In this paper, we present our ongoing work, CXLMemSim, a fast and lightweight CXL.mem simulator for performance characterization. CXLMemSim uses a performance model driven using performance monitoring events, which are supported by most commodity processors. Specifically, CXLMemSim attaches to an existing, unmodified program, and divides the execution of the program into multiple epochs; once an epoch finishes, CXLMemSim collects performance monitoring events and calculates the simulated execution time of the epoch based on these events. Through this method, CXLMemSim avoids the performance overhead of a full-system simulator (e.g., Gem5) and allows the memory hierarchy and latency to be easily adjusted, enabling research such as memory scheduling for complex applications. Our preliminary evaluation shows that CXLMemSim slows down the execution of the attached program by 4.41x on average for real-world applications.",cxlmemsim pure software simulated cxlmem performance characterization,emerging cxlmem standard provides new type byteaddressable remote memory variety memory types hierarchies cxlmem multiple layers memory eg local dram cxlattached remote memory different locations exposed operating systems user applications bringing new challenges research opportunities unfortunately since cxlmem devices commercially available difficult researchers conduct systems research uses cxlmem paper present ongoing work cxlmemsim fast lightweight cxlmem simulator performance characterization cxlmemsim uses performance model driven using performance monitoring events supported commodity processors specifically cxlmemsim attaches existing unmodified program divides execution program multiple epochs epoch finishes cxlmemsim collects performance monitoring events calculates simulated execution time epoch based events method cxlmemsim avoids performance overhead fullsystem simulator eg gem allows memory hierarchy latency easily adjusted enabling research memory scheduling complex applications preliminary evaluation shows cxlmemsim slows execution attached program x average realworld applications
Blockchain Goes Green? An Analysis of Blockchain on Low-Power Nodes,"Motivated by the massive energy usage of blockchain, on the one hand, and by significant performance improvements in low-power, wimpy systems, on the other hand, we perform an in-depth time-energy analysis of blockchain systems on low-power nodes in comparison to high-performance nodes. We use three low-power systems to represent a wide range of the performance-power spectrum, while covering both x86/64 and ARM architectures. We show that low-end wimpy nodes are struggling to run full-fledged blockchains mainly due to their small and low-bandwidth memory. On the other hand, wimpy systems with balanced performance-to-power ratio achieve reasonable performance while saving significant amounts of energy. For example, Jetson TX2 nodes achieve around 80% and 30% of the throughput of Parity and Hyperledger, respectively, while using 18x and 23x less energy compared to traditional brawny servers with Intel Xeon CPU.",blockchain goes green analysis blockchain lowpower nodes,motivated massive energy usage blockchain one hand significant performance improvements lowpower wimpy systems hand perform indepth timeenergy analysis blockchain systems lowpower nodes comparison highperformance nodes use three lowpower systems represent wide range performancepower spectrum covering x arm architectures show lowend wimpy nodes struggling run fullfledged blockchains mainly due small lowbandwidth memory hand wimpy systems balanced performancetopower ratio achieve reasonable performance saving significant amounts energy example jetson tx nodes achieve around throughput parity hyperledger respectively using x x less energy compared traditional brawny servers intel xeon cpu
Theory of sexes by Geodakian as it is advanced by Iskrin,"In 1960s V.Geodakian proposed a theory that explains sexes as a mechanism for evolutionary adaptation of the species to changing environmental conditions. In 2001 V.Iskrin refined and augmented the concepts of Geodakian and gave a new and interesting explanation to several phenomena which involve sex, and sex ratio, including the war-years phenomena. He also introduced a new concept of the ""catastrophic sex ratio."" This note is an attempt to digest technical aspects of the new ideas by Iskrin.",theory sexes geodakian advanced iskrin,vgeodakian proposed theory explains sexes mechanism evolutionary adaptation species changing environmental conditions viskrin refined augmented concepts geodakian gave new interesting explanation several phenomena involve sex sex ratio including waryears phenomena also introduced new concept catastrophic sex ratio note attempt digest technical aspects new ideas iskrin
